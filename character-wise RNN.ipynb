{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll load the text file and convert it into integers for our network to use. Here I'm creating a couple dictionaries to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt','r') as f:\n",
    "    text=f.read()\n",
    "vocab=sorted(set(text))\n",
    "chat_to_int = {c:i for i,c in enumerate(vocab)}\n",
    "int_to_car = dict(enumerate(vocab))\n",
    "encoded = np.array([chat_to_int[c] for c in text],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 64, 57, 72, 76, 61, 74,  1, 16,  0,  0,  0, 36, 57, 72, 72, 81,\n",
       "        1, 62, 57, 69, 65, 68, 65, 61, 75,  1, 57, 74, 61,  1, 57, 68, 68,\n",
       "        1, 57, 68, 65, 67, 61, 26,  1, 61, 78, 61, 74, 81,  1, 77, 70, 64,\n",
       "       57, 72, 72, 81,  1, 62, 57, 69, 65, 68, 81,  1, 65, 75,  1, 77, 70,\n",
       "       64, 57, 72, 72, 81,  1, 65, 70,  1, 65, 76, 75,  1, 71, 79, 70,  0,\n",
       "       79, 57, 81, 13,  0,  0, 33, 78, 61, 74, 81, 76, 64, 65, 70])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the network is working with individual characters, it's similar to a classification problem in which we are trying to predict the next character from the previous text. Here's how many 'classes' our network has to pick from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have our text encoded as integers as one long array in encoded. Let's create a function that will give us an iterator for our batches. I like using generator functions to do this. Then we can pass encoded into this function and get our batch generator.\n",
    "\n",
    "The first thing we need to do is discard some of the text so we only have completely full batches. Each batch contains $N \\times M$ characters, where $N$ is the batch size (the number of sequences) and $M$ is the number of steps. Then, to get the number of batches we can make from some array arr, you divide the length of arr by the batch size. Once you know the number of batches and the batch size, you can get the total number of characters to keep.\n",
    "\n",
    "After that, we need to split arr into $N$ sequences. You can do this using arr.reshape(size) where size is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences (n_seqs below), let's make that the size of the first dimension. For the second dimension, you can use -1 as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$ where $K$ is the number of batches.\n",
    "\n",
    "Now that we have this array, we can iterate through it to get our batches. The idea is each batch is a $N \\times M$ window on the array. For each subsequent batch, the window moves over by n_steps. We also want to create both the input and target arrays. Remember that the targets are the inputs shifted over one character. You'll usually see the first input character used as the last target character, so something like this:\n",
    "\n",
    "y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "where x is the input batch and y is the target batch.\n",
    "\n",
    "The way I like to do this window is use range to take steps of size n_steps from $0$ to arr.shape[1], the total number of steps in each sequence. That way, the integers you get from range always point to the start of a batch, and each window is n_steps wide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1985223,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(arr,seq_n,step_n):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    seq_n = batch size , number of seq per batch\n",
    "    step_n = number of time setps per batch\n",
    "    '''\n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    totalnochar=seq_n*step_n\n",
    "    n_batches= len(arr)//totalnochar\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr=arr[:totalnochar*n_batches]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((seq_n,-1))\n",
    "    #print (arr.shape)\n",
    "    \n",
    "    for n in range(0,arr.shape[1],step_n):\n",
    "        # The features\n",
    "        x = arr[:, n:n+step_n]\n",
    "        # The targets, shifted by one\n",
    "        y=np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:,0]\n",
    "        yield x,y\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 198500)\n",
      "[[31 64 57 ... 11  1 37]\n",
      " [ 1 57 69 ...  1 40 61]\n",
      " [78 65 70 ... 61 78 65]\n",
      " ...\n",
      " [26  1 58 ... 81  1 65]\n",
      " [76  1 65 ... 75 64 61]\n",
      " [ 1 75 57 ... 65 71 70]]\n",
      "x is:  [[31 64 57 72 76 61 74  1 16  0  0  0 36 57 72 72 81  1 62 57 69 65 68 65\n",
      "  61 75  1 57 74 61  1 57 68 68  1 57 68 65 67 61 26  1 61 78 61 74 81  1\n",
      "  77 70]\n",
      " [ 1 57 69  1 70 71 76  1 63 71 65 70 63  1 76 71  1 75 76 57 81 11  3  1\n",
      "  57 70 75 79 61 74 61 60  1 29 70 70 57 11  1 75 69 65 68 65 70 63 11  1\n",
      "  58 77]\n",
      " [78 65 70 13  0  0  3 53 61 75 11  1 65 76  7 75  1 75 61 76 76 68 61 60\n",
      "  13  1 48 64 61  1 72 74 65 59 61  1 65 75  1 69 57 63 70 65 62 65 59 61\n",
      "  70 76]\n",
      " [70  1 60 77 74 65 70 63  1 64 65 75  1 59 71 70 78 61 74 75 57 76 65 71\n",
      "  70  1 79 65 76 64  1 64 65 75  0 58 74 71 76 64 61 74  1 79 57 75  1 76\n",
      "  64 65]\n",
      " [ 1 65 76  1 65 75 11  1 75 65 74  2  3  1 75 57 65 60  1 76 64 61  1 71\n",
      "  68 60  1 69 57 70 11  1 63 61 76 76 65 70 63  1 77 72 11  1 57 70 60  0\n",
      "  59 74]\n",
      " [ 1 37 76  1 79 57 75  0 71 70 68 81  1 79 64 61 70  1 76 64 61  1 75 57\n",
      "  69 61  1 61 78 61 70 65 70 63  1 64 61  1 59 57 69 61  1 76 71  1 76 64\n",
      "  61 65]\n",
      " [64 61 70  1 59 71 69 61  1 62 71 74  1 69 61 11  3  1 75 64 61  1 75 57\n",
      "  65 60 11  1 57 70 60  1 79 61 70 76  1 58 57 59 67  1 65 70 76 71  1 76\n",
      "  64 61]\n",
      " [26  1 58 77 76  1 70 71 79  1 75 64 61  1 79 71 77 68 60  1 74 61 57 60\n",
      "  65 68 81  1 64 57 78 61  1 75 57 59 74 65 62 65 59 61 60 11  1 70 71 76\n",
      "   1 69]\n",
      " [76  1 65 75 70  7 76 13  1 48 64 61 81  7 74 61  1 72 74 71 72 74 65 61\n",
      "  76 71 74 75  1 71 62  1 57  1 75 71 74 76 11  0 58 77 76  1 79 61  7 74\n",
      "  61  1]\n",
      " [ 1 75 57 65 60  1 76 71  1 64 61 74 75 61 68 62 11  1 57 70 60  1 58 61\n",
      "  63 57 70  1 57 63 57 65 70  1 62 74 71 69  1 76 64 61  1 58 61 63 65 70\n",
      "  70 65]]\n",
      "y is:  [[64 57 72 76 61 74  1 16  0  0  0 36 57 72 72 81  1 62 57 69 65 68 65 61\n",
      "  75  1 57 74 61  1 57 68 68  1 57 68 65 67 61 26  1 61 78 61 74 81  1 77\n",
      "  70 31]\n",
      " [57 69  1 70 71 76  1 63 71 65 70 63  1 76 71  1 75 76 57 81 11  3  1 57\n",
      "  70 75 79 61 74 61 60  1 29 70 70 57 11  1 75 69 65 68 65 70 63 11  1 58\n",
      "  77  1]\n",
      " [65 70 13  0  0  3 53 61 75 11  1 65 76  7 75  1 75 61 76 76 68 61 60 13\n",
      "   1 48 64 61  1 72 74 65 59 61  1 65 75  1 69 57 63 70 65 62 65 59 61 70\n",
      "  76 78]\n",
      " [ 1 60 77 74 65 70 63  1 64 65 75  1 59 71 70 78 61 74 75 57 76 65 71 70\n",
      "   1 79 65 76 64  1 64 65 75  0 58 74 71 76 64 61 74  1 79 57 75  1 76 64\n",
      "  65 70]\n",
      " [65 76  1 65 75 11  1 75 65 74  2  3  1 75 57 65 60  1 76 64 61  1 71 68\n",
      "  60  1 69 57 70 11  1 63 61 76 76 65 70 63  1 77 72 11  1 57 70 60  0 59\n",
      "  74  1]\n",
      " [37 76  1 79 57 75  0 71 70 68 81  1 79 64 61 70  1 76 64 61  1 75 57 69\n",
      "  61  1 61 78 61 70 65 70 63  1 64 61  1 59 57 69 61  1 76 71  1 76 64 61\n",
      "  65  1]\n",
      " [61 70  1 59 71 69 61  1 62 71 74  1 69 61 11  3  1 75 64 61  1 75 57 65\n",
      "  60 11  1 57 70 60  1 79 61 70 76  1 58 57 59 67  1 65 70 76 71  1 76 64\n",
      "  61 64]\n",
      " [ 1 58 77 76  1 70 71 79  1 75 64 61  1 79 71 77 68 60  1 74 61 57 60 65\n",
      "  68 81  1 64 57 78 61  1 75 57 59 74 65 62 65 59 61 60 11  1 70 71 76  1\n",
      "  69 26]\n",
      " [ 1 65 75 70  7 76 13  1 48 64 61 81  7 74 61  1 72 74 71 72 74 65 61 76\n",
      "  71 74 75  1 71 62  1 57  1 75 71 74 76 11  0 58 77 76  1 79 61  7 74 61\n",
      "   1 76]\n",
      " [75 57 65 60  1 76 71  1 64 61 74 75 61 68 62 11  1 57 70 60  1 58 61 63\n",
      "  57 70  1 57 63 57 65 70  1 62 74 71 69  1 76 64 61  1 58 61 63 65 70 70\n",
      "  65  1]]\n"
     ]
    }
   ],
   "source": [
    "#Visualize what is happpening\n",
    "\n",
    "# In this code n_steps is the size of horizontal width of what moves\n",
    "# And n_seqs is the no. of rows \n",
    "# y is the same size as x which is n_steps just one shifted right\n",
    "n_seqs = 10\n",
    "n_steps = 50\n",
    "arr = encoded\n",
    "\n",
    "characters_per_batch = n_seqs*n_steps\n",
    "n_batches = len(arr)//characters_per_batch\n",
    "\n",
    "arr = arr[:characters_per_batch*n_batches]\n",
    "# Reshape into n_seqs rows\n",
    "arr = arr.reshape((n_seqs,-1))\n",
    "print(arr.shape)\n",
    "print(arr)\n",
    "for n in range(0, arr.shape[1], n_steps)[:1]:\n",
    "    # The features\n",
    "    x = arr[:, n:n+n_steps]\n",
    "    # The targets, shifted by one\n",
    "    y = np.zeros_like(x)\n",
    "    y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "    print(\"x is: \", x)\n",
    "    print(\"y is: \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches=create_batches(encoded,10,50)\n",
    "x,y=next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 75, 57, 65, 60,  1, 76, 71,  1, 64, 61, 74, 75, 61, 68, 62, 11,\n",
       "        1, 57, 70, 60,  1, 58, 61, 63, 57, 70,  1, 57, 63, 57, 65, 70,  1,\n",
       "       62, 74, 71, 69,  1, 76, 64, 61,  1, 58, 61, 63, 65, 70, 70, 65])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[9,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs\n",
    "First off we'll create our input placeholders. As usual we need placeholders for the training data and the targets. We'll also create a placeholder for dropout layers called keep_prob. This will be a scalar, that is a 0-D tensor. To make a scalar, you create a placeholder without giving it a size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size,num_steps):\n",
    "    inputs = tf.placeholder(tf.int32,shape=[batch_size,num_steps],name='input')\n",
    "    output = tf.placeholder(tf.int32,shape=[batch_size,num_steps],name='output')\n",
    "    keep_proba = tf.placeholder(tf.float32,name='keep_proba')\n",
    "    \n",
    "    return inputs,output,keep_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Cell\n",
    "Here we will create the LSTM cell we'll use in the hidden layer. We'll use this cell as a building block for the RNN. So we aren't actually defining the RNN here, just the type of cell we'll use in the hidden layer.\n",
    "\n",
    "We first create a basic LSTM cell with\n",
    "\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "where num_units is the number of units in the hidden layers in the cell. Then we can add dropout by wrapping it with\n",
    "\n",
    "tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "You pass in a cell and it will automatically add dropout to the inputs or outputs. Finally, we can stack up the LSTM cells into layers with tf.contrib.rnn.MultiRNNCell. With this, you pass in a list of cells and it will send the output of one cell into the next cell. Previously with TensorFlow 1.0, you could do this\n",
    "\n",
    "tf.contrib.rnn.MultiRNNCell([cell]*num_layers)\n",
    "This might look a little weird if you know Python well because this will create a list of the same cell object. However, TensorFlow 1.0 will create different weight matrices for all cell objects. But, starting with TensorFlow 1.1 you actually need to create new cell objects in the list. To get it to work in TensorFlow 1.1, it should look like\n",
    "\n",
    "def build_cell(num_units, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "    return drop\n",
    "\n",
    "tf.contrib.rnn.MultiRNNCell([build_cell(num_units, keep_prob) for _ in range(num_layers)])\n",
    "Even though this is actually multiple LSTM cells stacked on each other, you can treat the multiple layers as one cell.\n",
    "\n",
    "We also need to create an initial cell state of all zeros. This can be done like so\n",
    "\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "Below, we implement the build_lstm function to create these LSTM cells and the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size,num_layers,batch_size,keep_proba):\n",
    "    ''' Build LSTM cell.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        keep_prob: Scalar tensor (tf.placeholder) for the dropout keep probability\n",
    "        lstm_size: Size of the hidden layers in the LSTM cells\n",
    "        num_layers: Number of LSTM layers\n",
    "        batch_size: Batch size\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def build_cell(lstm_size,keep_proba):\n",
    "        cell=tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop=tf.contrib.rnn.DropoutWrapper(cell,output_keep_prob=keep_proba)\n",
    "        \n",
    "        return drop\n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size,keep_proba) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "    return cell,initial_state\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Output\n",
    "\n",
    "Here we'll create the output layer. We need to connect the output of the RNN cells to a full connected layer with a softmax output. The softmax output gives us a probability distribution we can use to predict the next character, so we want this layer to have size $C$, the number of classes/characters we have in our text.\n",
    "\n",
    "If our input has batch size $N$, number of steps $M$, and the hidden layer has $L$ hidden units, then the output is a 3D tensor with size $N \\times M \\times L$. The output of each LSTM cell has size $L$, we have $M$ of them, one for each sequence step, and we have $N$ sequences. So the total size is $N \\times M \\times L$\n",
    "\n",
    "We are using the same fully connected layer, the same weights, for each of the outputs. Then, to make things easier, we should reshape the outputs into a 2D tensor with shape $(M * N) \\times L$. That is, one row for each sequence and step, where the values of each row are the output from the LSTM cells. We get the LSTM output as a list, lstm_output. First we need to concatenate this whole list into one array with tf.concat. Then, reshape it (with tf.reshape) to size $(M * N) \\times L$.\n",
    "\n",
    "One we have the outputs reshaped, we can do the matrix multiplication with the weights. We need to wrap the weight and bias variables in a variable scope with tf.variable_scope(scope_name) because there are weights being created in the LSTM cells. TensorFlow will throw an error if the weights created here have the same names as the weights created in the LSTM cells, which they will be default. To avoid this, we wrap the variables in a variable scope so we can give them unique names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' Build a softmax layer, return the softmax output and logits.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        lstm_output: List of output tensors from the LSTM layer\n",
    "        in_size: Size of the input tensor, for example, size of the LSTM cells\n",
    "        out_size: Size of this softmax layer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one row for each step for each sequence.\n",
    "    # Concatenate lstm_output over axis 1 (the columns)\n",
    "    \n",
    "    seq_out = tf.concat(lstm_output,axis=1)\n",
    "    \n",
    "    # Reshape seq_output to a 2D tensor with lstm_size columns\n",
    "    \n",
    "    x = tf.reshape(seq_out,([-1,in_size]))\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w=tf.Variable(tf.truncated_normal((in_size,out_size),stddev=0.1))\n",
    "        softmax_b=tf.Variable(tf.zeros(out_size))\n",
    "        \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    logits = tf.matmul(x,softmax_w)+softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    out = tf.nn.softmax(logits,name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loss\n",
    "\n",
    "Next up is the training loss. We get the logits and targets and calculate the softmax cross-entropy loss. First we need to one-hot encode the targets, we're getting them as encoded characters. Then, reshape the one-hot targets so it's a 2D tensor with size $(M*N) \\times C$ where $C$ is the number of classes/characters we have. Remember that we reshaped the LSTM outputs and ran them through a fully connected layer with $C$ units. So our logits will also have size $(M*N) \\times C$.\n",
    "\n",
    "Then we run the logits and targets through tf.nn.softmax_cross_entropy_with_logits and find the mean to get the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    ''' Calculate the loss from the logits and the targets.\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        logits: Logits from final fully connected layer\n",
    "        targets: Targets for supervised learning\n",
    "        lstm_size: Number of LSTM hidden units\n",
    "        num_classes: Number of classes in targets\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot encode targets and reshape to match logits, one row per sequence per step\n",
    "    y_one_hot = tf.one_hot(targets,num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot,(logits.get_shape()))\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_reshaped,logits=logits)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "Here we build the optimizer. Normal RNNs have have issues gradients exploding and disappearing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' Build optmizer for training, using gradient clipping.\n",
    "    \n",
    "        Arguments:\n",
    "        loss: Network loss\n",
    "        learning_rate: Learning rate for optimizer\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    grads,_= tf.clip_by_global_norm(tf.gradients(loss,tvars),grad_clip)\n",
    "    train_op=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads,tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the network¶\n",
    "\n",
    "Now we can put all the pieces together and build a class for the network. To actually run data through the LSTM cells, we will use tf.nn.dynamic_rnn. This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as final_state so we can pass it to the first LSTM cell in the the next mini-batch run. For tf.nn.dynamic_rnn, we pass in the cell and initial state we get from build_lstm, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class charRNN:\n",
    "    \n",
    "    def __init__(self, numClasses,batch_size=64,num_steps=50,lstm_size=128,num_layers=2,learning_rate=0.001,\n",
    "                grad_clip=5,sampling=False):\n",
    "        \n",
    "        # When we're using this network for sampling later, we'll be passing in\n",
    "        # one character at a time, so providing an option for that\n",
    "        if sampling == True:\n",
    "            batch_size,num_steps=1,1\n",
    "        else:\n",
    "            batch_size,num_steps=batch_size,num_steps\n",
    "            \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_proba = build_inputs(batch_size,num_steps)\n",
    "        \n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size,num_layers,batch_size,self.keep_proba) \n",
    "        \n",
    "        ### Run the data through the RNN layers\n",
    "        # First, one-hot encode the input tokens\n",
    "        x_one_hot = tf.one_hot(self.inputs,numClasses)\n",
    "        \n",
    "        # Run each sequence step through the RNN with tf.nn.dynamic_rnn\n",
    "        \n",
    "        outputs, state = tf.nn.dynamic_rnn(cell,x_one_hot,initial_state=self.initial_state)\n",
    "        self.final_state =state\n",
    "        \n",
    "        # Get softmax predictions and logits\n",
    "        self.prediction,self.logits = build_output(outputs,lstm_size,numClasses)\n",
    "        \n",
    "        #Loss and optimizer (with gradient clipping )\n",
    "        self.loss = build_loss(self.logits,self.targets,lstm_size,numClasses)\n",
    "        self.optimizer = build_optimizer(self.loss,learning_rate,grad_clip)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for training\n",
    "This is typical training code, passing inputs and targets into the network, then running the optimizer. Here we also get back the final LSTM state for the mini-batch. Then, we pass that state back into the network so the next batch can continue the state from the previous batch. And every so often (set by save_every_n) I save a checkpoint.\n",
    "\n",
    "Here I'm saving checkpoints with the format\n",
    "\n",
    "i{iteration number}_l{# hidden layer units}.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 1...  Training loss: 4.4188...  4.9150 sec/batch\n",
      "Epoch: 1/20...  Training Step: 2...  Training loss: 4.3292...  4.4780 sec/batch\n",
      "Epoch: 1/20...  Training Step: 3...  Training loss: 3.8415...  4.7900 sec/batch\n",
      "Epoch: 1/20...  Training Step: 4...  Training loss: 5.7999...  4.7750 sec/batch\n",
      "Epoch: 1/20...  Training Step: 5...  Training loss: 4.1311...  4.7030 sec/batch\n",
      "Epoch: 1/20...  Training Step: 6...  Training loss: 3.8876...  4.4570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 7...  Training loss: 3.7852...  4.8420 sec/batch\n",
      "Epoch: 1/20...  Training Step: 8...  Training loss: 3.6761...  5.2670 sec/batch\n",
      "Epoch: 1/20...  Training Step: 9...  Training loss: 3.5410...  5.0290 sec/batch\n",
      "Epoch: 1/20...  Training Step: 10...  Training loss: 3.4720...  4.6490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 11...  Training loss: 3.3952...  4.9040 sec/batch\n",
      "Epoch: 1/20...  Training Step: 12...  Training loss: 3.3888...  4.6340 sec/batch\n",
      "Epoch: 1/20...  Training Step: 13...  Training loss: 3.3686...  5.0970 sec/batch\n",
      "Epoch: 1/20...  Training Step: 14...  Training loss: 3.3910...  4.9820 sec/batch\n",
      "Epoch: 1/20...  Training Step: 15...  Training loss: 3.3591...  4.7090 sec/batch\n",
      "Epoch: 1/20...  Training Step: 16...  Training loss: 3.3311...  5.4490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 17...  Training loss: 3.3139...  5.3720 sec/batch\n",
      "Epoch: 1/20...  Training Step: 18...  Training loss: 3.3297...  5.5490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 19...  Training loss: 3.2910...  5.2060 sec/batch\n",
      "Epoch: 1/20...  Training Step: 20...  Training loss: 3.2582...  4.7970 sec/batch\n",
      "Epoch: 1/20...  Training Step: 21...  Training loss: 3.2743...  4.7910 sec/batch\n",
      "Epoch: 1/20...  Training Step: 22...  Training loss: 3.2567...  4.8020 sec/batch\n",
      "Epoch: 1/20...  Training Step: 23...  Training loss: 3.2597...  5.1960 sec/batch\n",
      "Epoch: 1/20...  Training Step: 24...  Training loss: 3.2642...  4.8140 sec/batch\n",
      "Epoch: 1/20...  Training Step: 25...  Training loss: 3.2375...  4.6550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 26...  Training loss: 3.2495...  4.6130 sec/batch\n",
      "Epoch: 1/20...  Training Step: 27...  Training loss: 3.2437...  4.8590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 28...  Training loss: 3.2245...  4.6200 sec/batch\n",
      "Epoch: 1/20...  Training Step: 29...  Training loss: 3.2186...  4.6690 sec/batch\n",
      "Epoch: 1/20...  Training Step: 30...  Training loss: 3.2218...  4.5910 sec/batch\n",
      "Epoch: 1/20...  Training Step: 31...  Training loss: 3.2491...  4.5620 sec/batch\n",
      "Epoch: 1/20...  Training Step: 32...  Training loss: 3.2121...  4.6650 sec/batch\n",
      "Epoch: 1/20...  Training Step: 33...  Training loss: 3.1958...  4.5820 sec/batch\n",
      "Epoch: 1/20...  Training Step: 34...  Training loss: 3.2281...  4.6510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 35...  Training loss: 3.1980...  4.7160 sec/batch\n",
      "Epoch: 1/20...  Training Step: 36...  Training loss: 3.2050...  4.6610 sec/batch\n",
      "Epoch: 1/20...  Training Step: 37...  Training loss: 3.1832...  4.7240 sec/batch\n",
      "Epoch: 1/20...  Training Step: 38...  Training loss: 3.1789...  4.8350 sec/batch\n",
      "Epoch: 1/20...  Training Step: 39...  Training loss: 3.1802...  5.3460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 40...  Training loss: 3.1786...  5.8540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 41...  Training loss: 3.1654...  7.6880 sec/batch\n",
      "Epoch: 1/20...  Training Step: 42...  Training loss: 3.1701...  7.2200 sec/batch\n",
      "Epoch: 1/20...  Training Step: 43...  Training loss: 3.1631...  5.3960 sec/batch\n",
      "Epoch: 1/20...  Training Step: 44...  Training loss: 3.1619...  5.1830 sec/batch\n",
      "Epoch: 1/20...  Training Step: 45...  Training loss: 3.1517...  4.7120 sec/batch\n",
      "Epoch: 1/20...  Training Step: 46...  Training loss: 3.1734...  4.9610 sec/batch\n",
      "Epoch: 1/20...  Training Step: 47...  Training loss: 3.1726...  4.8530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 48...  Training loss: 3.1764...  5.1880 sec/batch\n",
      "Epoch: 1/20...  Training Step: 49...  Training loss: 3.1642...  4.8760 sec/batch\n",
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1704...  4.9280 sec/batch\n",
      "Epoch: 1/20...  Training Step: 51...  Training loss: 3.1599...  4.7160 sec/batch\n",
      "Epoch: 1/20...  Training Step: 52...  Training loss: 3.1465...  4.8210 sec/batch\n",
      "Epoch: 1/20...  Training Step: 53...  Training loss: 3.1567...  4.8020 sec/batch\n",
      "Epoch: 1/20...  Training Step: 54...  Training loss: 3.1454...  4.9930 sec/batch\n",
      "Epoch: 1/20...  Training Step: 55...  Training loss: 3.1589...  5.7050 sec/batch\n",
      "Epoch: 1/20...  Training Step: 56...  Training loss: 3.1277...  4.7620 sec/batch\n",
      "Epoch: 1/20...  Training Step: 57...  Training loss: 3.1490...  4.8170 sec/batch\n",
      "Epoch: 1/20...  Training Step: 58...  Training loss: 3.1441...  4.8460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 59...  Training loss: 3.1392...  4.7200 sec/batch\n",
      "Epoch: 1/20...  Training Step: 60...  Training loss: 3.1460...  5.5230 sec/batch\n",
      "Epoch: 1/20...  Training Step: 61...  Training loss: 3.1432...  5.2600 sec/batch\n",
      "Epoch: 1/20...  Training Step: 62...  Training loss: 3.1597...  4.9910 sec/batch\n",
      "Epoch: 1/20...  Training Step: 63...  Training loss: 3.1572...  4.6430 sec/batch\n",
      "Epoch: 1/20...  Training Step: 64...  Training loss: 3.1156...  5.0880 sec/batch\n",
      "Epoch: 1/20...  Training Step: 65...  Training loss: 3.1222...  5.1780 sec/batch\n",
      "Epoch: 1/20...  Training Step: 66...  Training loss: 3.1498...  5.0790 sec/batch\n",
      "Epoch: 1/20...  Training Step: 67...  Training loss: 3.1468...  4.9560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 68...  Training loss: 3.0964...  4.9030 sec/batch\n",
      "Epoch: 1/20...  Training Step: 69...  Training loss: 3.1209...  5.2240 sec/batch\n",
      "Epoch: 1/20...  Training Step: 70...  Training loss: 3.1353...  5.2510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 71...  Training loss: 3.1194...  5.5920 sec/batch\n",
      "Epoch: 1/20...  Training Step: 72...  Training loss: 3.1437...  6.2080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 73...  Training loss: 3.2876...  5.4220 sec/batch\n",
      "Epoch: 1/20...  Training Step: 74...  Training loss: 4.3821...  5.8820 sec/batch\n",
      "Epoch: 1/20...  Training Step: 75...  Training loss: 4.2477...  5.9700 sec/batch\n",
      "Epoch: 1/20...  Training Step: 76...  Training loss: 4.0100...  5.9570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 77...  Training loss: 3.7589...  6.4350 sec/batch\n",
      "Epoch: 1/20...  Training Step: 78...  Training loss: 3.5527...  6.0920 sec/batch\n",
      "Epoch: 1/20...  Training Step: 79...  Training loss: 3.2113...  6.3080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 80...  Training loss: 3.1633...  5.6560 sec/batch\n",
      "Epoch: 1/20...  Training Step: 81...  Training loss: 3.1516...  5.1120 sec/batch\n",
      "Epoch: 1/20...  Training Step: 82...  Training loss: 3.1676...  4.8230 sec/batch\n",
      "Epoch: 1/20...  Training Step: 83...  Training loss: 3.1790...  5.1400 sec/batch\n",
      "Epoch: 1/20...  Training Step: 84...  Training loss: 3.1644...  4.6540 sec/batch\n",
      "Epoch: 1/20...  Training Step: 85...  Training loss: 3.1465...  4.7700 sec/batch\n",
      "Epoch: 1/20...  Training Step: 86...  Training loss: 3.1547...  5.1570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 87...  Training loss: 3.1461...  5.3270 sec/batch\n",
      "Epoch: 1/20...  Training Step: 88...  Training loss: 3.1450...  5.0170 sec/batch\n",
      "Epoch: 1/20...  Training Step: 89...  Training loss: 3.1514...  4.9030 sec/batch\n",
      "Epoch: 1/20...  Training Step: 90...  Training loss: 3.1594...  4.7530 sec/batch\n",
      "Epoch: 1/20...  Training Step: 91...  Training loss: 3.1500...  4.8730 sec/batch\n",
      "Epoch: 1/20...  Training Step: 92...  Training loss: 3.1404...  4.8180 sec/batch\n",
      "Epoch: 1/20...  Training Step: 93...  Training loss: 3.1425...  4.7690 sec/batch\n",
      "Epoch: 1/20...  Training Step: 94...  Training loss: 3.1403...  4.7700 sec/batch\n",
      "Epoch: 1/20...  Training Step: 95...  Training loss: 3.1347...  4.9950 sec/batch\n",
      "Epoch: 1/20...  Training Step: 96...  Training loss: 3.1207...  5.9020 sec/batch\n",
      "Epoch: 1/20...  Training Step: 97...  Training loss: 3.1349...  5.9330 sec/batch\n",
      "Epoch: 1/20...  Training Step: 98...  Training loss: 3.1195...  5.6100 sec/batch\n",
      "Epoch: 1/20...  Training Step: 99...  Training loss: 3.1285...  5.4400 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.1149...  5.9870 sec/batch\n",
      "Epoch: 1/20...  Training Step: 101...  Training loss: 3.1181...  6.5290 sec/batch\n",
      "Epoch: 1/20...  Training Step: 102...  Training loss: 3.1108...  7.3170 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 103...  Training loss: 3.1137...  10.4080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 104...  Training loss: 3.1094...  10.4570 sec/batch\n",
      "Epoch: 1/20...  Training Step: 105...  Training loss: 3.1059...  7.8470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 106...  Training loss: 3.1035...  6.5900 sec/batch\n",
      "Epoch: 1/20...  Training Step: 107...  Training loss: 3.1085...  5.4590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 108...  Training loss: 3.1000...  5.3790 sec/batch\n",
      "Epoch: 1/20...  Training Step: 109...  Training loss: 3.1209...  5.1960 sec/batch\n",
      "Epoch: 1/20...  Training Step: 110...  Training loss: 3.0858...  5.2070 sec/batch\n",
      "Epoch: 1/20...  Training Step: 111...  Training loss: 3.0946...  5.2490 sec/batch\n",
      "Epoch: 1/20...  Training Step: 112...  Training loss: 3.0999...  5.3920 sec/batch\n",
      "Epoch: 1/20...  Training Step: 113...  Training loss: 3.0965...  5.1320 sec/batch\n",
      "Epoch: 1/20...  Training Step: 114...  Training loss: 3.0781...  5.0940 sec/batch\n",
      "Epoch: 1/20...  Training Step: 115...  Training loss: 3.0738...  5.0080 sec/batch\n",
      "Epoch: 1/20...  Training Step: 116...  Training loss: 3.0729...  5.1020 sec/batch\n",
      "Epoch: 1/20...  Training Step: 117...  Training loss: 3.0815...  5.0290 sec/batch\n",
      "Epoch: 1/20...  Training Step: 118...  Training loss: 3.0986...  5.0840 sec/batch\n",
      "Epoch: 1/20...  Training Step: 119...  Training loss: 3.0934...  5.0520 sec/batch\n",
      "Epoch: 1/20...  Training Step: 120...  Training loss: 3.0703...  5.0330 sec/batch\n",
      "Epoch: 1/20...  Training Step: 121...  Training loss: 3.1034...  5.0470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 122...  Training loss: 3.0830...  5.0890 sec/batch\n",
      "Epoch: 1/20...  Training Step: 123...  Training loss: 3.0720...  5.2740 sec/batch\n",
      "Epoch: 1/20...  Training Step: 124...  Training loss: 3.0756...  5.0350 sec/batch\n",
      "Epoch: 1/20...  Training Step: 125...  Training loss: 3.0456...  4.8970 sec/batch\n",
      "Epoch: 1/20...  Training Step: 126...  Training loss: 3.0314...  4.9590 sec/batch\n",
      "Epoch: 1/20...  Training Step: 127...  Training loss: 3.0508...  4.9460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 128...  Training loss: 3.0516...  5.1040 sec/batch\n",
      "Epoch: 1/20...  Training Step: 129...  Training loss: 3.0327...  5.4510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 130...  Training loss: 3.0347...  5.5990 sec/batch\n",
      "Epoch: 1/20...  Training Step: 131...  Training loss: 3.0367...  5.8420 sec/batch\n",
      "Epoch: 1/20...  Training Step: 132...  Training loss: 3.0108...  5.2600 sec/batch\n",
      "Epoch: 1/20...  Training Step: 133...  Training loss: 3.0159...  5.1800 sec/batch\n",
      "Epoch: 1/20...  Training Step: 134...  Training loss: 3.0090...  4.8180 sec/batch\n",
      "Epoch: 1/20...  Training Step: 135...  Training loss: 2.9729...  4.7510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 136...  Training loss: 2.9692...  4.6700 sec/batch\n",
      "Epoch: 1/20...  Training Step: 137...  Training loss: 2.9854...  4.6370 sec/batch\n",
      "Epoch: 1/20...  Training Step: 138...  Training loss: 2.9700...  4.7360 sec/batch\n",
      "Epoch: 1/20...  Training Step: 139...  Training loss: 2.9865...  5.1250 sec/batch\n",
      "Epoch: 1/20...  Training Step: 140...  Training loss: 2.9629...  5.4510 sec/batch\n",
      "Epoch: 1/20...  Training Step: 141...  Training loss: 2.9623...  5.3770 sec/batch\n",
      "Epoch: 1/20...  Training Step: 142...  Training loss: 2.9384...  5.4250 sec/batch\n",
      "Epoch: 1/20...  Training Step: 143...  Training loss: 2.9313...  5.4920 sec/batch\n",
      "Epoch: 1/20...  Training Step: 144...  Training loss: 2.9341...  5.8220 sec/batch\n",
      "Epoch: 1/20...  Training Step: 145...  Training loss: 2.9339...  4.9710 sec/batch\n",
      "Epoch: 1/20...  Training Step: 146...  Training loss: 2.9212...  5.1020 sec/batch\n",
      "Epoch: 1/20...  Training Step: 147...  Training loss: 2.9327...  6.5140 sec/batch\n",
      "Epoch: 1/20...  Training Step: 148...  Training loss: 2.9374...  5.4260 sec/batch\n",
      "Epoch: 1/20...  Training Step: 149...  Training loss: 2.8887...  6.2700 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 2.9138...  5.4720 sec/batch\n",
      "Epoch: 1/20...  Training Step: 151...  Training loss: 2.9513...  7.1300 sec/batch\n",
      "Epoch: 1/20...  Training Step: 152...  Training loss: 2.9216...  6.7710 sec/batch\n",
      "Epoch: 1/20...  Training Step: 153...  Training loss: 2.8983...  5.5860 sec/batch\n",
      "Epoch: 1/20...  Training Step: 154...  Training loss: 2.8810...  5.3300 sec/batch\n",
      "Epoch: 1/20...  Training Step: 155...  Training loss: 2.8577...  5.0420 sec/batch\n",
      "Epoch: 1/20...  Training Step: 156...  Training loss: 2.8588...  5.1030 sec/batch\n",
      "Epoch: 1/20...  Training Step: 157...  Training loss: 2.8436...  4.9980 sec/batch\n",
      "Epoch: 1/20...  Training Step: 158...  Training loss: 2.8365...  4.8100 sec/batch\n",
      "Epoch: 1/20...  Training Step: 159...  Training loss: 2.8060...  4.8380 sec/batch\n",
      "Epoch: 1/20...  Training Step: 160...  Training loss: 2.8207...  4.8470 sec/batch\n",
      "Epoch: 1/20...  Training Step: 161...  Training loss: 2.8210...  5.1420 sec/batch\n",
      "Epoch: 1/20...  Training Step: 162...  Training loss: 2.7999...  5.0320 sec/batch\n",
      "Epoch: 1/20...  Training Step: 163...  Training loss: 2.8002...  5.6460 sec/batch\n",
      "Epoch: 1/20...  Training Step: 164...  Training loss: 2.7978...  6.0350 sec/batch\n",
      "Epoch: 1/20...  Training Step: 165...  Training loss: 2.7842...  5.0670 sec/batch\n",
      "Epoch: 1/20...  Training Step: 166...  Training loss: 2.7692...  5.0270 sec/batch\n",
      "Epoch: 1/20...  Training Step: 167...  Training loss: 2.7713...  4.9120 sec/batch\n",
      "Epoch: 1/20...  Training Step: 168...  Training loss: 2.7707...  4.9450 sec/batch\n",
      "Epoch: 1/20...  Training Step: 169...  Training loss: 2.7745...  5.0720 sec/batch\n",
      "Epoch: 1/20...  Training Step: 170...  Training loss: 2.7396...  5.3400 sec/batch\n",
      "Epoch: 1/20...  Training Step: 171...  Training loss: 2.7689...  5.2620 sec/batch\n",
      "Epoch: 1/20...  Training Step: 172...  Training loss: 2.7654...  5.7150 sec/batch\n",
      "Epoch: 1/20...  Training Step: 173...  Training loss: 2.7789...  6.2130 sec/batch\n",
      "Epoch: 1/20...  Training Step: 174...  Training loss: 2.7614...  7.2780 sec/batch\n",
      "Epoch: 1/20...  Training Step: 175...  Training loss: 2.7365...  6.8630 sec/batch\n",
      "Epoch: 1/20...  Training Step: 176...  Training loss: 2.6997...  6.8440 sec/batch\n",
      "Epoch: 1/20...  Training Step: 177...  Training loss: 2.7029...  5.6890 sec/batch\n",
      "Epoch: 1/20...  Training Step: 178...  Training loss: 2.6652...  5.2000 sec/batch\n",
      "Epoch: 1/20...  Training Step: 179...  Training loss: 2.6637...  5.1010 sec/batch\n",
      "Epoch: 1/20...  Training Step: 180...  Training loss: 2.6569...  5.1790 sec/batch\n",
      "Epoch: 1/20...  Training Step: 181...  Training loss: 2.6667...  5.4050 sec/batch\n",
      "Epoch: 1/20...  Training Step: 182...  Training loss: 2.6552...  5.2850 sec/batch\n",
      "Epoch: 1/20...  Training Step: 183...  Training loss: 2.6371...  5.0440 sec/batch\n",
      "Epoch: 1/20...  Training Step: 184...  Training loss: 2.6562...  5.2670 sec/batch\n",
      "Epoch: 1/20...  Training Step: 185...  Training loss: 2.6782...  5.3550 sec/batch\n",
      "Epoch: 1/20...  Training Step: 186...  Training loss: 2.6318...  4.9390 sec/batch\n",
      "Epoch: 1/20...  Training Step: 187...  Training loss: 2.6873...  5.3670 sec/batch\n",
      "Epoch: 1/20...  Training Step: 188...  Training loss: 2.6172...  4.9680 sec/batch\n",
      "Epoch: 1/20...  Training Step: 189...  Training loss: 2.6186...  5.1610 sec/batch\n",
      "Epoch: 1/20...  Training Step: 190...  Training loss: 2.6183...  5.4330 sec/batch\n",
      "Epoch: 1/20...  Training Step: 191...  Training loss: 2.6277...  4.9680 sec/batch\n",
      "Epoch: 1/20...  Training Step: 192...  Training loss: 2.5794...  4.9100 sec/batch\n",
      "Epoch: 1/20...  Training Step: 193...  Training loss: 2.6071...  5.1130 sec/batch\n",
      "Epoch: 1/20...  Training Step: 194...  Training loss: 2.5932...  4.9690 sec/batch\n",
      "Epoch: 1/20...  Training Step: 195...  Training loss: 2.5730...  4.8760 sec/batch\n",
      "Epoch: 1/20...  Training Step: 196...  Training loss: 2.5773...  5.0210 sec/batch\n",
      "Epoch: 1/20...  Training Step: 197...  Training loss: 2.5667...  5.2410 sec/batch\n",
      "Epoch: 1/20...  Training Step: 198...  Training loss: 2.5547...  5.3340 sec/batch\n",
      "Epoch: 2/20...  Training Step: 199...  Training loss: 2.6489...  5.6530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 2.5414...  5.2720 sec/batch\n",
      "Epoch: 2/20...  Training Step: 201...  Training loss: 2.5400...  5.4250 sec/batch\n",
      "Epoch: 2/20...  Training Step: 202...  Training loss: 2.5492...  4.8390 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 203...  Training loss: 2.5478...  4.8660 sec/batch\n",
      "Epoch: 2/20...  Training Step: 204...  Training loss: 2.5408...  5.0830 sec/batch\n",
      "Epoch: 2/20...  Training Step: 205...  Training loss: 2.5375...  5.8410 sec/batch\n",
      "Epoch: 2/20...  Training Step: 206...  Training loss: 2.5493...  5.2760 sec/batch\n",
      "Epoch: 2/20...  Training Step: 207...  Training loss: 2.5564...  5.2060 sec/batch\n",
      "Epoch: 2/20...  Training Step: 208...  Training loss: 2.5209...  5.2820 sec/batch\n",
      "Epoch: 2/20...  Training Step: 209...  Training loss: 2.5222...  5.3930 sec/batch\n",
      "Epoch: 2/20...  Training Step: 210...  Training loss: 2.5318...  5.7850 sec/batch\n",
      "Epoch: 2/20...  Training Step: 211...  Training loss: 2.5155...  5.2940 sec/batch\n",
      "Epoch: 2/20...  Training Step: 212...  Training loss: 2.5731...  5.3500 sec/batch\n",
      "Epoch: 2/20...  Training Step: 213...  Training loss: 2.5400...  5.5410 sec/batch\n",
      "Epoch: 2/20...  Training Step: 214...  Training loss: 2.5193...  5.1910 sec/batch\n",
      "Epoch: 2/20...  Training Step: 215...  Training loss: 2.5208...  5.1790 sec/batch\n",
      "Epoch: 2/20...  Training Step: 216...  Training loss: 2.5628...  5.3130 sec/batch\n",
      "Epoch: 2/20...  Training Step: 217...  Training loss: 2.5225...  5.0520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 218...  Training loss: 2.4913...  5.0880 sec/batch\n",
      "Epoch: 2/20...  Training Step: 219...  Training loss: 2.4936...  5.5200 sec/batch\n",
      "Epoch: 2/20...  Training Step: 220...  Training loss: 2.5294...  5.2480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 221...  Training loss: 2.5117...  5.4690 sec/batch\n",
      "Epoch: 2/20...  Training Step: 222...  Training loss: 2.4920...  5.4220 sec/batch\n",
      "Epoch: 2/20...  Training Step: 223...  Training loss: 2.4782...  4.8740 sec/batch\n",
      "Epoch: 2/20...  Training Step: 224...  Training loss: 2.4846...  5.0400 sec/batch\n",
      "Epoch: 2/20...  Training Step: 225...  Training loss: 2.4748...  4.8040 sec/batch\n",
      "Epoch: 2/20...  Training Step: 226...  Training loss: 2.4717...  5.1870 sec/batch\n",
      "Epoch: 2/20...  Training Step: 227...  Training loss: 2.4840...  4.9740 sec/batch\n",
      "Epoch: 2/20...  Training Step: 228...  Training loss: 2.4751...  4.9890 sec/batch\n",
      "Epoch: 2/20...  Training Step: 229...  Training loss: 2.4943...  5.1700 sec/batch\n",
      "Epoch: 2/20...  Training Step: 230...  Training loss: 2.4574...  4.6360 sec/batch\n",
      "Epoch: 2/20...  Training Step: 231...  Training loss: 2.4489...  4.9890 sec/batch\n",
      "Epoch: 2/20...  Training Step: 232...  Training loss: 2.4601...  4.9140 sec/batch\n",
      "Epoch: 2/20...  Training Step: 233...  Training loss: 2.4402...  5.0300 sec/batch\n",
      "Epoch: 2/20...  Training Step: 234...  Training loss: 2.4682...  5.0560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 235...  Training loss: 2.4357...  5.3750 sec/batch\n",
      "Epoch: 2/20...  Training Step: 236...  Training loss: 2.4210...  5.1850 sec/batch\n",
      "Epoch: 2/20...  Training Step: 237...  Training loss: 2.4280...  6.3590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 238...  Training loss: 2.4216...  4.9390 sec/batch\n",
      "Epoch: 2/20...  Training Step: 239...  Training loss: 2.4228...  5.2720 sec/batch\n",
      "Epoch: 2/20...  Training Step: 240...  Training loss: 2.4190...  5.1060 sec/batch\n",
      "Epoch: 2/20...  Training Step: 241...  Training loss: 2.4134...  4.9130 sec/batch\n",
      "Epoch: 2/20...  Training Step: 242...  Training loss: 2.4088...  5.8710 sec/batch\n",
      "Epoch: 2/20...  Training Step: 243...  Training loss: 2.4194...  5.6670 sec/batch\n",
      "Epoch: 2/20...  Training Step: 244...  Training loss: 2.3818...  5.8220 sec/batch\n",
      "Epoch: 2/20...  Training Step: 245...  Training loss: 2.4273...  5.1390 sec/batch\n",
      "Epoch: 2/20...  Training Step: 246...  Training loss: 2.4121...  5.1970 sec/batch\n",
      "Epoch: 2/20...  Training Step: 247...  Training loss: 2.4044...  4.7700 sec/batch\n",
      "Epoch: 2/20...  Training Step: 248...  Training loss: 2.4330...  5.0990 sec/batch\n",
      "Epoch: 2/20...  Training Step: 249...  Training loss: 2.3930...  5.0620 sec/batch\n",
      "Epoch: 2/20...  Training Step: 250...  Training loss: 2.4159...  5.0650 sec/batch\n",
      "Epoch: 2/20...  Training Step: 251...  Training loss: 2.4024...  5.1260 sec/batch\n",
      "Epoch: 2/20...  Training Step: 252...  Training loss: 2.3919...  5.0480 sec/batch\n",
      "Epoch: 2/20...  Training Step: 253...  Training loss: 2.3822...  5.0730 sec/batch\n",
      "Epoch: 2/20...  Training Step: 254...  Training loss: 2.4055...  5.0900 sec/batch\n",
      "Epoch: 2/20...  Training Step: 255...  Training loss: 2.3827...  5.3820 sec/batch\n",
      "Epoch: 2/20...  Training Step: 256...  Training loss: 2.3824...  4.8340 sec/batch\n",
      "Epoch: 2/20...  Training Step: 257...  Training loss: 2.3785...  4.6870 sec/batch\n",
      "Epoch: 2/20...  Training Step: 258...  Training loss: 2.4009...  4.7850 sec/batch\n",
      "Epoch: 2/20...  Training Step: 259...  Training loss: 2.3812...  4.7590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 260...  Training loss: 2.3919...  4.8020 sec/batch\n",
      "Epoch: 2/20...  Training Step: 261...  Training loss: 2.4118...  5.0180 sec/batch\n",
      "Epoch: 2/20...  Training Step: 262...  Training loss: 2.3741...  4.7760 sec/batch\n",
      "Epoch: 2/20...  Training Step: 263...  Training loss: 2.3711...  5.2510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 264...  Training loss: 2.3973...  5.3070 sec/batch\n",
      "Epoch: 2/20...  Training Step: 265...  Training loss: 2.3784...  5.1630 sec/batch\n",
      "Epoch: 2/20...  Training Step: 266...  Training loss: 2.3376...  5.6170 sec/batch\n",
      "Epoch: 2/20...  Training Step: 267...  Training loss: 2.3512...  5.2860 sec/batch\n",
      "Epoch: 2/20...  Training Step: 268...  Training loss: 2.3691...  5.0660 sec/batch\n",
      "Epoch: 2/20...  Training Step: 269...  Training loss: 2.3830...  4.7910 sec/batch\n",
      "Epoch: 2/20...  Training Step: 270...  Training loss: 2.3680...  4.7240 sec/batch\n",
      "Epoch: 2/20...  Training Step: 271...  Training loss: 2.3657...  4.8780 sec/batch\n",
      "Epoch: 2/20...  Training Step: 272...  Training loss: 2.3469...  4.6750 sec/batch\n",
      "Epoch: 2/20...  Training Step: 273...  Training loss: 2.3444...  4.6240 sec/batch\n",
      "Epoch: 2/20...  Training Step: 274...  Training loss: 2.3935...  4.6910 sec/batch\n",
      "Epoch: 2/20...  Training Step: 275...  Training loss: 2.3486...  5.1400 sec/batch\n",
      "Epoch: 2/20...  Training Step: 276...  Training loss: 2.3669...  4.7120 sec/batch\n",
      "Epoch: 2/20...  Training Step: 277...  Training loss: 2.3349...  4.7020 sec/batch\n",
      "Epoch: 2/20...  Training Step: 278...  Training loss: 2.3358...  4.6570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 279...  Training loss: 2.3123...  4.6560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 280...  Training loss: 2.3473...  4.6300 sec/batch\n",
      "Epoch: 2/20...  Training Step: 281...  Training loss: 2.3301...  4.6790 sec/batch\n",
      "Epoch: 2/20...  Training Step: 282...  Training loss: 2.3166...  4.6350 sec/batch\n",
      "Epoch: 2/20...  Training Step: 283...  Training loss: 2.2910...  4.6320 sec/batch\n",
      "Epoch: 2/20...  Training Step: 284...  Training loss: 2.3150...  4.8060 sec/batch\n",
      "Epoch: 2/20...  Training Step: 285...  Training loss: 2.3242...  4.9330 sec/batch\n",
      "Epoch: 2/20...  Training Step: 286...  Training loss: 2.3221...  5.0440 sec/batch\n",
      "Epoch: 2/20...  Training Step: 287...  Training loss: 2.2994...  5.0530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 288...  Training loss: 2.3251...  23.5960 sec/batch\n",
      "Epoch: 2/20...  Training Step: 289...  Training loss: 2.3020...  19.5590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 290...  Training loss: 2.3203...  6.8690 sec/batch\n",
      "Epoch: 2/20...  Training Step: 291...  Training loss: 2.2967...  5.3230 sec/batch\n",
      "Epoch: 2/20...  Training Step: 292...  Training loss: 2.3027...  5.3170 sec/batch\n",
      "Epoch: 2/20...  Training Step: 293...  Training loss: 2.2892...  4.8790 sec/batch\n",
      "Epoch: 2/20...  Training Step: 294...  Training loss: 2.3024...  4.9980 sec/batch\n",
      "Epoch: 2/20...  Training Step: 295...  Training loss: 2.2956...  5.9200 sec/batch\n",
      "Epoch: 2/20...  Training Step: 296...  Training loss: 2.2966...  7.1800 sec/batch\n",
      "Epoch: 2/20...  Training Step: 297...  Training loss: 2.2958...  6.3680 sec/batch\n",
      "Epoch: 2/20...  Training Step: 298...  Training loss: 2.2899...  5.0860 sec/batch\n",
      "Epoch: 2/20...  Training Step: 299...  Training loss: 2.3165...  5.7240 sec/batch\n",
      "Epoch: 2/20...  Training Step: 300...  Training loss: 2.2889...  5.1370 sec/batch\n",
      "Epoch: 2/20...  Training Step: 301...  Training loss: 2.2820...  6.1110 sec/batch\n",
      "Epoch: 2/20...  Training Step: 302...  Training loss: 2.2840...  7.4920 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/20...  Training Step: 303...  Training loss: 2.2743...  5.6030 sec/batch\n",
      "Epoch: 2/20...  Training Step: 304...  Training loss: 2.2863...  4.9520 sec/batch\n",
      "Epoch: 2/20...  Training Step: 305...  Training loss: 2.2768...  5.7040 sec/batch\n",
      "Epoch: 2/20...  Training Step: 306...  Training loss: 2.3070...  5.8740 sec/batch\n",
      "Epoch: 2/20...  Training Step: 307...  Training loss: 2.3012...  6.4960 sec/batch\n",
      "Epoch: 2/20...  Training Step: 308...  Training loss: 2.2620...  5.8380 sec/batch\n",
      "Epoch: 2/20...  Training Step: 309...  Training loss: 2.2873...  5.6310 sec/batch\n",
      "Epoch: 2/20...  Training Step: 310...  Training loss: 2.2909...  5.6540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 311...  Training loss: 2.2732...  5.1090 sec/batch\n",
      "Epoch: 2/20...  Training Step: 312...  Training loss: 2.2629...  6.0100 sec/batch\n",
      "Epoch: 2/20...  Training Step: 313...  Training loss: 2.2572...  5.7580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 314...  Training loss: 2.2316...  5.6040 sec/batch\n",
      "Epoch: 2/20...  Training Step: 315...  Training loss: 2.2713...  5.4900 sec/batch\n",
      "Epoch: 2/20...  Training Step: 316...  Training loss: 2.2692...  6.2930 sec/batch\n",
      "Epoch: 2/20...  Training Step: 317...  Training loss: 2.2821...  5.9610 sec/batch\n",
      "Epoch: 2/20...  Training Step: 318...  Training loss: 2.2724...  5.9800 sec/batch\n",
      "Epoch: 2/20...  Training Step: 319...  Training loss: 2.2867...  6.4850 sec/batch\n",
      "Epoch: 2/20...  Training Step: 320...  Training loss: 2.2475...  6.9590 sec/batch\n",
      "Epoch: 2/20...  Training Step: 321...  Training loss: 2.2419...  5.4720 sec/batch\n",
      "Epoch: 2/20...  Training Step: 322...  Training loss: 2.2753...  5.3060 sec/batch\n",
      "Epoch: 2/20...  Training Step: 323...  Training loss: 2.2553...  6.1470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 324...  Training loss: 2.2340...  6.2240 sec/batch\n",
      "Epoch: 2/20...  Training Step: 325...  Training loss: 2.2643...  5.9100 sec/batch\n",
      "Epoch: 2/20...  Training Step: 326...  Training loss: 2.2655...  6.0570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 327...  Training loss: 2.2508...  6.3220 sec/batch\n",
      "Epoch: 2/20...  Training Step: 328...  Training loss: 2.2559...  6.5240 sec/batch\n",
      "Epoch: 2/20...  Training Step: 329...  Training loss: 2.2393...  4.9540 sec/batch\n",
      "Epoch: 2/20...  Training Step: 330...  Training loss: 2.2154...  7.5020 sec/batch\n",
      "Epoch: 2/20...  Training Step: 331...  Training loss: 2.2555...  6.2570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 332...  Training loss: 2.2595...  6.5010 sec/batch\n",
      "Epoch: 2/20...  Training Step: 333...  Training loss: 2.2343...  6.8570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 334...  Training loss: 2.2380...  6.2200 sec/batch\n",
      "Epoch: 2/20...  Training Step: 335...  Training loss: 2.2363...  5.6720 sec/batch\n",
      "Epoch: 2/20...  Training Step: 336...  Training loss: 2.2468...  5.6100 sec/batch\n",
      "Epoch: 2/20...  Training Step: 337...  Training loss: 2.2753...  8.3800 sec/batch\n",
      "Epoch: 2/20...  Training Step: 338...  Training loss: 2.2197...  6.6010 sec/batch\n",
      "Epoch: 2/20...  Training Step: 339...  Training loss: 2.2457...  5.9100 sec/batch\n",
      "Epoch: 2/20...  Training Step: 340...  Training loss: 2.2194...  5.1290 sec/batch\n",
      "Epoch: 2/20...  Training Step: 341...  Training loss: 2.2214...  5.2920 sec/batch\n",
      "Epoch: 2/20...  Training Step: 342...  Training loss: 2.2184...  5.9990 sec/batch\n",
      "Epoch: 2/20...  Training Step: 343...  Training loss: 2.2196...  6.1610 sec/batch\n",
      "Epoch: 2/20...  Training Step: 344...  Training loss: 2.2407...  6.0160 sec/batch\n",
      "Epoch: 2/20...  Training Step: 345...  Training loss: 2.2296...  6.9780 sec/batch\n",
      "Epoch: 2/20...  Training Step: 346...  Training loss: 2.2468...  6.0510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 347...  Training loss: 2.2202...  6.2530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 348...  Training loss: 2.2019...  5.7850 sec/batch\n",
      "Epoch: 2/20...  Training Step: 349...  Training loss: 2.2235...  5.6180 sec/batch\n",
      "Epoch: 2/20...  Training Step: 350...  Training loss: 2.2600...  5.1160 sec/batch\n",
      "Epoch: 2/20...  Training Step: 351...  Training loss: 2.2164...  5.2710 sec/batch\n",
      "Epoch: 2/20...  Training Step: 352...  Training loss: 2.2193...  4.7800 sec/batch\n",
      "Epoch: 2/20...  Training Step: 353...  Training loss: 2.2023...  5.2220 sec/batch\n",
      "Epoch: 2/20...  Training Step: 354...  Training loss: 2.2038...  5.5210 sec/batch\n",
      "Epoch: 2/20...  Training Step: 355...  Training loss: 2.1963...  5.2380 sec/batch\n",
      "Epoch: 2/20...  Training Step: 356...  Training loss: 2.1985...  5.4580 sec/batch\n",
      "Epoch: 2/20...  Training Step: 357...  Training loss: 2.1787...  5.0060 sec/batch\n",
      "Epoch: 2/20...  Training Step: 358...  Training loss: 2.2482...  4.8740 sec/batch\n",
      "Epoch: 2/20...  Training Step: 359...  Training loss: 2.2019...  5.4410 sec/batch\n",
      "Epoch: 2/20...  Training Step: 360...  Training loss: 2.1842...  5.1230 sec/batch\n",
      "Epoch: 2/20...  Training Step: 361...  Training loss: 2.1919...  5.7370 sec/batch\n",
      "Epoch: 2/20...  Training Step: 362...  Training loss: 2.2032...  5.7380 sec/batch\n",
      "Epoch: 2/20...  Training Step: 363...  Training loss: 2.1966...  6.0930 sec/batch\n",
      "Epoch: 2/20...  Training Step: 364...  Training loss: 2.1911...  6.4450 sec/batch\n",
      "Epoch: 2/20...  Training Step: 365...  Training loss: 2.2090...  6.4430 sec/batch\n",
      "Epoch: 2/20...  Training Step: 366...  Training loss: 2.2156...  5.9900 sec/batch\n",
      "Epoch: 2/20...  Training Step: 367...  Training loss: 2.1897...  7.7700 sec/batch\n",
      "Epoch: 2/20...  Training Step: 368...  Training loss: 2.1755...  6.5510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 369...  Training loss: 2.1614...  5.4510 sec/batch\n",
      "Epoch: 2/20...  Training Step: 370...  Training loss: 2.1856...  5.2110 sec/batch\n",
      "Epoch: 2/20...  Training Step: 371...  Training loss: 2.2018...  5.6810 sec/batch\n",
      "Epoch: 2/20...  Training Step: 372...  Training loss: 2.1918...  5.3020 sec/batch\n",
      "Epoch: 2/20...  Training Step: 373...  Training loss: 2.1843...  4.9000 sec/batch\n",
      "Epoch: 2/20...  Training Step: 374...  Training loss: 2.1661...  5.0610 sec/batch\n",
      "Epoch: 2/20...  Training Step: 375...  Training loss: 2.1652...  5.7410 sec/batch\n",
      "Epoch: 2/20...  Training Step: 376...  Training loss: 2.1686...  5.9460 sec/batch\n",
      "Epoch: 2/20...  Training Step: 377...  Training loss: 2.1527...  5.7800 sec/batch\n",
      "Epoch: 2/20...  Training Step: 378...  Training loss: 2.1327...  5.8560 sec/batch\n",
      "Epoch: 2/20...  Training Step: 379...  Training loss: 2.1426...  5.6190 sec/batch\n",
      "Epoch: 2/20...  Training Step: 380...  Training loss: 2.1698...  4.7720 sec/batch\n",
      "Epoch: 2/20...  Training Step: 381...  Training loss: 2.1776...  5.1080 sec/batch\n",
      "Epoch: 2/20...  Training Step: 382...  Training loss: 2.1771...  6.1140 sec/batch\n",
      "Epoch: 2/20...  Training Step: 383...  Training loss: 2.1780...  6.0750 sec/batch\n",
      "Epoch: 2/20...  Training Step: 384...  Training loss: 2.1582...  6.3120 sec/batch\n",
      "Epoch: 2/20...  Training Step: 385...  Training loss: 2.1518...  5.7300 sec/batch\n",
      "Epoch: 2/20...  Training Step: 386...  Training loss: 2.1334...  5.2080 sec/batch\n",
      "Epoch: 2/20...  Training Step: 387...  Training loss: 2.1413...  5.6190 sec/batch\n",
      "Epoch: 2/20...  Training Step: 388...  Training loss: 2.1473...  5.6530 sec/batch\n",
      "Epoch: 2/20...  Training Step: 389...  Training loss: 2.1625...  5.0800 sec/batch\n",
      "Epoch: 2/20...  Training Step: 390...  Training loss: 2.1240...  5.7400 sec/batch\n",
      "Epoch: 2/20...  Training Step: 391...  Training loss: 2.1606...  5.4850 sec/batch\n",
      "Epoch: 2/20...  Training Step: 392...  Training loss: 2.1477...  5.8470 sec/batch\n",
      "Epoch: 2/20...  Training Step: 393...  Training loss: 2.1243...  5.8100 sec/batch\n",
      "Epoch: 2/20...  Training Step: 394...  Training loss: 2.1459...  5.4740 sec/batch\n",
      "Epoch: 2/20...  Training Step: 395...  Training loss: 2.1440...  5.0570 sec/batch\n",
      "Epoch: 2/20...  Training Step: 396...  Training loss: 2.1286...  5.2740 sec/batch\n",
      "Epoch: 3/20...  Training Step: 397...  Training loss: 2.2296...  5.0820 sec/batch\n",
      "Epoch: 3/20...  Training Step: 398...  Training loss: 2.1117...  5.2690 sec/batch\n",
      "Epoch: 3/20...  Training Step: 399...  Training loss: 2.1196...  5.4760 sec/batch\n",
      "Epoch: 3/20...  Training Step: 400...  Training loss: 2.1211...  6.3290 sec/batch\n",
      "Epoch: 3/20...  Training Step: 401...  Training loss: 2.1278...  6.0380 sec/batch\n",
      "Epoch: 3/20...  Training Step: 402...  Training loss: 2.0964...  5.7460 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 403...  Training loss: 2.1209...  6.3160 sec/batch\n",
      "Epoch: 3/20...  Training Step: 404...  Training loss: 2.1275...  4.9310 sec/batch\n",
      "Epoch: 3/20...  Training Step: 405...  Training loss: 2.1617...  4.9740 sec/batch\n",
      "Epoch: 3/20...  Training Step: 406...  Training loss: 2.1181...  5.4010 sec/batch\n",
      "Epoch: 3/20...  Training Step: 407...  Training loss: 2.1062...  5.2580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 408...  Training loss: 2.1024...  5.4820 sec/batch\n",
      "Epoch: 3/20...  Training Step: 409...  Training loss: 2.1209...  5.6280 sec/batch\n",
      "Epoch: 3/20...  Training Step: 410...  Training loss: 2.1517...  5.6840 sec/batch\n",
      "Epoch: 3/20...  Training Step: 411...  Training loss: 2.1171...  5.3200 sec/batch\n",
      "Epoch: 3/20...  Training Step: 412...  Training loss: 2.1019...  5.0390 sec/batch\n",
      "Epoch: 3/20...  Training Step: 413...  Training loss: 2.1113...  5.4660 sec/batch\n",
      "Epoch: 3/20...  Training Step: 414...  Training loss: 2.1513...  5.1390 sec/batch\n",
      "Epoch: 3/20...  Training Step: 415...  Training loss: 2.1143...  5.5930 sec/batch\n",
      "Epoch: 3/20...  Training Step: 416...  Training loss: 2.1052...  4.7060 sec/batch\n",
      "Epoch: 3/20...  Training Step: 417...  Training loss: 2.1034...  5.8040 sec/batch\n",
      "Epoch: 3/20...  Training Step: 418...  Training loss: 2.1516...  7.7410 sec/batch\n",
      "Epoch: 3/20...  Training Step: 419...  Training loss: 2.1102...  6.2060 sec/batch\n",
      "Epoch: 3/20...  Training Step: 420...  Training loss: 2.0941...  5.2600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 421...  Training loss: 2.1072...  5.5620 sec/batch\n",
      "Epoch: 3/20...  Training Step: 422...  Training loss: 2.0825...  5.5580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 423...  Training loss: 2.0892...  5.4280 sec/batch\n",
      "Epoch: 3/20...  Training Step: 424...  Training loss: 2.1007...  5.5280 sec/batch\n",
      "Epoch: 3/20...  Training Step: 425...  Training loss: 2.1313...  5.9590 sec/batch\n",
      "Epoch: 3/20...  Training Step: 426...  Training loss: 2.1134...  5.2170 sec/batch\n",
      "Epoch: 3/20...  Training Step: 427...  Training loss: 2.1004...  5.0450 sec/batch\n",
      "Epoch: 3/20...  Training Step: 428...  Training loss: 2.0805...  5.1900 sec/batch\n",
      "Epoch: 3/20...  Training Step: 429...  Training loss: 2.0936...  4.8580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 430...  Training loss: 2.1346...  5.2640 sec/batch\n",
      "Epoch: 3/20...  Training Step: 431...  Training loss: 2.0766...  5.6650 sec/batch\n",
      "Epoch: 3/20...  Training Step: 432...  Training loss: 2.0840...  5.8600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 433...  Training loss: 2.0846...  7.4150 sec/batch\n",
      "Epoch: 3/20...  Training Step: 434...  Training loss: 2.0616...  6.1040 sec/batch\n",
      "Epoch: 3/20...  Training Step: 435...  Training loss: 2.0492...  5.3260 sec/batch\n",
      "Epoch: 3/20...  Training Step: 436...  Training loss: 2.0571...  4.9460 sec/batch\n",
      "Epoch: 3/20...  Training Step: 437...  Training loss: 2.0670...  4.7660 sec/batch\n",
      "Epoch: 3/20...  Training Step: 438...  Training loss: 2.0796...  5.9240 sec/batch\n",
      "Epoch: 3/20...  Training Step: 439...  Training loss: 2.0593...  5.0890 sec/batch\n",
      "Epoch: 3/20...  Training Step: 440...  Training loss: 2.0533...  5.3680 sec/batch\n",
      "Epoch: 3/20...  Training Step: 441...  Training loss: 2.0808...  6.8550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 442...  Training loss: 2.0193...  5.8730 sec/batch\n",
      "Epoch: 3/20...  Training Step: 443...  Training loss: 2.0798...  5.6350 sec/batch\n",
      "Epoch: 3/20...  Training Step: 444...  Training loss: 2.0594...  4.8820 sec/batch\n",
      "Epoch: 3/20...  Training Step: 445...  Training loss: 2.0608...  5.2140 sec/batch\n",
      "Epoch: 3/20...  Training Step: 446...  Training loss: 2.1102...  5.3550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 447...  Training loss: 2.0248...  4.8280 sec/batch\n",
      "Epoch: 3/20...  Training Step: 448...  Training loss: 2.1068...  6.3220 sec/batch\n",
      "Epoch: 3/20...  Training Step: 449...  Training loss: 2.0477...  5.6500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 450...  Training loss: 2.0525...  5.2160 sec/batch\n",
      "Epoch: 3/20...  Training Step: 451...  Training loss: 2.0489...  5.3810 sec/batch\n",
      "Epoch: 3/20...  Training Step: 452...  Training loss: 2.0668...  4.9850 sec/batch\n",
      "Epoch: 3/20...  Training Step: 453...  Training loss: 2.0697...  5.5930 sec/batch\n",
      "Epoch: 3/20...  Training Step: 454...  Training loss: 2.0517...  6.5430 sec/batch\n",
      "Epoch: 3/20...  Training Step: 455...  Training loss: 2.0410...  6.4860 sec/batch\n",
      "Epoch: 3/20...  Training Step: 456...  Training loss: 2.0880...  5.3630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 457...  Training loss: 2.0561...  5.4910 sec/batch\n",
      "Epoch: 3/20...  Training Step: 458...  Training loss: 2.0902...  6.0580 sec/batch\n",
      "Epoch: 3/20...  Training Step: 459...  Training loss: 2.0769...  4.9550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 460...  Training loss: 2.0520...  4.8560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 461...  Training loss: 2.0411...  5.2870 sec/batch\n",
      "Epoch: 3/20...  Training Step: 462...  Training loss: 2.0846...  5.2940 sec/batch\n",
      "Epoch: 3/20...  Training Step: 463...  Training loss: 2.0588...  4.7300 sec/batch\n",
      "Epoch: 3/20...  Training Step: 464...  Training loss: 2.0222...  6.2280 sec/batch\n",
      "Epoch: 3/20...  Training Step: 465...  Training loss: 2.0339...  5.4060 sec/batch\n",
      "Epoch: 3/20...  Training Step: 466...  Training loss: 2.0383...  5.1850 sec/batch\n",
      "Epoch: 3/20...  Training Step: 467...  Training loss: 2.0769...  5.5090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 468...  Training loss: 2.0556...  6.1560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 469...  Training loss: 2.0536...  6.4680 sec/batch\n",
      "Epoch: 3/20...  Training Step: 470...  Training loss: 2.0265...  5.2730 sec/batch\n",
      "Epoch: 3/20...  Training Step: 471...  Training loss: 2.0316...  6.0330 sec/batch\n",
      "Epoch: 3/20...  Training Step: 472...  Training loss: 2.0630...  6.2060 sec/batch\n",
      "Epoch: 3/20...  Training Step: 473...  Training loss: 2.0413...  5.9680 sec/batch\n",
      "Epoch: 3/20...  Training Step: 474...  Training loss: 2.0512...  5.8750 sec/batch\n",
      "Epoch: 3/20...  Training Step: 475...  Training loss: 2.0090...  5.2640 sec/batch\n",
      "Epoch: 3/20...  Training Step: 476...  Training loss: 2.0211...  5.2370 sec/batch\n",
      "Epoch: 3/20...  Training Step: 477...  Training loss: 2.0014...  6.9460 sec/batch\n",
      "Epoch: 3/20...  Training Step: 478...  Training loss: 2.0426...  5.3630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 479...  Training loss: 2.0044...  5.9930 sec/batch\n",
      "Epoch: 3/20...  Training Step: 480...  Training loss: 2.0117...  6.8350 sec/batch\n",
      "Epoch: 3/20...  Training Step: 481...  Training loss: 1.9960...  5.9790 sec/batch\n",
      "Epoch: 3/20...  Training Step: 482...  Training loss: 2.0098...  5.9630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 483...  Training loss: 2.0168...  5.8770 sec/batch\n",
      "Epoch: 3/20...  Training Step: 484...  Training loss: 2.0032...  5.3940 sec/batch\n",
      "Epoch: 3/20...  Training Step: 485...  Training loss: 1.9890...  6.7010 sec/batch\n",
      "Epoch: 3/20...  Training Step: 486...  Training loss: 2.0282...  6.1720 sec/batch\n",
      "Epoch: 3/20...  Training Step: 487...  Training loss: 2.0000...  6.2430 sec/batch\n",
      "Epoch: 3/20...  Training Step: 488...  Training loss: 2.0105...  5.7760 sec/batch\n",
      "Epoch: 3/20...  Training Step: 489...  Training loss: 1.9889...  5.2220 sec/batch\n",
      "Epoch: 3/20...  Training Step: 490...  Training loss: 2.0018...  6.0560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 491...  Training loss: 1.9962...  5.8420 sec/batch\n",
      "Epoch: 3/20...  Training Step: 492...  Training loss: 2.0081...  5.7370 sec/batch\n",
      "Epoch: 3/20...  Training Step: 493...  Training loss: 2.0130...  6.0950 sec/batch\n",
      "Epoch: 3/20...  Training Step: 494...  Training loss: 1.9992...  6.0950 sec/batch\n",
      "Epoch: 3/20...  Training Step: 495...  Training loss: 1.9892...  5.7340 sec/batch\n",
      "Epoch: 3/20...  Training Step: 496...  Training loss: 1.9661...  6.4690 sec/batch\n",
      "Epoch: 3/20...  Training Step: 497...  Training loss: 2.0121...  5.3090 sec/batch\n",
      "Epoch: 3/20...  Training Step: 498...  Training loss: 2.0085...  5.4450 sec/batch\n",
      "Epoch: 3/20...  Training Step: 499...  Training loss: 1.9949...  6.0950 sec/batch\n",
      "Epoch: 3/20...  Training Step: 500...  Training loss: 1.9986...  6.6350 sec/batch\n",
      "Epoch: 3/20...  Training Step: 501...  Training loss: 1.9891...  5.3220 sec/batch\n",
      "Epoch: 3/20...  Training Step: 502...  Training loss: 2.0058...  5.5080 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/20...  Training Step: 503...  Training loss: 1.9990...  4.9210 sec/batch\n",
      "Epoch: 3/20...  Training Step: 504...  Training loss: 2.0134...  5.9000 sec/batch\n",
      "Epoch: 3/20...  Training Step: 505...  Training loss: 2.0078...  4.8950 sec/batch\n",
      "Epoch: 3/20...  Training Step: 506...  Training loss: 2.0019...  5.7630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 507...  Training loss: 1.9984...  5.7070 sec/batch\n",
      "Epoch: 3/20...  Training Step: 508...  Training loss: 1.9916...  5.2530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 509...  Training loss: 1.9882...  5.2240 sec/batch\n",
      "Epoch: 3/20...  Training Step: 510...  Training loss: 1.9816...  5.4900 sec/batch\n",
      "Epoch: 3/20...  Training Step: 511...  Training loss: 1.9757...  6.2390 sec/batch\n",
      "Epoch: 3/20...  Training Step: 512...  Training loss: 1.9497...  6.0280 sec/batch\n",
      "Epoch: 3/20...  Training Step: 513...  Training loss: 1.9904...  4.8960 sec/batch\n",
      "Epoch: 3/20...  Training Step: 514...  Training loss: 1.9810...  5.6810 sec/batch\n",
      "Epoch: 3/20...  Training Step: 515...  Training loss: 2.0009...  5.9290 sec/batch\n",
      "Epoch: 3/20...  Training Step: 516...  Training loss: 1.9907...  5.5550 sec/batch\n",
      "Epoch: 3/20...  Training Step: 517...  Training loss: 2.0049...  5.5030 sec/batch\n",
      "Epoch: 3/20...  Training Step: 518...  Training loss: 1.9675...  5.0120 sec/batch\n",
      "Epoch: 3/20...  Training Step: 519...  Training loss: 1.9623...  6.4560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 520...  Training loss: 2.0098...  7.0690 sec/batch\n",
      "Epoch: 3/20...  Training Step: 521...  Training loss: 1.9775...  5.6970 sec/batch\n",
      "Epoch: 3/20...  Training Step: 522...  Training loss: 1.9332...  6.1730 sec/batch\n",
      "Epoch: 3/20...  Training Step: 523...  Training loss: 1.9958...  7.7200 sec/batch\n",
      "Epoch: 3/20...  Training Step: 524...  Training loss: 1.9930...  6.6500 sec/batch\n",
      "Epoch: 3/20...  Training Step: 525...  Training loss: 1.9817...  5.1120 sec/batch\n",
      "Epoch: 3/20...  Training Step: 526...  Training loss: 1.9803...  5.2420 sec/batch\n",
      "Epoch: 3/20...  Training Step: 527...  Training loss: 1.9584...  5.8730 sec/batch\n",
      "Epoch: 3/20...  Training Step: 528...  Training loss: 1.9551...  4.9890 sec/batch\n",
      "Epoch: 3/20...  Training Step: 529...  Training loss: 1.9871...  5.3880 sec/batch\n",
      "Epoch: 3/20...  Training Step: 530...  Training loss: 1.9857...  5.2990 sec/batch\n",
      "Epoch: 3/20...  Training Step: 531...  Training loss: 1.9720...  4.7600 sec/batch\n",
      "Epoch: 3/20...  Training Step: 532...  Training loss: 1.9845...  5.2040 sec/batch\n",
      "Epoch: 3/20...  Training Step: 533...  Training loss: 1.9820...  4.8620 sec/batch\n",
      "Epoch: 3/20...  Training Step: 534...  Training loss: 1.9752...  4.7840 sec/batch\n",
      "Epoch: 3/20...  Training Step: 535...  Training loss: 2.0038...  5.1230 sec/batch\n",
      "Epoch: 3/20...  Training Step: 536...  Training loss: 1.9619...  5.6520 sec/batch\n",
      "Epoch: 3/20...  Training Step: 537...  Training loss: 1.9919...  6.0200 sec/batch\n",
      "Epoch: 3/20...  Training Step: 538...  Training loss: 1.9600...  7.4240 sec/batch\n",
      "Epoch: 3/20...  Training Step: 539...  Training loss: 1.9646...  7.3000 sec/batch\n",
      "Epoch: 3/20...  Training Step: 540...  Training loss: 1.9704...  5.1530 sec/batch\n",
      "Epoch: 3/20...  Training Step: 541...  Training loss: 1.9541...  5.5140 sec/batch\n",
      "Epoch: 3/20...  Training Step: 542...  Training loss: 1.9811...  6.9880 sec/batch\n",
      "Epoch: 3/20...  Training Step: 543...  Training loss: 1.9817...  6.5440 sec/batch\n",
      "Epoch: 3/20...  Training Step: 544...  Training loss: 1.9898...  7.6110 sec/batch\n",
      "Epoch: 3/20...  Training Step: 545...  Training loss: 1.9772...  6.3450 sec/batch\n",
      "Epoch: 3/20...  Training Step: 546...  Training loss: 1.9493...  5.6010 sec/batch\n",
      "Epoch: 3/20...  Training Step: 547...  Training loss: 1.9541...  6.0430 sec/batch\n",
      "Epoch: 3/20...  Training Step: 548...  Training loss: 1.9994...  5.5630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 549...  Training loss: 1.9687...  4.8290 sec/batch\n",
      "Epoch: 3/20...  Training Step: 550...  Training loss: 1.9686...  5.4170 sec/batch\n",
      "Epoch: 3/20...  Training Step: 551...  Training loss: 1.9494...  5.3560 sec/batch\n",
      "Epoch: 3/20...  Training Step: 552...  Training loss: 1.9617...  5.3790 sec/batch\n",
      "Epoch: 3/20...  Training Step: 553...  Training loss: 1.9622...  5.7320 sec/batch\n",
      "Epoch: 3/20...  Training Step: 554...  Training loss: 1.9419...  5.9180 sec/batch\n",
      "Epoch: 3/20...  Training Step: 555...  Training loss: 1.9205...  6.0730 sec/batch\n",
      "Epoch: 3/20...  Training Step: 556...  Training loss: 1.9826...  6.0360 sec/batch\n",
      "Epoch: 3/20...  Training Step: 557...  Training loss: 1.9693...  5.6260 sec/batch\n",
      "Epoch: 3/20...  Training Step: 558...  Training loss: 1.9425...  5.7750 sec/batch\n",
      "Epoch: 3/20...  Training Step: 559...  Training loss: 1.9687...  5.1690 sec/batch\n",
      "Epoch: 3/20...  Training Step: 560...  Training loss: 1.9637...  5.5350 sec/batch\n",
      "Epoch: 3/20...  Training Step: 561...  Training loss: 1.9533...  5.7910 sec/batch\n",
      "Epoch: 3/20...  Training Step: 562...  Training loss: 1.9369...  5.4230 sec/batch\n",
      "Epoch: 3/20...  Training Step: 563...  Training loss: 1.9556...  6.2730 sec/batch\n",
      "Epoch: 3/20...  Training Step: 564...  Training loss: 2.0014...  5.7140 sec/batch\n",
      "Epoch: 3/20...  Training Step: 565...  Training loss: 1.9368...  4.8400 sec/batch\n",
      "Epoch: 3/20...  Training Step: 566...  Training loss: 1.9353...  5.2640 sec/batch\n",
      "Epoch: 3/20...  Training Step: 567...  Training loss: 1.9213...  5.3100 sec/batch\n",
      "Epoch: 3/20...  Training Step: 568...  Training loss: 1.9358...  5.5960 sec/batch\n",
      "Epoch: 3/20...  Training Step: 569...  Training loss: 1.9553...  7.0420 sec/batch\n",
      "Epoch: 3/20...  Training Step: 570...  Training loss: 1.9472...  5.7360 sec/batch\n",
      "Epoch: 3/20...  Training Step: 571...  Training loss: 1.9522...  5.0250 sec/batch\n",
      "Epoch: 3/20...  Training Step: 572...  Training loss: 1.9361...  5.5870 sec/batch\n",
      "Epoch: 3/20...  Training Step: 573...  Training loss: 1.9243...  6.9390 sec/batch\n",
      "Epoch: 3/20...  Training Step: 574...  Training loss: 1.9438...  5.8450 sec/batch\n",
      "Epoch: 3/20...  Training Step: 575...  Training loss: 1.9116...  6.2460 sec/batch\n",
      "Epoch: 3/20...  Training Step: 576...  Training loss: 1.8992...  6.2800 sec/batch\n",
      "Epoch: 3/20...  Training Step: 577...  Training loss: 1.9152...  6.4000 sec/batch\n",
      "Epoch: 3/20...  Training Step: 578...  Training loss: 1.9310...  6.2680 sec/batch\n",
      "Epoch: 3/20...  Training Step: 579...  Training loss: 1.9306...  5.5160 sec/batch\n",
      "Epoch: 3/20...  Training Step: 580...  Training loss: 1.9525...  5.0830 sec/batch\n",
      "Epoch: 3/20...  Training Step: 581...  Training loss: 1.9355...  5.6470 sec/batch\n",
      "Epoch: 3/20...  Training Step: 582...  Training loss: 1.9200...  5.4200 sec/batch\n",
      "Epoch: 3/20...  Training Step: 583...  Training loss: 1.9288...  6.1190 sec/batch\n",
      "Epoch: 3/20...  Training Step: 584...  Training loss: 1.9021...  5.7630 sec/batch\n",
      "Epoch: 3/20...  Training Step: 585...  Training loss: 1.9108...  5.5370 sec/batch\n",
      "Epoch: 3/20...  Training Step: 586...  Training loss: 1.9307...  5.2320 sec/batch\n",
      "Epoch: 3/20...  Training Step: 587...  Training loss: 1.9346...  5.3390 sec/batch\n",
      "Epoch: 3/20...  Training Step: 588...  Training loss: 1.9021...  5.3440 sec/batch\n",
      "Epoch: 3/20...  Training Step: 589...  Training loss: 1.9220...  5.3970 sec/batch\n",
      "Epoch: 3/20...  Training Step: 590...  Training loss: 1.9026...  5.2890 sec/batch\n",
      "Epoch: 3/20...  Training Step: 591...  Training loss: 1.8834...  5.0980 sec/batch\n",
      "Epoch: 3/20...  Training Step: 592...  Training loss: 1.9191...  6.1400 sec/batch\n",
      "Epoch: 3/20...  Training Step: 593...  Training loss: 1.9217...  5.6410 sec/batch\n",
      "Epoch: 3/20...  Training Step: 594...  Training loss: 1.9057...  6.7930 sec/batch\n",
      "Epoch: 4/20...  Training Step: 595...  Training loss: 2.0042...  6.6780 sec/batch\n",
      "Epoch: 4/20...  Training Step: 596...  Training loss: 1.9078...  4.7800 sec/batch\n",
      "Epoch: 4/20...  Training Step: 597...  Training loss: 1.8976...  5.5860 sec/batch\n",
      "Epoch: 4/20...  Training Step: 598...  Training loss: 1.9094...  5.2960 sec/batch\n",
      "Epoch: 4/20...  Training Step: 599...  Training loss: 1.9102...  6.6500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 600...  Training loss: 1.8715...  6.4840 sec/batch\n",
      "Epoch: 4/20...  Training Step: 601...  Training loss: 1.9078...  7.2030 sec/batch\n",
      "Epoch: 4/20...  Training Step: 602...  Training loss: 1.8949...  4.9220 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 603...  Training loss: 1.9415...  5.1150 sec/batch\n",
      "Epoch: 4/20...  Training Step: 604...  Training loss: 1.9011...  5.8840 sec/batch\n",
      "Epoch: 4/20...  Training Step: 605...  Training loss: 1.8884...  5.1580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 606...  Training loss: 1.8779...  5.1220 sec/batch\n",
      "Epoch: 4/20...  Training Step: 607...  Training loss: 1.8973...  5.7650 sec/batch\n",
      "Epoch: 4/20...  Training Step: 608...  Training loss: 1.9334...  5.4470 sec/batch\n",
      "Epoch: 4/20...  Training Step: 609...  Training loss: 1.8897...  5.2670 sec/batch\n",
      "Epoch: 4/20...  Training Step: 610...  Training loss: 1.8762...  5.1540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 611...  Training loss: 1.8956...  4.6940 sec/batch\n",
      "Epoch: 4/20...  Training Step: 612...  Training loss: 1.9334...  5.4320 sec/batch\n",
      "Epoch: 4/20...  Training Step: 613...  Training loss: 1.8989...  6.5050 sec/batch\n",
      "Epoch: 4/20...  Training Step: 614...  Training loss: 1.9005...  6.3560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 615...  Training loss: 1.8781...  5.7480 sec/batch\n",
      "Epoch: 4/20...  Training Step: 616...  Training loss: 1.9368...  5.5850 sec/batch\n",
      "Epoch: 4/20...  Training Step: 617...  Training loss: 1.8891...  6.4630 sec/batch\n",
      "Epoch: 4/20...  Training Step: 618...  Training loss: 1.8945...  6.2040 sec/batch\n",
      "Epoch: 4/20...  Training Step: 619...  Training loss: 1.8960...  5.9040 sec/batch\n",
      "Epoch: 4/20...  Training Step: 620...  Training loss: 1.8651...  6.6110 sec/batch\n",
      "Epoch: 4/20...  Training Step: 621...  Training loss: 1.8789...  8.8520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 622...  Training loss: 1.9069...  6.4410 sec/batch\n",
      "Epoch: 4/20...  Training Step: 623...  Training loss: 1.9347...  5.6140 sec/batch\n",
      "Epoch: 4/20...  Training Step: 624...  Training loss: 1.9004...  6.0580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 625...  Training loss: 1.8907...  5.8500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 626...  Training loss: 1.8692...  5.1400 sec/batch\n",
      "Epoch: 4/20...  Training Step: 627...  Training loss: 1.8978...  6.6890 sec/batch\n",
      "Epoch: 4/20...  Training Step: 628...  Training loss: 1.9249...  6.1760 sec/batch\n",
      "Epoch: 4/20...  Training Step: 629...  Training loss: 1.8814...  6.2930 sec/batch\n",
      "Epoch: 4/20...  Training Step: 630...  Training loss: 1.8793...  5.6970 sec/batch\n",
      "Epoch: 4/20...  Training Step: 631...  Training loss: 1.8785...  5.1410 sec/batch\n",
      "Epoch: 4/20...  Training Step: 632...  Training loss: 1.8474...  5.0150 sec/batch\n",
      "Epoch: 4/20...  Training Step: 633...  Training loss: 1.8373...  4.9920 sec/batch\n",
      "Epoch: 4/20...  Training Step: 634...  Training loss: 1.8531...  5.9210 sec/batch\n",
      "Epoch: 4/20...  Training Step: 635...  Training loss: 1.8522...  5.9910 sec/batch\n",
      "Epoch: 4/20...  Training Step: 636...  Training loss: 1.8771...  5.0640 sec/batch\n",
      "Epoch: 4/20...  Training Step: 637...  Training loss: 1.8617...  5.3830 sec/batch\n",
      "Epoch: 4/20...  Training Step: 638...  Training loss: 1.8501...  6.2650 sec/batch\n",
      "Epoch: 4/20...  Training Step: 639...  Training loss: 1.8727...  8.7420 sec/batch\n",
      "Epoch: 4/20...  Training Step: 640...  Training loss: 1.8299...  7.6170 sec/batch\n",
      "Epoch: 4/20...  Training Step: 641...  Training loss: 1.8761...  6.4890 sec/batch\n",
      "Epoch: 4/20...  Training Step: 642...  Training loss: 1.8549...  8.7490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 643...  Training loss: 1.8631...  8.1010 sec/batch\n",
      "Epoch: 4/20...  Training Step: 644...  Training loss: 1.9054...  6.2540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 645...  Training loss: 1.8312...  6.9860 sec/batch\n",
      "Epoch: 4/20...  Training Step: 646...  Training loss: 1.9294...  6.6060 sec/batch\n",
      "Epoch: 4/20...  Training Step: 647...  Training loss: 1.8691...  6.7460 sec/batch\n",
      "Epoch: 4/20...  Training Step: 648...  Training loss: 1.8688...  5.6250 sec/batch\n",
      "Epoch: 4/20...  Training Step: 649...  Training loss: 1.8543...  5.5970 sec/batch\n",
      "Epoch: 4/20...  Training Step: 650...  Training loss: 1.8713...  6.4700 sec/batch\n",
      "Epoch: 4/20...  Training Step: 651...  Training loss: 1.8854...  5.3400 sec/batch\n",
      "Epoch: 4/20...  Training Step: 652...  Training loss: 1.8603...  5.6740 sec/batch\n",
      "Epoch: 4/20...  Training Step: 653...  Training loss: 1.8446...  5.7710 sec/batch\n",
      "Epoch: 4/20...  Training Step: 654...  Training loss: 1.8988...  5.0540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 655...  Training loss: 1.8617...  7.0440 sec/batch\n",
      "Epoch: 4/20...  Training Step: 656...  Training loss: 1.9053...  6.4180 sec/batch\n",
      "Epoch: 4/20...  Training Step: 657...  Training loss: 1.8995...  5.7230 sec/batch\n",
      "Epoch: 4/20...  Training Step: 658...  Training loss: 1.8856...  5.7260 sec/batch\n",
      "Epoch: 4/20...  Training Step: 659...  Training loss: 1.8637...  5.7920 sec/batch\n",
      "Epoch: 4/20...  Training Step: 660...  Training loss: 1.8904...  5.6200 sec/batch\n",
      "Epoch: 4/20...  Training Step: 661...  Training loss: 1.8826...  6.1620 sec/batch\n",
      "Epoch: 4/20...  Training Step: 662...  Training loss: 1.8424...  6.2100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 663...  Training loss: 1.8492...  5.4590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 664...  Training loss: 1.8521...  4.6780 sec/batch\n",
      "Epoch: 4/20...  Training Step: 665...  Training loss: 1.8892...  5.1590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 666...  Training loss: 1.8717...  5.8920 sec/batch\n",
      "Epoch: 4/20...  Training Step: 667...  Training loss: 1.8827...  6.2050 sec/batch\n",
      "Epoch: 4/20...  Training Step: 668...  Training loss: 1.8458...  6.1010 sec/batch\n",
      "Epoch: 4/20...  Training Step: 669...  Training loss: 1.8445...  5.3910 sec/batch\n",
      "Epoch: 4/20...  Training Step: 670...  Training loss: 1.8834...  4.9500 sec/batch\n",
      "Epoch: 4/20...  Training Step: 671...  Training loss: 1.8588...  4.6640 sec/batch\n",
      "Epoch: 4/20...  Training Step: 672...  Training loss: 1.8651...  5.0830 sec/batch\n",
      "Epoch: 4/20...  Training Step: 673...  Training loss: 1.8274...  5.0590 sec/batch\n",
      "Epoch: 4/20...  Training Step: 674...  Training loss: 1.8432...  5.3310 sec/batch\n",
      "Epoch: 4/20...  Training Step: 675...  Training loss: 1.8158...  5.9030 sec/batch\n",
      "Epoch: 4/20...  Training Step: 676...  Training loss: 1.8647...  5.9950 sec/batch\n",
      "Epoch: 4/20...  Training Step: 677...  Training loss: 1.8131...  6.3390 sec/batch\n",
      "Epoch: 4/20...  Training Step: 678...  Training loss: 1.8475...  7.4100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 679...  Training loss: 1.8106...  5.7570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 680...  Training loss: 1.8354...  6.1080 sec/batch\n",
      "Epoch: 4/20...  Training Step: 681...  Training loss: 1.8321...  5.2570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 682...  Training loss: 1.8261...  4.9210 sec/batch\n",
      "Epoch: 4/20...  Training Step: 683...  Training loss: 1.8098...  6.3850 sec/batch\n",
      "Epoch: 4/20...  Training Step: 684...  Training loss: 1.8528...  7.6370 sec/batch\n",
      "Epoch: 4/20...  Training Step: 685...  Training loss: 1.8159...  6.2920 sec/batch\n",
      "Epoch: 4/20...  Training Step: 686...  Training loss: 1.8340...  5.9450 sec/batch\n",
      "Epoch: 4/20...  Training Step: 687...  Training loss: 1.8099...  6.7840 sec/batch\n",
      "Epoch: 4/20...  Training Step: 688...  Training loss: 1.8170...  6.3650 sec/batch\n",
      "Epoch: 4/20...  Training Step: 689...  Training loss: 1.8147...  6.4770 sec/batch\n",
      "Epoch: 4/20...  Training Step: 690...  Training loss: 1.8354...  6.2880 sec/batch\n",
      "Epoch: 4/20...  Training Step: 691...  Training loss: 1.8368...  5.7170 sec/batch\n",
      "Epoch: 4/20...  Training Step: 692...  Training loss: 1.8027...  6.2230 sec/batch\n",
      "Epoch: 4/20...  Training Step: 693...  Training loss: 1.8170...  6.3450 sec/batch\n",
      "Epoch: 4/20...  Training Step: 694...  Training loss: 1.7828...  5.6290 sec/batch\n",
      "Epoch: 4/20...  Training Step: 695...  Training loss: 1.8399...  4.8220 sec/batch\n",
      "Epoch: 4/20...  Training Step: 696...  Training loss: 1.8231...  4.7730 sec/batch\n",
      "Epoch: 4/20...  Training Step: 697...  Training loss: 1.8128...  5.3180 sec/batch\n",
      "Epoch: 4/20...  Training Step: 698...  Training loss: 1.8200...  5.0240 sec/batch\n",
      "Epoch: 4/20...  Training Step: 699...  Training loss: 1.8105...  6.2660 sec/batch\n",
      "Epoch: 4/20...  Training Step: 700...  Training loss: 1.8311...  6.0930 sec/batch\n",
      "Epoch: 4/20...  Training Step: 701...  Training loss: 1.8189...  5.7320 sec/batch\n",
      "Epoch: 4/20...  Training Step: 702...  Training loss: 1.8373...  5.1250 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4/20...  Training Step: 703...  Training loss: 1.8348...  6.8090 sec/batch\n",
      "Epoch: 4/20...  Training Step: 704...  Training loss: 1.8363...  6.4880 sec/batch\n",
      "Epoch: 4/20...  Training Step: 705...  Training loss: 1.8281...  5.9380 sec/batch\n",
      "Epoch: 4/20...  Training Step: 706...  Training loss: 1.8180...  5.4300 sec/batch\n",
      "Epoch: 4/20...  Training Step: 707...  Training loss: 1.8314...  5.6440 sec/batch\n",
      "Epoch: 4/20...  Training Step: 708...  Training loss: 1.8199...  5.8730 sec/batch\n",
      "Epoch: 4/20...  Training Step: 709...  Training loss: 1.8121...  6.6070 sec/batch\n",
      "Epoch: 4/20...  Training Step: 710...  Training loss: 1.7800...  6.1680 sec/batch\n",
      "Epoch: 4/20...  Training Step: 711...  Training loss: 1.8311...  5.1630 sec/batch\n",
      "Epoch: 4/20...  Training Step: 712...  Training loss: 1.8117...  5.6960 sec/batch\n",
      "Epoch: 4/20...  Training Step: 713...  Training loss: 1.8177...  5.9260 sec/batch\n",
      "Epoch: 4/20...  Training Step: 714...  Training loss: 1.8193...  7.5770 sec/batch\n",
      "Epoch: 4/20...  Training Step: 715...  Training loss: 1.8277...  6.0260 sec/batch\n",
      "Epoch: 4/20...  Training Step: 716...  Training loss: 1.7789...  5.4140 sec/batch\n",
      "Epoch: 4/20...  Training Step: 717...  Training loss: 1.7986...  5.2940 sec/batch\n",
      "Epoch: 4/20...  Training Step: 718...  Training loss: 1.8420...  5.5270 sec/batch\n",
      "Epoch: 4/20...  Training Step: 719...  Training loss: 1.7979...  6.5180 sec/batch\n",
      "Epoch: 4/20...  Training Step: 720...  Training loss: 1.7735...  7.2170 sec/batch\n",
      "Epoch: 4/20...  Training Step: 721...  Training loss: 1.8306...  5.9880 sec/batch\n",
      "Epoch: 4/20...  Training Step: 722...  Training loss: 1.8279...  7.1600 sec/batch\n",
      "Epoch: 4/20...  Training Step: 723...  Training loss: 1.7990...  6.2420 sec/batch\n",
      "Epoch: 4/20...  Training Step: 724...  Training loss: 1.7992...  5.7510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 725...  Training loss: 1.7812...  7.5350 sec/batch\n",
      "Epoch: 4/20...  Training Step: 726...  Training loss: 1.7833...  6.4580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 727...  Training loss: 1.8144...  6.3030 sec/batch\n",
      "Epoch: 4/20...  Training Step: 728...  Training loss: 1.8258...  6.4970 sec/batch\n",
      "Epoch: 4/20...  Training Step: 729...  Training loss: 1.8035...  6.6990 sec/batch\n",
      "Epoch: 4/20...  Training Step: 730...  Training loss: 1.8115...  6.3570 sec/batch\n",
      "Epoch: 4/20...  Training Step: 731...  Training loss: 1.8243...  5.8540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 732...  Training loss: 1.8042...  5.0060 sec/batch\n",
      "Epoch: 4/20...  Training Step: 733...  Training loss: 1.8404...  4.9490 sec/batch\n",
      "Epoch: 4/20...  Training Step: 734...  Training loss: 1.8011...  5.6200 sec/batch\n",
      "Epoch: 4/20...  Training Step: 735...  Training loss: 1.8552...  6.3380 sec/batch\n",
      "Epoch: 4/20...  Training Step: 736...  Training loss: 1.7949...  5.9890 sec/batch\n",
      "Epoch: 4/20...  Training Step: 737...  Training loss: 1.7994...  6.2770 sec/batch\n",
      "Epoch: 4/20...  Training Step: 738...  Training loss: 1.8135...  6.0980 sec/batch\n",
      "Epoch: 4/20...  Training Step: 739...  Training loss: 1.7869...  6.5680 sec/batch\n",
      "Epoch: 4/20...  Training Step: 740...  Training loss: 1.8099...  6.0090 sec/batch\n",
      "Epoch: 4/20...  Training Step: 741...  Training loss: 1.8147...  5.2350 sec/batch\n",
      "Epoch: 4/20...  Training Step: 742...  Training loss: 1.8422...  5.6810 sec/batch\n",
      "Epoch: 4/20...  Training Step: 743...  Training loss: 1.8083...  6.5860 sec/batch\n",
      "Epoch: 4/20...  Training Step: 744...  Training loss: 1.7945...  6.0440 sec/batch\n",
      "Epoch: 4/20...  Training Step: 745...  Training loss: 1.7794...  5.7830 sec/batch\n",
      "Epoch: 4/20...  Training Step: 746...  Training loss: 1.8249...  6.3160 sec/batch\n",
      "Epoch: 4/20...  Training Step: 747...  Training loss: 1.8153...  5.4160 sec/batch\n",
      "Epoch: 4/20...  Training Step: 748...  Training loss: 1.8200...  4.9960 sec/batch\n",
      "Epoch: 4/20...  Training Step: 749...  Training loss: 1.8045...  5.0310 sec/batch\n",
      "Epoch: 4/20...  Training Step: 750...  Training loss: 1.7975...  5.6620 sec/batch\n",
      "Epoch: 4/20...  Training Step: 751...  Training loss: 1.8041...  5.5030 sec/batch\n",
      "Epoch: 4/20...  Training Step: 752...  Training loss: 1.7947...  7.0520 sec/batch\n",
      "Epoch: 4/20...  Training Step: 753...  Training loss: 1.7634...  5.9900 sec/batch\n",
      "Epoch: 4/20...  Training Step: 754...  Training loss: 1.8164...  5.8190 sec/batch\n",
      "Epoch: 4/20...  Training Step: 755...  Training loss: 1.8197...  5.3330 sec/batch\n",
      "Epoch: 4/20...  Training Step: 756...  Training loss: 1.7981...  6.2360 sec/batch\n",
      "Epoch: 4/20...  Training Step: 757...  Training loss: 1.8025...  5.6840 sec/batch\n",
      "Epoch: 4/20...  Training Step: 758...  Training loss: 1.7972...  5.2950 sec/batch\n",
      "Epoch: 4/20...  Training Step: 759...  Training loss: 1.7974...  7.7070 sec/batch\n",
      "Epoch: 4/20...  Training Step: 760...  Training loss: 1.7867...  6.1680 sec/batch\n",
      "Epoch: 4/20...  Training Step: 761...  Training loss: 1.7964...  5.2900 sec/batch\n",
      "Epoch: 4/20...  Training Step: 762...  Training loss: 1.8570...  5.4790 sec/batch\n",
      "Epoch: 4/20...  Training Step: 763...  Training loss: 1.7894...  4.9640 sec/batch\n",
      "Epoch: 4/20...  Training Step: 764...  Training loss: 1.7821...  5.0000 sec/batch\n",
      "Epoch: 4/20...  Training Step: 765...  Training loss: 1.7728...  4.9070 sec/batch\n",
      "Epoch: 4/20...  Training Step: 766...  Training loss: 1.7713...  5.1620 sec/batch\n",
      "Epoch: 4/20...  Training Step: 767...  Training loss: 1.8103...  4.7540 sec/batch\n",
      "Epoch: 4/20...  Training Step: 768...  Training loss: 1.7989...  5.9580 sec/batch\n",
      "Epoch: 4/20...  Training Step: 769...  Training loss: 1.7991...  6.8970 sec/batch\n",
      "Epoch: 4/20...  Training Step: 770...  Training loss: 1.7826...  7.4380 sec/batch\n",
      "Epoch: 4/20...  Training Step: 771...  Training loss: 1.7656...  5.3150 sec/batch\n",
      "Epoch: 4/20...  Training Step: 772...  Training loss: 1.7980...  5.2970 sec/batch\n",
      "Epoch: 4/20...  Training Step: 773...  Training loss: 1.7582...  6.5360 sec/batch\n",
      "Epoch: 4/20...  Training Step: 774...  Training loss: 1.7492...  6.0100 sec/batch\n",
      "Epoch: 4/20...  Training Step: 775...  Training loss: 1.7585...  6.5890 sec/batch\n",
      "Epoch: 4/20...  Training Step: 776...  Training loss: 1.7881...  5.7830 sec/batch\n",
      "Epoch: 4/20...  Training Step: 777...  Training loss: 1.7718...  5.9510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 778...  Training loss: 1.7958...  7.7870 sec/batch\n",
      "Epoch: 4/20...  Training Step: 779...  Training loss: 1.7849...  7.0680 sec/batch\n",
      "Epoch: 4/20...  Training Step: 780...  Training loss: 1.7711...  6.9080 sec/batch\n",
      "Epoch: 4/20...  Training Step: 781...  Training loss: 1.7922...  6.3150 sec/batch\n",
      "Epoch: 4/20...  Training Step: 782...  Training loss: 1.7646...  6.4310 sec/batch\n",
      "Epoch: 4/20...  Training Step: 783...  Training loss: 1.7641...  5.5160 sec/batch\n",
      "Epoch: 4/20...  Training Step: 784...  Training loss: 1.7803...  5.6800 sec/batch\n",
      "Epoch: 4/20...  Training Step: 785...  Training loss: 1.7782...  5.4560 sec/batch\n",
      "Epoch: 4/20...  Training Step: 786...  Training loss: 1.7552...  6.5340 sec/batch\n",
      "Epoch: 4/20...  Training Step: 787...  Training loss: 1.7880...  6.8080 sec/batch\n",
      "Epoch: 4/20...  Training Step: 788...  Training loss: 1.7462...  6.4730 sec/batch\n",
      "Epoch: 4/20...  Training Step: 789...  Training loss: 1.7402...  5.8510 sec/batch\n",
      "Epoch: 4/20...  Training Step: 790...  Training loss: 1.7825...  5.9240 sec/batch\n",
      "Epoch: 4/20...  Training Step: 791...  Training loss: 1.7778...  6.7150 sec/batch\n",
      "Epoch: 4/20...  Training Step: 792...  Training loss: 1.7568...  6.1200 sec/batch\n",
      "Epoch: 5/20...  Training Step: 793...  Training loss: 1.8572...  5.6340 sec/batch\n",
      "Epoch: 5/20...  Training Step: 794...  Training loss: 1.7733...  5.6360 sec/batch\n",
      "Epoch: 5/20...  Training Step: 795...  Training loss: 1.7517...  7.7430 sec/batch\n",
      "Epoch: 5/20...  Training Step: 796...  Training loss: 1.7669...  6.7400 sec/batch\n",
      "Epoch: 5/20...  Training Step: 797...  Training loss: 1.7533...  5.7210 sec/batch\n",
      "Epoch: 5/20...  Training Step: 798...  Training loss: 1.7232...  5.9550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 799...  Training loss: 1.7694...  5.5410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 800...  Training loss: 1.7439...  5.9450 sec/batch\n",
      "Epoch: 5/20...  Training Step: 801...  Training loss: 1.7881...  5.8360 sec/batch\n",
      "Epoch: 5/20...  Training Step: 802...  Training loss: 1.7726...  5.4730 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 803...  Training loss: 1.7468...  5.7580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 804...  Training loss: 1.7408...  5.6750 sec/batch\n",
      "Epoch: 5/20...  Training Step: 805...  Training loss: 1.7520...  6.3730 sec/batch\n",
      "Epoch: 5/20...  Training Step: 806...  Training loss: 1.8043...  5.6670 sec/batch\n",
      "Epoch: 5/20...  Training Step: 807...  Training loss: 1.7623...  5.4660 sec/batch\n",
      "Epoch: 5/20...  Training Step: 808...  Training loss: 1.7486...  7.0950 sec/batch\n",
      "Epoch: 5/20...  Training Step: 809...  Training loss: 1.7735...  6.9810 sec/batch\n",
      "Epoch: 5/20...  Training Step: 810...  Training loss: 1.7981...  5.8280 sec/batch\n",
      "Epoch: 5/20...  Training Step: 811...  Training loss: 1.7742...  5.5480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 812...  Training loss: 1.7642...  6.8080 sec/batch\n",
      "Epoch: 5/20...  Training Step: 813...  Training loss: 1.7545...  6.5450 sec/batch\n",
      "Epoch: 5/20...  Training Step: 814...  Training loss: 1.7868...  5.0690 sec/batch\n",
      "Epoch: 5/20...  Training Step: 815...  Training loss: 1.7498...  5.5480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 816...  Training loss: 1.7627...  5.2890 sec/batch\n",
      "Epoch: 5/20...  Training Step: 817...  Training loss: 1.7582...  5.4510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 818...  Training loss: 1.7306...  5.9750 sec/batch\n",
      "Epoch: 5/20...  Training Step: 819...  Training loss: 1.7296...  6.2080 sec/batch\n",
      "Epoch: 5/20...  Training Step: 820...  Training loss: 1.7673...  6.3300 sec/batch\n",
      "Epoch: 5/20...  Training Step: 821...  Training loss: 1.7831...  6.4740 sec/batch\n",
      "Epoch: 5/20...  Training Step: 822...  Training loss: 1.7656...  6.1440 sec/batch\n",
      "Epoch: 5/20...  Training Step: 823...  Training loss: 1.7567...  5.7670 sec/batch\n",
      "Epoch: 5/20...  Training Step: 824...  Training loss: 1.7303...  6.0440 sec/batch\n",
      "Epoch: 5/20...  Training Step: 825...  Training loss: 1.7651...  5.8370 sec/batch\n",
      "Epoch: 5/20...  Training Step: 826...  Training loss: 1.7743...  6.5110 sec/batch\n",
      "Epoch: 5/20...  Training Step: 827...  Training loss: 1.7522...  7.0030 sec/batch\n",
      "Epoch: 5/20...  Training Step: 828...  Training loss: 1.7449...  5.2100 sec/batch\n",
      "Epoch: 5/20...  Training Step: 829...  Training loss: 1.7313...  5.3470 sec/batch\n",
      "Epoch: 5/20...  Training Step: 830...  Training loss: 1.7041...  6.3930 sec/batch\n",
      "Epoch: 5/20...  Training Step: 831...  Training loss: 1.7006...  6.8100 sec/batch\n",
      "Epoch: 5/20...  Training Step: 832...  Training loss: 1.7186...  6.0330 sec/batch\n",
      "Epoch: 5/20...  Training Step: 833...  Training loss: 1.7208...  5.8480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 834...  Training loss: 1.7662...  5.7490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 835...  Training loss: 1.7301...  5.1650 sec/batch\n",
      "Epoch: 5/20...  Training Step: 836...  Training loss: 1.7184...  5.5730 sec/batch\n",
      "Epoch: 5/20...  Training Step: 837...  Training loss: 1.7530...  4.9340 sec/batch\n",
      "Epoch: 5/20...  Training Step: 838...  Training loss: 1.6955...  5.1970 sec/batch\n",
      "Epoch: 5/20...  Training Step: 839...  Training loss: 1.7330...  6.5410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 840...  Training loss: 1.7277...  8.6050 sec/batch\n",
      "Epoch: 5/20...  Training Step: 841...  Training loss: 1.7292...  6.4830 sec/batch\n",
      "Epoch: 5/20...  Training Step: 842...  Training loss: 1.7807...  5.8730 sec/batch\n",
      "Epoch: 5/20...  Training Step: 843...  Training loss: 1.7143...  6.3410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 844...  Training loss: 1.7919...  6.2560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 845...  Training loss: 1.7328...  5.6190 sec/batch\n",
      "Epoch: 5/20...  Training Step: 846...  Training loss: 1.7369...  4.8450 sec/batch\n",
      "Epoch: 5/20...  Training Step: 847...  Training loss: 1.7240...  5.4300 sec/batch\n",
      "Epoch: 5/20...  Training Step: 848...  Training loss: 1.7524...  5.3650 sec/batch\n",
      "Epoch: 5/20...  Training Step: 849...  Training loss: 1.7598...  5.7350 sec/batch\n",
      "Epoch: 5/20...  Training Step: 850...  Training loss: 1.7168...  6.2720 sec/batch\n",
      "Epoch: 5/20...  Training Step: 851...  Training loss: 1.7186...  5.6360 sec/batch\n",
      "Epoch: 5/20...  Training Step: 852...  Training loss: 1.7761...  5.5520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 853...  Training loss: 1.7311...  4.9900 sec/batch\n",
      "Epoch: 5/20...  Training Step: 854...  Training loss: 1.7807...  4.9220 sec/batch\n",
      "Epoch: 5/20...  Training Step: 855...  Training loss: 1.7643...  5.3540 sec/batch\n",
      "Epoch: 5/20...  Training Step: 856...  Training loss: 1.7578...  6.0890 sec/batch\n",
      "Epoch: 5/20...  Training Step: 857...  Training loss: 1.7303...  7.6970 sec/batch\n",
      "Epoch: 5/20...  Training Step: 858...  Training loss: 1.7536...  6.1810 sec/batch\n",
      "Epoch: 5/20...  Training Step: 859...  Training loss: 1.7405...  6.2340 sec/batch\n",
      "Epoch: 5/20...  Training Step: 860...  Training loss: 1.7108...  5.3570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 861...  Training loss: 1.7233...  5.2180 sec/batch\n",
      "Epoch: 5/20...  Training Step: 862...  Training loss: 1.7305...  5.1340 sec/batch\n",
      "Epoch: 5/20...  Training Step: 863...  Training loss: 1.7738...  5.9130 sec/batch\n",
      "Epoch: 5/20...  Training Step: 864...  Training loss: 1.7530...  5.2340 sec/batch\n",
      "Epoch: 5/20...  Training Step: 865...  Training loss: 1.7536...  5.7690 sec/batch\n",
      "Epoch: 5/20...  Training Step: 866...  Training loss: 1.7172...  6.1500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 867...  Training loss: 1.7279...  7.2740 sec/batch\n",
      "Epoch: 5/20...  Training Step: 868...  Training loss: 1.7652...  5.2600 sec/batch\n",
      "Epoch: 5/20...  Training Step: 869...  Training loss: 1.7371...  5.0500 sec/batch\n",
      "Epoch: 5/20...  Training Step: 870...  Training loss: 1.7304...  5.0190 sec/batch\n",
      "Epoch: 5/20...  Training Step: 871...  Training loss: 1.6890...  6.2360 sec/batch\n",
      "Epoch: 5/20...  Training Step: 872...  Training loss: 1.7149...  5.6900 sec/batch\n",
      "Epoch: 5/20...  Training Step: 873...  Training loss: 1.6836...  7.5410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 874...  Training loss: 1.7316...  5.4760 sec/batch\n",
      "Epoch: 5/20...  Training Step: 875...  Training loss: 1.6850...  5.2460 sec/batch\n",
      "Epoch: 5/20...  Training Step: 876...  Training loss: 1.7261...  4.9530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 877...  Training loss: 1.6832...  5.9660 sec/batch\n",
      "Epoch: 5/20...  Training Step: 878...  Training loss: 1.6963...  7.1940 sec/batch\n",
      "Epoch: 5/20...  Training Step: 879...  Training loss: 1.6942...  8.1240 sec/batch\n",
      "Epoch: 5/20...  Training Step: 880...  Training loss: 1.7100...  6.7370 sec/batch\n",
      "Epoch: 5/20...  Training Step: 881...  Training loss: 1.6775...  6.0020 sec/batch\n",
      "Epoch: 5/20...  Training Step: 882...  Training loss: 1.7315...  5.4800 sec/batch\n",
      "Epoch: 5/20...  Training Step: 883...  Training loss: 1.6874...  6.2360 sec/batch\n",
      "Epoch: 5/20...  Training Step: 884...  Training loss: 1.7017...  5.1530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 885...  Training loss: 1.6805...  4.9460 sec/batch\n",
      "Epoch: 5/20...  Training Step: 886...  Training loss: 1.6918...  6.3340 sec/batch\n",
      "Epoch: 5/20...  Training Step: 887...  Training loss: 1.6877...  6.2440 sec/batch\n",
      "Epoch: 5/20...  Training Step: 888...  Training loss: 1.7106...  4.8710 sec/batch\n",
      "Epoch: 5/20...  Training Step: 889...  Training loss: 1.7075...  5.9010 sec/batch\n",
      "Epoch: 5/20...  Training Step: 890...  Training loss: 1.6802...  5.8870 sec/batch\n",
      "Epoch: 5/20...  Training Step: 891...  Training loss: 1.6962...  6.1350 sec/batch\n",
      "Epoch: 5/20...  Training Step: 892...  Training loss: 1.6643...  6.8450 sec/batch\n",
      "Epoch: 5/20...  Training Step: 893...  Training loss: 1.7057...  5.7890 sec/batch\n",
      "Epoch: 5/20...  Training Step: 894...  Training loss: 1.7127...  6.8120 sec/batch\n",
      "Epoch: 5/20...  Training Step: 895...  Training loss: 1.6934...  6.5120 sec/batch\n",
      "Epoch: 5/20...  Training Step: 896...  Training loss: 1.6893...  5.9650 sec/batch\n",
      "Epoch: 5/20...  Training Step: 897...  Training loss: 1.6893...  5.8600 sec/batch\n",
      "Epoch: 5/20...  Training Step: 898...  Training loss: 1.7032...  5.7420 sec/batch\n",
      "Epoch: 5/20...  Training Step: 899...  Training loss: 1.7023...  5.4980 sec/batch\n",
      "Epoch: 5/20...  Training Step: 900...  Training loss: 1.7127...  5.4350 sec/batch\n",
      "Epoch: 5/20...  Training Step: 901...  Training loss: 1.7227...  5.6070 sec/batch\n",
      "Epoch: 5/20...  Training Step: 902...  Training loss: 1.7295...  5.7290 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5/20...  Training Step: 903...  Training loss: 1.6992...  5.2370 sec/batch\n",
      "Epoch: 5/20...  Training Step: 904...  Training loss: 1.7032...  6.0810 sec/batch\n",
      "Epoch: 5/20...  Training Step: 905...  Training loss: 1.6896...  5.2410 sec/batch\n",
      "Epoch: 5/20...  Training Step: 906...  Training loss: 1.6919...  5.4740 sec/batch\n",
      "Epoch: 5/20...  Training Step: 907...  Training loss: 1.6887...  5.3230 sec/batch\n",
      "Epoch: 5/20...  Training Step: 908...  Training loss: 1.6715...  5.0470 sec/batch\n",
      "Epoch: 5/20...  Training Step: 909...  Training loss: 1.7102...  5.1210 sec/batch\n",
      "Epoch: 5/20...  Training Step: 910...  Training loss: 1.6966...  5.8140 sec/batch\n",
      "Epoch: 5/20...  Training Step: 911...  Training loss: 1.6974...  6.6250 sec/batch\n",
      "Epoch: 5/20...  Training Step: 912...  Training loss: 1.6964...  7.3110 sec/batch\n",
      "Epoch: 5/20...  Training Step: 913...  Training loss: 1.7137...  5.5850 sec/batch\n",
      "Epoch: 5/20...  Training Step: 914...  Training loss: 1.6779...  5.3470 sec/batch\n",
      "Epoch: 5/20...  Training Step: 915...  Training loss: 1.6688...  5.5120 sec/batch\n",
      "Epoch: 5/20...  Training Step: 916...  Training loss: 1.7169...  5.3870 sec/batch\n",
      "Epoch: 5/20...  Training Step: 917...  Training loss: 1.6850...  5.7100 sec/batch\n",
      "Epoch: 5/20...  Training Step: 918...  Training loss: 1.6604...  5.4550 sec/batch\n",
      "Epoch: 5/20...  Training Step: 919...  Training loss: 1.7193...  5.4420 sec/batch\n",
      "Epoch: 5/20...  Training Step: 920...  Training loss: 1.7054...  5.7760 sec/batch\n",
      "Epoch: 5/20...  Training Step: 921...  Training loss: 1.6877...  6.0290 sec/batch\n",
      "Epoch: 5/20...  Training Step: 922...  Training loss: 1.6760...  5.5680 sec/batch\n",
      "Epoch: 5/20...  Training Step: 923...  Training loss: 1.6616...  5.0860 sec/batch\n",
      "Epoch: 5/20...  Training Step: 924...  Training loss: 1.6717...  5.4600 sec/batch\n",
      "Epoch: 5/20...  Training Step: 925...  Training loss: 1.7016...  5.9490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 926...  Training loss: 1.7016...  5.4660 sec/batch\n",
      "Epoch: 5/20...  Training Step: 927...  Training loss: 1.7015...  6.3910 sec/batch\n",
      "Epoch: 5/20...  Training Step: 928...  Training loss: 1.6934...  6.5180 sec/batch\n",
      "Epoch: 5/20...  Training Step: 929...  Training loss: 1.7166...  5.2840 sec/batch\n",
      "Epoch: 5/20...  Training Step: 930...  Training loss: 1.6969...  5.8590 sec/batch\n",
      "Epoch: 5/20...  Training Step: 931...  Training loss: 1.7159...  5.1900 sec/batch\n",
      "Epoch: 5/20...  Training Step: 932...  Training loss: 1.6896...  5.6380 sec/batch\n",
      "Epoch: 5/20...  Training Step: 933...  Training loss: 1.7434...  5.6560 sec/batch\n",
      "Epoch: 5/20...  Training Step: 934...  Training loss: 1.6847...  5.5480 sec/batch\n",
      "Epoch: 5/20...  Training Step: 935...  Training loss: 1.6997...  5.4620 sec/batch\n",
      "Epoch: 5/20...  Training Step: 936...  Training loss: 1.7196...  5.4540 sec/batch\n",
      "Epoch: 5/20...  Training Step: 937...  Training loss: 1.6816...  5.2180 sec/batch\n",
      "Epoch: 5/20...  Training Step: 938...  Training loss: 1.7099...  5.2860 sec/batch\n",
      "Epoch: 5/20...  Training Step: 939...  Training loss: 1.7043...  5.0520 sec/batch\n",
      "Epoch: 5/20...  Training Step: 940...  Training loss: 1.7208...  4.9580 sec/batch\n",
      "Epoch: 5/20...  Training Step: 941...  Training loss: 1.7096...  5.3230 sec/batch\n",
      "Epoch: 5/20...  Training Step: 942...  Training loss: 1.6724...  6.4200 sec/batch\n",
      "Epoch: 5/20...  Training Step: 943...  Training loss: 1.6548...  6.4440 sec/batch\n",
      "Epoch: 5/20...  Training Step: 944...  Training loss: 1.7075...  5.5100 sec/batch\n",
      "Epoch: 5/20...  Training Step: 945...  Training loss: 1.6964...  5.2220 sec/batch\n",
      "Epoch: 5/20...  Training Step: 946...  Training loss: 1.6939...  5.5080 sec/batch\n",
      "Epoch: 5/20...  Training Step: 947...  Training loss: 1.6873...  5.1220 sec/batch\n",
      "Epoch: 5/20...  Training Step: 948...  Training loss: 1.6785...  5.0320 sec/batch\n",
      "Epoch: 5/20...  Training Step: 949...  Training loss: 1.6975...  5.6950 sec/batch\n",
      "Epoch: 5/20...  Training Step: 950...  Training loss: 1.6817...  4.9110 sec/batch\n",
      "Epoch: 5/20...  Training Step: 951...  Training loss: 1.6599...  5.2460 sec/batch\n",
      "Epoch: 5/20...  Training Step: 952...  Training loss: 1.7122...  6.0070 sec/batch\n",
      "Epoch: 5/20...  Training Step: 953...  Training loss: 1.7233...  5.1050 sec/batch\n",
      "Epoch: 5/20...  Training Step: 954...  Training loss: 1.6788...  6.1430 sec/batch\n",
      "Epoch: 5/20...  Training Step: 955...  Training loss: 1.7047...  6.5040 sec/batch\n",
      "Epoch: 5/20...  Training Step: 956...  Training loss: 1.6936...  5.5420 sec/batch\n",
      "Epoch: 5/20...  Training Step: 957...  Training loss: 1.6867...  5.4430 sec/batch\n",
      "Epoch: 5/20...  Training Step: 958...  Training loss: 1.6869...  5.4400 sec/batch\n",
      "Epoch: 5/20...  Training Step: 959...  Training loss: 1.6995...  4.9880 sec/batch\n",
      "Epoch: 5/20...  Training Step: 960...  Training loss: 1.7616...  4.7530 sec/batch\n",
      "Epoch: 5/20...  Training Step: 961...  Training loss: 1.6752...  5.1210 sec/batch\n",
      "Epoch: 5/20...  Training Step: 962...  Training loss: 1.6739...  5.3050 sec/batch\n",
      "Epoch: 5/20...  Training Step: 963...  Training loss: 1.6718...  5.1290 sec/batch\n",
      "Epoch: 5/20...  Training Step: 964...  Training loss: 1.6675...  5.9300 sec/batch\n",
      "Epoch: 5/20...  Training Step: 965...  Training loss: 1.7041...  8.2340 sec/batch\n",
      "Epoch: 5/20...  Training Step: 966...  Training loss: 1.6919...  7.6680 sec/batch\n",
      "Epoch: 5/20...  Training Step: 967...  Training loss: 1.6984...  7.5190 sec/batch\n",
      "Epoch: 5/20...  Training Step: 968...  Training loss: 1.6737...  5.5900 sec/batch\n",
      "Epoch: 5/20...  Training Step: 969...  Training loss: 1.6629...  5.2790 sec/batch\n",
      "Epoch: 5/20...  Training Step: 970...  Training loss: 1.7057...  5.0070 sec/batch\n",
      "Epoch: 5/20...  Training Step: 971...  Training loss: 1.6555...  4.9080 sec/batch\n",
      "Epoch: 5/20...  Training Step: 972...  Training loss: 1.6642...  4.7050 sec/batch\n",
      "Epoch: 5/20...  Training Step: 973...  Training loss: 1.6573...  6.3570 sec/batch\n",
      "Epoch: 5/20...  Training Step: 974...  Training loss: 1.6798...  5.5320 sec/batch\n",
      "Epoch: 5/20...  Training Step: 975...  Training loss: 1.6686...  6.3000 sec/batch\n",
      "Epoch: 5/20...  Training Step: 976...  Training loss: 1.6757...  6.2880 sec/batch\n",
      "Epoch: 5/20...  Training Step: 977...  Training loss: 1.6694...  6.9490 sec/batch\n",
      "Epoch: 5/20...  Training Step: 978...  Training loss: 1.6603...  5.3050 sec/batch\n",
      "Epoch: 5/20...  Training Step: 979...  Training loss: 1.6832...  5.1170 sec/batch\n",
      "Epoch: 5/20...  Training Step: 980...  Training loss: 1.6645...  5.3890 sec/batch\n",
      "Epoch: 5/20...  Training Step: 981...  Training loss: 1.6666...  5.5810 sec/batch\n",
      "Epoch: 5/20...  Training Step: 982...  Training loss: 1.6747...  8.2510 sec/batch\n",
      "Epoch: 5/20...  Training Step: 983...  Training loss: 1.6677...  7.0390 sec/batch\n",
      "Epoch: 5/20...  Training Step: 984...  Training loss: 1.6467...  6.1800 sec/batch\n",
      "Epoch: 5/20...  Training Step: 985...  Training loss: 1.6728...  6.3760 sec/batch\n",
      "Epoch: 5/20...  Training Step: 986...  Training loss: 1.6435...  6.0820 sec/batch\n",
      "Epoch: 5/20...  Training Step: 987...  Training loss: 1.6421...  6.0770 sec/batch\n",
      "Epoch: 5/20...  Training Step: 988...  Training loss: 1.6667...  7.3300 sec/batch\n",
      "Epoch: 5/20...  Training Step: 989...  Training loss: 1.6626...  6.0210 sec/batch\n",
      "Epoch: 5/20...  Training Step: 990...  Training loss: 1.6554...  5.7640 sec/batch\n",
      "Epoch: 6/20...  Training Step: 991...  Training loss: 1.7697...  5.2450 sec/batch\n",
      "Epoch: 6/20...  Training Step: 992...  Training loss: 1.6584...  5.4200 sec/batch\n",
      "Epoch: 6/20...  Training Step: 993...  Training loss: 1.6484...  6.0080 sec/batch\n",
      "Epoch: 6/20...  Training Step: 994...  Training loss: 1.6745...  5.2290 sec/batch\n",
      "Epoch: 6/20...  Training Step: 995...  Training loss: 1.6428...  5.4530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 996...  Training loss: 1.6233...  6.3980 sec/batch\n",
      "Epoch: 6/20...  Training Step: 997...  Training loss: 1.6668...  6.1620 sec/batch\n",
      "Epoch: 6/20...  Training Step: 998...  Training loss: 1.6426...  5.6800 sec/batch\n",
      "Epoch: 6/20...  Training Step: 999...  Training loss: 1.6836...  4.9860 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1000...  Training loss: 1.6523...  5.5530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1001...  Training loss: 1.6363...  6.0390 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1002...  Training loss: 1.6406...  5.4270 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 1003...  Training loss: 1.6505...  5.4990 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1004...  Training loss: 1.6911...  6.9560 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1005...  Training loss: 1.6510...  6.5990 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1006...  Training loss: 1.6303...  5.7760 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1007...  Training loss: 1.6591...  5.9840 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1008...  Training loss: 1.6886...  6.0060 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1009...  Training loss: 1.6662...  5.3090 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1010...  Training loss: 1.6729...  7.5290 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1011...  Training loss: 1.6540...  7.0770 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1012...  Training loss: 1.6783...  5.7060 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1013...  Training loss: 1.6523...  5.3940 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1014...  Training loss: 1.6674...  6.7860 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1015...  Training loss: 1.6545...  6.6270 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1016...  Training loss: 1.6217...  5.5130 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1017...  Training loss: 1.6251...  5.1420 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1018...  Training loss: 1.6601...  4.8850 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1019...  Training loss: 1.6800...  7.3680 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1020...  Training loss: 1.6623...  6.0190 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1021...  Training loss: 1.6425...  5.5480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1022...  Training loss: 1.6220...  5.9120 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1023...  Training loss: 1.6594...  5.7730 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1024...  Training loss: 1.6645...  5.7140 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1025...  Training loss: 1.6475...  5.4390 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1026...  Training loss: 1.6447...  5.4970 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1027...  Training loss: 1.6296...  5.8460 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1028...  Training loss: 1.6097...  6.3000 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1029...  Training loss: 1.6027...  5.8170 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1030...  Training loss: 1.6171...  6.0160 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1031...  Training loss: 1.6220...  6.0720 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1032...  Training loss: 1.6650...  6.0570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1033...  Training loss: 1.6223...  6.3980 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1034...  Training loss: 1.6171...  6.0780 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1035...  Training loss: 1.6610...  6.2570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1036...  Training loss: 1.6091...  5.9750 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1037...  Training loss: 1.6421...  7.0520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1038...  Training loss: 1.6174...  5.7480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1039...  Training loss: 1.6226...  6.5110 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1040...  Training loss: 1.6675...  6.5680 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1041...  Training loss: 1.6139...  6.5610 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1042...  Training loss: 1.6880...  5.3060 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1043...  Training loss: 1.6393...  5.4130 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1044...  Training loss: 1.6414...  6.3140 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1045...  Training loss: 1.6242...  7.1110 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1046...  Training loss: 1.6461...  5.9160 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1047...  Training loss: 1.6560...  7.2200 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1048...  Training loss: 1.6183...  7.0320 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1049...  Training loss: 1.6112...  6.8530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1050...  Training loss: 1.6592...  5.9530 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1051...  Training loss: 1.6445...  7.0300 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1052...  Training loss: 1.6864...  6.2830 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1053...  Training loss: 1.6694...  6.5420 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1054...  Training loss: 1.6538...  6.1020 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1055...  Training loss: 1.6311...  6.6150 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1056...  Training loss: 1.6554...  5.6480 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1057...  Training loss: 1.6495...  5.1960 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1058...  Training loss: 1.6211...  4.9120 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1059...  Training loss: 1.6421...  5.5730 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1060...  Training loss: 1.6241...  5.4550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1061...  Training loss: 1.6903...  6.1890 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1062...  Training loss: 1.6597...  5.9570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1063...  Training loss: 1.6673...  6.9920 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1064...  Training loss: 1.6270...  6.4230 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1065...  Training loss: 1.6300...  5.2110 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1066...  Training loss: 1.6682...  6.7860 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1067...  Training loss: 1.6306...  6.3070 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1068...  Training loss: 1.6343...  5.3170 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1069...  Training loss: 1.5974...  5.5970 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1070...  Training loss: 1.6360...  5.3900 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1071...  Training loss: 1.5860...  5.4640 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1072...  Training loss: 1.6484...  5.1180 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1073...  Training loss: 1.5930...  4.9460 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1074...  Training loss: 1.6336...  5.9320 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1075...  Training loss: 1.5998...  6.7870 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1076...  Training loss: 1.6121...  5.6240 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1077...  Training loss: 1.5992...  6.1960 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1078...  Training loss: 1.6096...  6.1550 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1079...  Training loss: 1.5937...  5.9610 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1080...  Training loss: 1.6313...  5.0400 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1081...  Training loss: 1.5928...  6.7070 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1082...  Training loss: 1.6229...  7.2430 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1083...  Training loss: 1.5928...  6.4940 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1084...  Training loss: 1.6038...  5.7240 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1085...  Training loss: 1.6107...  5.6230 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1086...  Training loss: 1.6337...  5.3290 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1087...  Training loss: 1.6184...  5.2960 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1088...  Training loss: 1.5864...  5.3350 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1089...  Training loss: 1.6054...  4.9520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1090...  Training loss: 1.5792...  5.4930 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1091...  Training loss: 1.6244...  5.3510 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1092...  Training loss: 1.6206...  5.1120 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1093...  Training loss: 1.6177...  5.7370 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1094...  Training loss: 1.6056...  6.1990 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1095...  Training loss: 1.6041...  6.4070 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1096...  Training loss: 1.6123...  6.6210 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1097...  Training loss: 1.6138...  6.3990 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1098...  Training loss: 1.6096...  6.4030 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1099...  Training loss: 1.6175...  5.4830 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1100...  Training loss: 1.6272...  5.6610 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1101...  Training loss: 1.6011...  5.6560 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6/20...  Training Step: 1102...  Training loss: 1.6083...  5.7580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1103...  Training loss: 1.6149...  5.6330 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1104...  Training loss: 1.6058...  6.6130 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1105...  Training loss: 1.5905...  7.2820 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1106...  Training loss: 1.5650...  5.3850 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1107...  Training loss: 1.6074...  5.9800 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1108...  Training loss: 1.6168...  6.2710 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1109...  Training loss: 1.6193...  5.7340 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1110...  Training loss: 1.6125...  5.1090 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1111...  Training loss: 1.6203...  4.9970 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1112...  Training loss: 1.5769...  5.7030 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1113...  Training loss: 1.5696...  5.8440 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1114...  Training loss: 1.6229...  6.0630 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1115...  Training loss: 1.6038...  5.5270 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1116...  Training loss: 1.5606...  5.4200 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1117...  Training loss: 1.6277...  5.1050 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1118...  Training loss: 1.6195...  4.9870 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1119...  Training loss: 1.5891...  4.9950 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1120...  Training loss: 1.5830...  4.7370 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1121...  Training loss: 1.5651...  5.0160 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1122...  Training loss: 1.5770...  5.2860 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1123...  Training loss: 1.6152...  4.9660 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1124...  Training loss: 1.6182...  6.1780 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1125...  Training loss: 1.6068...  6.6950 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1126...  Training loss: 1.5987...  5.3780 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1127...  Training loss: 1.6393...  5.5020 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1128...  Training loss: 1.6189...  4.9590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1129...  Training loss: 1.6256...  5.1590 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1130...  Training loss: 1.6086...  5.3500 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1131...  Training loss: 1.6493...  5.1020 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1132...  Training loss: 1.6003...  5.3520 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1133...  Training loss: 1.6048...  5.3980 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1134...  Training loss: 1.6293...  5.8260 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1135...  Training loss: 1.6014...  5.7000 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1136...  Training loss: 1.6205...  5.4580 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1137...  Training loss: 1.6267...  6.2630 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1138...  Training loss: 1.6387...  6.7690 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1139...  Training loss: 1.6305...  6.4850 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1140...  Training loss: 1.5919...  6.9390 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1141...  Training loss: 1.5685...  6.3700 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1142...  Training loss: 1.6093...  4.9360 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1143...  Training loss: 1.6148...  5.6490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1144...  Training loss: 1.6084...  6.1770 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1145...  Training loss: 1.6065...  5.1490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1146...  Training loss: 1.5955...  5.4990 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1147...  Training loss: 1.6217...  5.9140 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1148...  Training loss: 1.6022...  6.1400 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1149...  Training loss: 1.5689...  7.2430 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1150...  Training loss: 1.6171...  7.3780 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1151...  Training loss: 1.6348...  3500.0080 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1152...  Training loss: 1.6026...  7.9990 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1153...  Training loss: 1.6113...  9.2640 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1154...  Training loss: 1.6064...  8.2060 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1155...  Training loss: 1.5974...  7.2130 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1156...  Training loss: 1.6041...  6.9030 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1157...  Training loss: 1.6238...  6.1570 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1158...  Training loss: 1.6690...  6.5640 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1159...  Training loss: 1.5910...  7.7770 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1160...  Training loss: 1.5847...  7.7760 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1161...  Training loss: 1.5895...  7.5040 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1162...  Training loss: 1.5830...  7.3460 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1163...  Training loss: 1.6217...  6.0620 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1164...  Training loss: 1.6105...  6.0120 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1165...  Training loss: 1.6065...  6.2290 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1166...  Training loss: 1.5814...  6.6720 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1167...  Training loss: 1.5805...  6.5490 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1168...  Training loss: 1.6255...  6.8780 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1169...  Training loss: 1.5700...  5.7200 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1170...  Training loss: 1.5626...  5.8210 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1171...  Training loss: 1.5710...  5.6250 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1172...  Training loss: 1.5860...  5.6420 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1173...  Training loss: 1.5892...  5.3370 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1174...  Training loss: 1.5913...  5.2410 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1175...  Training loss: 1.5875...  5.2610 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1176...  Training loss: 1.5749...  5.0740 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1177...  Training loss: 1.6151...  4.9000 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1178...  Training loss: 1.5838...  5.0860 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1179...  Training loss: 1.5837...  4.9810 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1180...  Training loss: 1.5926...  5.1840 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1181...  Training loss: 1.5819...  5.1760 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1182...  Training loss: 1.5683...  5.0350 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1183...  Training loss: 1.5844...  5.8180 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1184...  Training loss: 1.5653...  5.2290 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1185...  Training loss: 1.5497...  4.9380 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1186...  Training loss: 1.5858...  4.7750 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1187...  Training loss: 1.5749...  4.8930 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1188...  Training loss: 1.5795...  4.7050 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1189...  Training loss: 1.6944...  4.6690 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1190...  Training loss: 1.5887...  4.6250 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1191...  Training loss: 1.5716...  4.6080 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1192...  Training loss: 1.6015...  4.7760 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1193...  Training loss: 1.5700...  5.1220 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1194...  Training loss: 1.5408...  4.7250 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1195...  Training loss: 1.5825...  4.7110 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1196...  Training loss: 1.5650...  4.7810 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1197...  Training loss: 1.5932...  5.1910 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1198...  Training loss: 1.5613...  4.9760 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1199...  Training loss: 1.5599...  4.8870 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1200...  Training loss: 1.5652...  4.6490 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 1201...  Training loss: 1.5655...  5.4000 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1202...  Training loss: 1.6091...  4.9710 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1203...  Training loss: 1.5696...  5.5570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1204...  Training loss: 1.5507...  5.6830 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1205...  Training loss: 1.5864...  5.6990 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1206...  Training loss: 1.6083...  5.7370 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1207...  Training loss: 1.5915...  5.6440 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1208...  Training loss: 1.5965...  5.6280 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1209...  Training loss: 1.5717...  5.5600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1210...  Training loss: 1.6118...  5.3960 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1211...  Training loss: 1.5695...  5.9290 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1212...  Training loss: 1.5868...  5.4850 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1213...  Training loss: 1.5772...  5.1010 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1214...  Training loss: 1.5412...  4.8670 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1215...  Training loss: 1.5316...  5.0660 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1216...  Training loss: 1.5867...  5.2840 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1217...  Training loss: 1.5899...  5.4160 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1218...  Training loss: 1.5937...  5.3480 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1219...  Training loss: 1.5760...  5.2860 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1220...  Training loss: 1.5555...  5.1440 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1221...  Training loss: 1.5865...  5.1220 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1222...  Training loss: 1.5888...  4.8910 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1223...  Training loss: 1.5647...  5.1370 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1224...  Training loss: 1.5724...  4.9590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1225...  Training loss: 1.5412...  5.2570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1226...  Training loss: 1.5276...  5.2590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1227...  Training loss: 1.5222...  5.4930 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1228...  Training loss: 1.5531...  5.3140 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1229...  Training loss: 1.5418...  6.5810 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1230...  Training loss: 1.5870...  6.8340 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1231...  Training loss: 1.5367...  7.4140 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1232...  Training loss: 1.5398...  5.8520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1233...  Training loss: 1.5738...  5.0750 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1234...  Training loss: 1.5207...  5.4520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1235...  Training loss: 1.5581...  5.1860 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1236...  Training loss: 1.5376...  5.0920 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1237...  Training loss: 1.5488...  4.9360 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1238...  Training loss: 1.5842...  6.2520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1239...  Training loss: 1.5353...  6.5770 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1240...  Training loss: 1.6086...  6.2530 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1241...  Training loss: 1.5625...  6.5000 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1242...  Training loss: 1.5537...  6.3300 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1243...  Training loss: 1.5495...  6.5699 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1244...  Training loss: 1.5707...  6.5553 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1245...  Training loss: 1.5884...  6.0424 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1246...  Training loss: 1.5300...  5.9674 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1247...  Training loss: 1.5402...  6.4294 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1248...  Training loss: 1.5887...  5.9394 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1249...  Training loss: 1.5618...  5.2465 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1250...  Training loss: 1.6213...  5.0615 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1251...  Training loss: 1.5791...  5.2425 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1252...  Training loss: 1.5637...  5.3235 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1253...  Training loss: 1.5603...  4.9165 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1254...  Training loss: 1.5704...  4.8875 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1255...  Training loss: 1.5854...  4.7555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1256...  Training loss: 1.5387...  5.3525 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1257...  Training loss: 1.5533...  6.1424 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1258...  Training loss: 1.5541...  6.3814 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1259...  Training loss: 1.5999...  7.9842 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1260...  Training loss: 1.5740...  6.3374 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1261...  Training loss: 1.5883...  5.2085 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1262...  Training loss: 1.5372...  5.3225 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1263...  Training loss: 1.5534...  5.4085 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1264...  Training loss: 1.5746...  6.0474 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1265...  Training loss: 1.5509...  6.2314 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1266...  Training loss: 1.5433...  5.5244 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1267...  Training loss: 1.5082...  5.3875 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1268...  Training loss: 1.5506...  5.2615 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1269...  Training loss: 1.5193...  5.9984 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1270...  Training loss: 1.5670...  5.7474 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1271...  Training loss: 1.5172...  5.0885 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1272...  Training loss: 1.5487...  5.4305 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1273...  Training loss: 1.5251...  6.4494 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1274...  Training loss: 1.5428...  6.9453 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1275...  Training loss: 1.5247...  7.4173 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1276...  Training loss: 1.5372...  6.8153 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1277...  Training loss: 1.5200...  5.9484 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1278...  Training loss: 1.5612...  6.2054 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1279...  Training loss: 1.5233...  5.9724 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1280...  Training loss: 1.5319...  6.0114 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1281...  Training loss: 1.5293...  6.3134 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1282...  Training loss: 1.5229...  5.6444 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1283...  Training loss: 1.5281...  5.4535 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1284...  Training loss: 1.5537...  5.3475 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1285...  Training loss: 1.5611...  5.7174 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1286...  Training loss: 1.5180...  5.9424 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1287...  Training loss: 1.5314...  5.4555 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1288...  Training loss: 1.5055...  5.3665 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1289...  Training loss: 1.5444...  5.2415 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1290...  Training loss: 1.5375...  5.4705 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1291...  Training loss: 1.5329...  5.6164 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1292...  Training loss: 1.5354...  5.8764 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1293...  Training loss: 1.5274...  5.3945 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1294...  Training loss: 1.5430...  5.7024 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1295...  Training loss: 1.5338...  5.1346 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1296...  Training loss: 1.5306...  5.1870 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1297...  Training loss: 1.5389...  4.6790 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1298...  Training loss: 1.5497...  5.6560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1299...  Training loss: 1.5228...  5.3890 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7/20...  Training Step: 1300...  Training loss: 1.5244...  5.9450 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1301...  Training loss: 1.5247...  6.0800 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1302...  Training loss: 1.5229...  5.5270 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1303...  Training loss: 1.5120...  5.4210 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1304...  Training loss: 1.4925...  5.2570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1305...  Training loss: 1.5375...  5.2520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1306...  Training loss: 1.5385...  5.0090 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1307...  Training loss: 1.5264...  4.9230 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1308...  Training loss: 1.5340...  4.8980 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1309...  Training loss: 1.5368...  4.9300 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1310...  Training loss: 1.5018...  5.4230 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1311...  Training loss: 1.4880...  4.8880 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1312...  Training loss: 1.5510...  5.2580 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1313...  Training loss: 1.5192...  4.7810 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1314...  Training loss: 1.4832...  4.7750 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1315...  Training loss: 1.5529...  4.8180 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1316...  Training loss: 1.5388...  4.6930 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1317...  Training loss: 1.5107...  4.7180 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1318...  Training loss: 1.5015...  4.9330 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1319...  Training loss: 1.4839...  4.6550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1320...  Training loss: 1.4966...  4.8060 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1321...  Training loss: 1.5415...  5.6420 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1322...  Training loss: 1.5467...  6.4220 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1323...  Training loss: 1.5301...  6.7330 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1324...  Training loss: 1.5407...  6.4590 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1325...  Training loss: 1.5523...  5.9730 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1326...  Training loss: 1.5328...  5.2630 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1327...  Training loss: 1.5362...  5.2250 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1328...  Training loss: 1.5273...  5.1660 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1329...  Training loss: 1.5757...  5.3990 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1330...  Training loss: 1.5298...  4.8320 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1331...  Training loss: 1.5231...  4.9740 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1332...  Training loss: 1.5502...  4.8410 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1333...  Training loss: 1.5056...  4.7570 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1334...  Training loss: 1.5621...  4.9380 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1335...  Training loss: 1.5424...  5.0190 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1336...  Training loss: 1.5688...  5.2220 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1337...  Training loss: 1.5468...  6.1730 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1338...  Training loss: 1.5092...  5.2810 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1339...  Training loss: 1.4932...  5.2240 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1340...  Training loss: 1.5243...  5.3410 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1341...  Training loss: 1.5363...  5.4750 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1342...  Training loss: 1.5241...  4.9980 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1343...  Training loss: 1.5229...  4.9010 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1344...  Training loss: 1.5128...  4.7860 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1345...  Training loss: 1.5291...  4.8330 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1346...  Training loss: 1.5197...  5.4510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1347...  Training loss: 1.4845...  5.2030 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1348...  Training loss: 1.5406...  5.8750 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1349...  Training loss: 1.5560...  5.4930 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1350...  Training loss: 1.5212...  5.9070 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1351...  Training loss: 1.5408...  5.5450 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1352...  Training loss: 1.5406...  5.2070 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1353...  Training loss: 1.5365...  4.9770 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1354...  Training loss: 1.5254...  5.1600 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1355...  Training loss: 1.5588...  5.3500 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1356...  Training loss: 1.5962...  5.5190 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1357...  Training loss: 1.5149...  5.6420 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1358...  Training loss: 1.5149...  5.6100 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1359...  Training loss: 1.5196...  6.5680 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1360...  Training loss: 1.5015...  5.6560 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1361...  Training loss: 1.5372...  5.2990 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1362...  Training loss: 1.5219...  5.4720 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1363...  Training loss: 1.5415...  5.5460 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1364...  Training loss: 1.5010...  5.4270 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1365...  Training loss: 1.5156...  6.1640 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1366...  Training loss: 1.5457...  5.6680 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1367...  Training loss: 1.4897...  5.4410 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1368...  Training loss: 1.4866...  5.6520 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1369...  Training loss: 1.4868...  5.6960 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1370...  Training loss: 1.5040...  5.5920 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1371...  Training loss: 1.5142...  5.1140 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1372...  Training loss: 1.5118...  5.3790 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1373...  Training loss: 1.5161...  7.1510 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1374...  Training loss: 1.4963...  6.5320 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1375...  Training loss: 1.5418...  6.6410 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1376...  Training loss: 1.5067...  7.5890 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1377...  Training loss: 1.5056...  6.7410 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1378...  Training loss: 1.5075...  6.9410 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1379...  Training loss: 1.5002...  7.2900 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1380...  Training loss: 1.4852...  5.5550 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1381...  Training loss: 1.5220...  5.8830 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1382...  Training loss: 1.4898...  5.6890 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1383...  Training loss: 1.4835...  6.0940 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1384...  Training loss: 1.5134...  5.8620 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1385...  Training loss: 1.5066...  5.9150 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1386...  Training loss: 1.4931...  5.4810 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1387...  Training loss: 1.6514...  5.9520 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1388...  Training loss: 1.5330...  5.5120 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1389...  Training loss: 1.5083...  5.8220 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1390...  Training loss: 1.5260...  5.9370 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1391...  Training loss: 1.5016...  6.6650 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1392...  Training loss: 1.4764...  6.3960 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1393...  Training loss: 1.5209...  5.7990 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1394...  Training loss: 1.5074...  6.1040 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1395...  Training loss: 1.5289...  5.8270 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1396...  Training loss: 1.4955...  6.5770 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1397...  Training loss: 1.4874...  5.8960 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1398...  Training loss: 1.4983...  5.6270 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 1399...  Training loss: 1.5112...  5.2840 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1400...  Training loss: 1.5324...  5.6760 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1401...  Training loss: 1.4999...  7.7550 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1402...  Training loss: 1.4795...  7.4910 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1403...  Training loss: 1.5224...  6.1380 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1404...  Training loss: 1.5237...  5.5960 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1405...  Training loss: 1.5071...  5.6380 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1406...  Training loss: 1.5245...  5.4910 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1407...  Training loss: 1.5106...  5.2510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1408...  Training loss: 1.5385...  5.5010 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1409...  Training loss: 1.4974...  5.5370 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1410...  Training loss: 1.5267...  6.1490 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1411...  Training loss: 1.5123...  6.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1412...  Training loss: 1.4714...  5.4180 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1413...  Training loss: 1.4715...  4.8400 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1414...  Training loss: 1.5188...  5.2780 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1415...  Training loss: 1.5198...  5.2870 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1416...  Training loss: 1.5246...  5.9890 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1417...  Training loss: 1.5125...  7.2360 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1418...  Training loss: 1.4922...  5.8610 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1419...  Training loss: 1.5189...  5.9090 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1420...  Training loss: 1.5287...  5.5250 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1421...  Training loss: 1.5135...  6.1970 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1422...  Training loss: 1.5130...  6.2610 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1423...  Training loss: 1.4787...  5.1650 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1424...  Training loss: 1.4718...  5.1070 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1425...  Training loss: 1.4583...  6.1230 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1426...  Training loss: 1.4835...  8.2523 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1427...  Training loss: 1.4770...  5.0425 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1428...  Training loss: 1.5402...  7.1806 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1429...  Training loss: 1.4861...  5.7509 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1430...  Training loss: 1.4828...  5.5548 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1431...  Training loss: 1.5190...  6.5673 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1432...  Training loss: 1.4717...  5.5688 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1433...  Training loss: 1.4890...  4.6253 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1434...  Training loss: 1.4889...  4.9385 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1435...  Training loss: 1.4833...  5.0475 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1436...  Training loss: 1.5169...  4.8934 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1437...  Training loss: 1.4795...  4.8194 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1438...  Training loss: 1.5367...  5.2016 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1439...  Training loss: 1.4932...  5.0325 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1440...  Training loss: 1.4970...  5.2026 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1441...  Training loss: 1.4896...  6.1751 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1442...  Training loss: 1.5021...  6.0160 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1443...  Training loss: 1.5173...  5.3527 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1444...  Training loss: 1.4760...  4.9235 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1445...  Training loss: 1.4770...  4.9965 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1446...  Training loss: 1.5271...  4.9445 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1447...  Training loss: 1.4940...  5.3787 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1448...  Training loss: 1.5542...  5.2106 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1449...  Training loss: 1.5246...  5.1366 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1450...  Training loss: 1.4928...  5.4127 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1451...  Training loss: 1.4893...  5.2046 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1452...  Training loss: 1.5217...  5.0625 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1453...  Training loss: 1.5133...  4.9825 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1454...  Training loss: 1.4781...  4.9555 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1455...  Training loss: 1.4966...  4.8654 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1456...  Training loss: 1.4974...  4.8284 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1457...  Training loss: 1.5450...  4.6853 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1458...  Training loss: 1.5219...  4.7734 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1459...  Training loss: 1.5396...  5.7489 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1460...  Training loss: 1.4844...  4.9445 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1461...  Training loss: 1.4880...  4.4772 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1462...  Training loss: 1.5080...  4.8244 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1463...  Training loss: 1.4877...  5.3177 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1464...  Training loss: 1.4834...  6.3452 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1465...  Training loss: 1.4504...  4.9785 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1466...  Training loss: 1.5069...  4.4492 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1467...  Training loss: 1.4508...  4.9645 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1468...  Training loss: 1.4993...  5.5718 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1469...  Training loss: 1.4540...  5.8889 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1470...  Training loss: 1.4833...  4.4452 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1471...  Training loss: 1.4649...  4.3472 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1472...  Training loss: 1.4771...  4.6783 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1473...  Training loss: 1.4532...  5.4997 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1474...  Training loss: 1.4669...  7.3317 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1475...  Training loss: 1.4470...  4.7814 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1476...  Training loss: 1.5053...  4.8804 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1477...  Training loss: 1.4746...  5.8659 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1478...  Training loss: 1.4755...  6.4462 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1479...  Training loss: 1.4644...  6.2571 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1480...  Training loss: 1.4632...  4.4232 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1481...  Training loss: 1.4648...  4.9805 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1482...  Training loss: 1.4935...  8.1471 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1483...  Training loss: 1.4722...  5.4816 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1484...  Training loss: 1.4545...  4.7139 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1485...  Training loss: 1.4682...  5.3731 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1486...  Training loss: 1.4464...  5.6981 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1487...  Training loss: 1.4881...  6.9244 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1488...  Training loss: 1.4812...  4.6059 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1489...  Training loss: 1.4809...  4.4899 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1490...  Training loss: 1.4684...  4.8040 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1491...  Training loss: 1.4681...  5.2310 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1492...  Training loss: 1.4801...  4.8620 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1493...  Training loss: 1.4822...  5.7602 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1494...  Training loss: 1.4768...  5.3221 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1495...  Training loss: 1.4726...  4.3559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1496...  Training loss: 1.5043...  5.6411 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1497...  Training loss: 1.4719...  5.3621 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8/20...  Training Step: 1498...  Training loss: 1.4830...  5.1980 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1499...  Training loss: 1.4852...  6.1582 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1500...  Training loss: 1.4737...  6.1712 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1501...  Training loss: 1.4532...  4.2919 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1502...  Training loss: 1.4369...  4.3269 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1503...  Training loss: 1.4744...  4.7329 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1504...  Training loss: 1.4765...  4.9690 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1505...  Training loss: 1.4708...  6.8944 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1506...  Training loss: 1.4679...  5.7021 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1507...  Training loss: 1.4777...  4.4039 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1508...  Training loss: 1.4393...  4.5099 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1509...  Training loss: 1.4307...  4.8330 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1510...  Training loss: 1.4892...  6.1062 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1511...  Training loss: 1.4690...  5.1870 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1512...  Training loss: 1.4348...  4.2869 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1513...  Training loss: 1.4859...  4.4869 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1514...  Training loss: 1.4827...  4.6709 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1515...  Training loss: 1.4643...  4.7720 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1516...  Training loss: 1.4506...  5.1180 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1517...  Training loss: 1.4290...  5.8192 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1518...  Training loss: 1.4599...  4.4129 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1519...  Training loss: 1.5030...  5.0000 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1520...  Training loss: 1.4976...  5.4051 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1521...  Training loss: 1.4857...  4.9310 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1522...  Training loss: 1.4833...  5.0650 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1523...  Training loss: 1.5034...  4.8300 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1524...  Training loss: 1.4855...  5.3901 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1525...  Training loss: 1.4845...  5.8412 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1526...  Training loss: 1.4743...  4.2559 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1527...  Training loss: 1.5329...  4.6079 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1528...  Training loss: 1.4856...  4.6859 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1529...  Training loss: 1.4647...  4.8200 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1530...  Training loss: 1.5108...  4.7139 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1531...  Training loss: 1.4570...  5.9342 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1532...  Training loss: 1.5041...  5.0630 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1533...  Training loss: 1.4869...  4.4689 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1534...  Training loss: 1.5104...  4.8440 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1535...  Training loss: 1.4883...  5.1510 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1536...  Training loss: 1.4661...  5.1940 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1537...  Training loss: 1.4441...  6.2563 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1538...  Training loss: 1.4679...  5.0570 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1539...  Training loss: 1.4776...  4.6849 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1540...  Training loss: 1.4768...  5.1400 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1541...  Training loss: 1.4660...  5.8560 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1542...  Training loss: 1.4624...  8.2088 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1543...  Training loss: 1.4769...  4.8165 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1544...  Training loss: 1.4607...  5.3655 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1545...  Training loss: 1.4315...  5.7386 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1546...  Training loss: 1.4893...  6.8577 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1547...  Training loss: 1.5042...  5.9676 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1548...  Training loss: 1.4682...  5.4405 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1549...  Training loss: 1.4661...  5.6026 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1550...  Training loss: 1.4687...  7.4477 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1551...  Training loss: 1.4759...  4.9525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1552...  Training loss: 1.4724...  4.4864 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1553...  Training loss: 1.4996...  4.6125 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1554...  Training loss: 1.5418...  4.8185 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1555...  Training loss: 1.4669...  4.8435 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1556...  Training loss: 1.4647...  6.2846 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1557...  Training loss: 1.4587...  4.6405 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1558...  Training loss: 1.4522...  4.6215 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1559...  Training loss: 1.4944...  4.7255 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1560...  Training loss: 1.4721...  4.8365 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1561...  Training loss: 1.4936...  4.7755 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1562...  Training loss: 1.4603...  5.8756 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1563...  Training loss: 1.4535...  5.5886 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1564...  Training loss: 1.4920...  4.9375 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1565...  Training loss: 1.4454...  5.1125 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1566...  Training loss: 1.4418...  4.8135 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1567...  Training loss: 1.4426...  5.3525 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1568...  Training loss: 1.4575...  5.2785 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1569...  Training loss: 1.4704...  5.8906 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1570...  Training loss: 1.4452...  4.5295 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1571...  Training loss: 1.4616...  4.4684 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1572...  Training loss: 1.4470...  4.9495 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1573...  Training loss: 1.4895...  6.8227 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1574...  Training loss: 1.4550...  6.2786 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1575...  Training loss: 1.4531...  4.4494 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1576...  Training loss: 1.4519...  4.7595 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1577...  Training loss: 1.4502...  5.2835 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1578...  Training loss: 1.4431...  5.5706 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1579...  Training loss: 1.4611...  6.8737 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1580...  Training loss: 1.4408...  4.7375 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1581...  Training loss: 1.4244...  5.2705 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1582...  Training loss: 1.4699...  5.2925 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1583...  Training loss: 1.4606...  5.0365 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1584...  Training loss: 1.4419...  5.2465 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1585...  Training loss: 1.6364...  5.3625 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1586...  Training loss: 1.4730...  5.6456 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1587...  Training loss: 1.4580...  7.5488 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1588...  Training loss: 1.4773...  4.8405 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1589...  Training loss: 1.4428...  5.4905 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1590...  Training loss: 1.4344...  6.8127 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1591...  Training loss: 1.4700...  7.6878 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1592...  Training loss: 1.4562...  5.7556 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1593...  Training loss: 1.4688...  6.3716 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1594...  Training loss: 1.4511...  9.4009 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1595...  Training loss: 1.4340...  11.9142 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1596...  Training loss: 1.4606...  7.6468 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 1597...  Training loss: 1.4543...  6.3476 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1598...  Training loss: 1.4940...  7.4397 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1599...  Training loss: 1.4611...  5.4005 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1600...  Training loss: 1.4339...  4.9005 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1601...  Training loss: 1.4788...  5.5366 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1602...  Training loss: 1.4917...  5.7856 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1603...  Training loss: 1.4660...  7.4867 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1604...  Training loss: 1.4859...  5.5376 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1605...  Training loss: 1.4499...  4.6715 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1606...  Training loss: 1.4649...  4.7845 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1607...  Training loss: 1.4561...  5.2815 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1608...  Training loss: 1.4702...  7.0787 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1609...  Training loss: 1.4590...  5.8156 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1610...  Training loss: 1.4180...  4.4884 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1611...  Training loss: 1.4272...  4.5004 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1612...  Training loss: 1.4727...  4.7545 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1613...  Training loss: 1.4729...  4.8095 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1614...  Training loss: 1.4766...  4.7065 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1615...  Training loss: 1.4450...  5.9216 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1616...  Training loss: 1.4267...  4.9831 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1617...  Training loss: 1.4674...  4.6953 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1618...  Training loss: 1.4705...  5.5893 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1619...  Training loss: 1.4564...  5.0410 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1620...  Training loss: 1.4478...  5.3454 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1621...  Training loss: 1.4330...  4.9926 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1622...  Training loss: 1.4146...  5.0229 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1623...  Training loss: 1.4153...  4.8828 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1624...  Training loss: 1.4427...  5.9259 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1625...  Training loss: 1.4199...  8.0443 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1626...  Training loss: 1.4821...  6.9528 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1627...  Training loss: 1.4406...  5.2940 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1628...  Training loss: 1.4212...  5.9339 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1629...  Training loss: 1.4639...  5.8573 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1630...  Training loss: 1.4148...  6.0962 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1631...  Training loss: 1.4521...  7.5011 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1632...  Training loss: 1.4373...  5.0229 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1633...  Training loss: 1.4395...  4.8102 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1634...  Training loss: 1.4765...  5.1106 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1635...  Training loss: 1.4302...  5.0561 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1636...  Training loss: 1.4900...  7.9032 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1637...  Training loss: 1.4580...  8.2216 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1638...  Training loss: 1.4594...  4.9060 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1639...  Training loss: 1.4421...  5.2668 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1640...  Training loss: 1.4476...  5.2133 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1641...  Training loss: 1.4749...  6.5104 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1642...  Training loss: 1.4277...  5.7304 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1643...  Training loss: 1.4197...  4.9614 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1644...  Training loss: 1.4889...  4.7578 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1645...  Training loss: 1.4593...  5.7566 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1646...  Training loss: 1.5081...  8.7124 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1647...  Training loss: 1.4735...  5.7656 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1648...  Training loss: 1.4612...  5.5510 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1649...  Training loss: 1.4461...  6.2907 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1650...  Training loss: 1.4598...  7.1060 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1651...  Training loss: 1.4615...  6.4157 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1652...  Training loss: 1.4245...  5.4502 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1653...  Training loss: 1.4488...  5.2537 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1654...  Training loss: 1.4378...  7.4335 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1655...  Training loss: 1.4924...  7.2985 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1656...  Training loss: 1.4699...  4.8102 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1657...  Training loss: 1.4829...  4.7296 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1658...  Training loss: 1.4295...  5.2083 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1659...  Training loss: 1.4440...  6.7785 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1660...  Training loss: 1.4739...  6.0025 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1661...  Training loss: 1.4381...  4.3335 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1662...  Training loss: 1.4376...  4.3779 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1663...  Training loss: 1.4000...  4.7538 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1664...  Training loss: 1.4570...  4.9564 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1665...  Training loss: 1.3879...  6.0659 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1666...  Training loss: 1.4443...  4.9281 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1667...  Training loss: 1.4152...  4.3214 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1668...  Training loss: 1.4440...  4.5411 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1669...  Training loss: 1.4183...  5.3387 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1670...  Training loss: 1.4382...  5.4926 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1671...  Training loss: 1.4194...  7.6038 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1672...  Training loss: 1.4233...  4.9789 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1673...  Training loss: 1.4119...  5.1244 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1674...  Training loss: 1.4551...  5.5137 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1675...  Training loss: 1.4221...  8.2750 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1676...  Training loss: 1.4359...  5.9291 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1677...  Training loss: 1.4152...  4.3477 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1678...  Training loss: 1.4233...  4.3979 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1679...  Training loss: 1.4179...  4.6899 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1680...  Training loss: 1.4440...  6.4719 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1681...  Training loss: 1.4387...  6.1649 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1682...  Training loss: 1.3989...  4.4240 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1683...  Training loss: 1.4103...  4.3307 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1684...  Training loss: 1.3980...  5.0300 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1685...  Training loss: 1.4330...  5.9773 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1686...  Training loss: 1.4315...  8.2680 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1687...  Training loss: 1.4411...  5.3291 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1688...  Training loss: 1.4243...  5.7936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1689...  Training loss: 1.4199...  5.9191 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1690...  Training loss: 1.4266...  9.1831 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1691...  Training loss: 1.4246...  7.1191 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1692...  Training loss: 1.4318...  6.1157 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1693...  Training loss: 1.4247...  6.0174 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1694...  Training loss: 1.4470...  7.0409 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1695...  Training loss: 1.4196...  5.4956 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9/20...  Training Step: 1696...  Training loss: 1.4353...  4.5083 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1697...  Training loss: 1.4380...  5.0300 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1698...  Training loss: 1.4231...  5.2719 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1699...  Training loss: 1.4114...  6.9275 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1700...  Training loss: 1.3949...  5.1525 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1701...  Training loss: 1.4229...  5.6040 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1702...  Training loss: 1.4276...  5.4605 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1703...  Training loss: 1.4280...  6.2381 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1704...  Training loss: 1.4266...  6.7780 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1705...  Training loss: 1.4302...  4.8414 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1706...  Training loss: 1.3983...  5.3170 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1707...  Training loss: 1.3850...  4.9458 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1708...  Training loss: 1.4436...  5.0913 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1709...  Training loss: 1.4214...  4.8936 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1710...  Training loss: 1.3858...  4.8695 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1711...  Training loss: 1.4477...  7.1924 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1712...  Training loss: 1.4393...  5.2137 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1713...  Training loss: 1.4180...  6.0625 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1714...  Training loss: 1.3903...  6.9044 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1715...  Training loss: 1.3861...  5.2719 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1716...  Training loss: 1.4129...  4.5394 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1717...  Training loss: 1.4546...  4.6477 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1718...  Training loss: 1.4443...  4.6989 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1719...  Training loss: 1.4435...  6.0445 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1720...  Training loss: 1.4357...  6.1468 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1721...  Training loss: 1.4657...  4.6644 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1722...  Training loss: 1.4504...  4.4961 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1723...  Training loss: 1.4365...  4.7565 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1724...  Training loss: 1.4363...  6.1541 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1725...  Training loss: 1.4850...  4.7515 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1726...  Training loss: 1.4451...  4.5532 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1727...  Training loss: 1.4384...  5.0781 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1728...  Training loss: 1.4563...  6.8423 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1729...  Training loss: 1.4156...  4.9789 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1730...  Training loss: 1.4500...  4.4520 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1731...  Training loss: 1.4384...  4.7024 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1732...  Training loss: 1.4677...  4.6924 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1733...  Training loss: 1.4597...  4.7535 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1734...  Training loss: 1.4201...  5.3115 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1735...  Training loss: 1.3988...  4.9920 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1736...  Training loss: 1.4254...  5.0611 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1737...  Training loss: 1.4452...  4.9078 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1738...  Training loss: 1.4292...  4.8858 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1739...  Training loss: 1.4341...  4.8297 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1740...  Training loss: 1.4254...  4.8637 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1741...  Training loss: 1.4395...  5.0671 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1742...  Training loss: 1.4218...  5.9146 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1743...  Training loss: 1.3955...  5.4608 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1744...  Training loss: 1.4526...  4.7776 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1745...  Training loss: 1.4524...  4.9639 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1746...  Training loss: 1.4215...  4.8467 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1747...  Training loss: 1.4170...  4.8718 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1748...  Training loss: 1.4313...  4.9799 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1749...  Training loss: 1.4324...  5.1583 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1750...  Training loss: 1.4253...  7.4324 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1751...  Training loss: 1.4560...  5.4688 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1752...  Training loss: 1.4968...  4.9860 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1753...  Training loss: 1.4169...  5.1773 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1754...  Training loss: 1.4291...  6.2252 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1755...  Training loss: 1.4299...  8.5995 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1756...  Training loss: 1.4091...  5.8575 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1757...  Training loss: 1.4609...  7.0056 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1758...  Training loss: 1.4363...  5.3596 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1759...  Training loss: 1.4375...  7.3522 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1760...  Training loss: 1.4052...  4.8567 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1761...  Training loss: 1.4219...  4.5542 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1762...  Training loss: 1.4617...  5.0230 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1763...  Training loss: 1.4043...  5.9086 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1764...  Training loss: 1.3995...  5.4157 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1765...  Training loss: 1.4027...  4.4630 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1766...  Training loss: 1.4158...  4.5392 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1767...  Training loss: 1.4129...  4.6934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1768...  Training loss: 1.4136...  4.7666 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1769...  Training loss: 1.4152...  4.8197 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1770...  Training loss: 1.4062...  5.7934 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1771...  Training loss: 1.4498...  5.4127 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1772...  Training loss: 1.4170...  4.5702 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1773...  Training loss: 1.4224...  5.1513 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1774...  Training loss: 1.4139...  5.0591 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1775...  Training loss: 1.4092...  6.3304 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1776...  Training loss: 1.3949...  4.8147 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1777...  Training loss: 1.4266...  4.2977 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1778...  Training loss: 1.4056...  4.5562 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1779...  Training loss: 1.3990...  5.1752 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1780...  Training loss: 1.4291...  5.4055 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1781...  Training loss: 1.4199...  6.5408 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1782...  Training loss: 1.3991...  5.4445 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1783...  Training loss: 1.5871...  4.4613 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1784...  Training loss: 1.4424...  4.6165 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1785...  Training loss: 1.4216...  5.1572 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1786...  Training loss: 1.4443...  5.0370 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1787...  Training loss: 1.4009...  5.2062 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1788...  Training loss: 1.3962...  5.1061 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1789...  Training loss: 1.4288...  5.1301 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1790...  Training loss: 1.4099...  4.8979 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1791...  Training loss: 1.4367...  4.6376 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1792...  Training loss: 1.4233...  6.2515 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1793...  Training loss: 1.4135...  5.3124 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1794...  Training loss: 1.4183...  4.4293 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 1795...  Training loss: 1.4312...  4.5454 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1796...  Training loss: 1.4489...  4.7627 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1797...  Training loss: 1.4217...  4.7387 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1798...  Training loss: 1.3988...  4.7647 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1799...  Training loss: 1.4375...  5.2523 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1800...  Training loss: 1.4514...  7.5561 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1801...  Training loss: 1.4288...  6.0138 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1802...  Training loss: 1.4607...  5.1316 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1803...  Training loss: 1.4230...  5.3058 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1804...  Training loss: 1.4454...  5.0290 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1805...  Training loss: 1.4157...  5.1216 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1806...  Training loss: 1.4393...  4.8468 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1807...  Training loss: 1.4292...  4.7924 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1808...  Training loss: 1.3788...  4.7230 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1809...  Training loss: 1.3771...  4.7924 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1810...  Training loss: 1.4438...  5.3299 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1811...  Training loss: 1.4375...  5.6702 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1812...  Training loss: 1.4389...  5.6420 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1813...  Training loss: 1.4054...  5.6953 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1814...  Training loss: 1.3946...  5.6621 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1815...  Training loss: 1.4199...  5.5957 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1816...  Training loss: 1.4265...  5.1407 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1817...  Training loss: 1.4051...  5.3219 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1818...  Training loss: 1.4105...  5.1296 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1819...  Training loss: 1.3923...  5.0390 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1820...  Training loss: 1.3798...  4.8438 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1821...  Training loss: 1.3638...  4.8146 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1822...  Training loss: 1.3982...  4.8206 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1823...  Training loss: 1.3834...  4.9162 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1824...  Training loss: 1.4406...  4.7501 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1825...  Training loss: 1.3980...  5.2243 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1826...  Training loss: 1.3822...  5.9269 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1827...  Training loss: 1.4365...  6.3527 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1828...  Training loss: 1.3920...  5.1780 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1829...  Training loss: 1.4081...  5.0068 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1830...  Training loss: 1.4018...  5.2665 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1831...  Training loss: 1.4050...  5.2706 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1832...  Training loss: 1.4223...  5.2555 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1833...  Training loss: 1.3865...  5.0501 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1834...  Training loss: 1.4532...  5.2555 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1835...  Training loss: 1.4163...  5.0652 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1836...  Training loss: 1.4177...  4.9837 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1837...  Training loss: 1.3982...  6.6033 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1838...  Training loss: 1.4058...  4.7340 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1839...  Training loss: 1.4251...  4.7612 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1840...  Training loss: 1.3871...  4.8488 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1841...  Training loss: 1.3876...  5.4598 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1842...  Training loss: 1.4547...  5.7326 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1843...  Training loss: 1.4257...  5.5142 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1844...  Training loss: 1.4671...  5.6984 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1845...  Training loss: 1.4342...  5.3924 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1846...  Training loss: 1.4254...  5.0008 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1847...  Training loss: 1.4042...  5.2434 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1848...  Training loss: 1.4226...  5.3088 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1849...  Training loss: 1.4261...  4.9273 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1850...  Training loss: 1.3889...  5.3501 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1851...  Training loss: 1.4074...  4.7471 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1852...  Training loss: 1.3954...  4.7481 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1853...  Training loss: 1.4599...  4.7210 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1854...  Training loss: 1.4253...  4.7340 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1855...  Training loss: 1.4356...  4.9525 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1856...  Training loss: 1.3996...  5.4205 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1857...  Training loss: 1.4075...  6.0225 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1858...  Training loss: 1.4307...  5.8403 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1859...  Training loss: 1.4121...  5.8960 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1860...  Training loss: 1.4031...  5.5151 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1861...  Training loss: 1.3707...  5.9937 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1862...  Training loss: 1.4164...  5.9696 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1863...  Training loss: 1.3681...  5.6295 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1864...  Training loss: 1.4081...  5.5914 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1865...  Training loss: 1.3817...  5.7971 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1866...  Training loss: 1.4079...  5.3887 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1867...  Training loss: 1.3959...  4.8740 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1868...  Training loss: 1.3924...  4.9603 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1869...  Training loss: 1.3755...  5.0145 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1870...  Training loss: 1.3868...  5.1449 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1871...  Training loss: 1.3681...  5.6325 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1872...  Training loss: 1.4120...  5.2643 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1873...  Training loss: 1.3797...  5.2924 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1874...  Training loss: 1.3972...  5.4459 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1875...  Training loss: 1.3886...  5.3677 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1876...  Training loss: 1.3901...  5.4539 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1877...  Training loss: 1.3809...  5.7589 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1878...  Training loss: 1.4124...  6.2074 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1879...  Training loss: 1.4079...  5.6606 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1880...  Training loss: 1.3627...  5.2493 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1881...  Training loss: 1.3889...  5.2001 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1882...  Training loss: 1.3699...  5.3366 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1883...  Training loss: 1.3979...  5.0877 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1884...  Training loss: 1.3827...  5.1650 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1885...  Training loss: 1.4004...  4.8921 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1886...  Training loss: 1.3955...  4.9703 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1887...  Training loss: 1.3879...  4.9352 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1888...  Training loss: 1.4014...  5.0787 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1889...  Training loss: 1.3995...  5.1660 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1890...  Training loss: 1.4085...  5.6064 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1891...  Training loss: 1.3942...  5.4198 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1892...  Training loss: 1.4183...  5.3486 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20...  Training Step: 1893...  Training loss: 1.3849...  5.6596 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1894...  Training loss: 1.4068...  6.0529 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1895...  Training loss: 1.4021...  5.6195 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1896...  Training loss: 1.3900...  5.2814 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1897...  Training loss: 1.3775...  5.3095 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1898...  Training loss: 1.3553...  5.1118 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1899...  Training loss: 1.3980...  4.7065 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1900...  Training loss: 1.4069...  4.6964 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1901...  Training loss: 1.3932...  4.9423 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1902...  Training loss: 1.3975...  4.8249 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1903...  Training loss: 1.3935...  4.9593 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1904...  Training loss: 1.3580...  4.8770 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1905...  Training loss: 1.3515...  4.8038 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1906...  Training loss: 1.4046...  5.0205 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1907...  Training loss: 1.3843...  5.2252 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1908...  Training loss: 1.3553...  5.0877 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1909...  Training loss: 1.4173...  5.1479 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1910...  Training loss: 1.4092...  4.8058 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1911...  Training loss: 1.3820...  5.0346 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1912...  Training loss: 1.3648...  4.8389 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1913...  Training loss: 1.3472...  5.7469 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1914...  Training loss: 1.3814...  5.7499 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1915...  Training loss: 1.4250...  5.6395 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1916...  Training loss: 1.4189...  5.5882 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1917...  Training loss: 1.4033...  5.7320 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1918...  Training loss: 1.3983...  5.5897 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1919...  Training loss: 1.4297...  4.9905 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1920...  Training loss: 1.4119...  4.9474 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1921...  Training loss: 1.3985...  4.9915 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1922...  Training loss: 1.4030...  5.4174 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1923...  Training loss: 1.4582...  5.0115 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1924...  Training loss: 1.4079...  4.9844 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1925...  Training loss: 1.3895...  5.1748 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1926...  Training loss: 1.4282...  4.7880 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1927...  Training loss: 1.3790...  4.8812 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1928...  Training loss: 1.4152...  5.1688 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1929...  Training loss: 1.4010...  5.6959 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1930...  Training loss: 1.4242...  6.2912 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1931...  Training loss: 1.4091...  5.4594 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1932...  Training loss: 1.3943...  5.1728 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1933...  Training loss: 1.3604...  5.0847 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1934...  Training loss: 1.3846...  4.8211 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1935...  Training loss: 1.4046...  4.9434 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1936...  Training loss: 1.3934...  5.0416 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1937...  Training loss: 1.3965...  5.3933 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1938...  Training loss: 1.3881...  5.1227 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1939...  Training loss: 1.3941...  4.7740 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1940...  Training loss: 1.3892...  5.2761 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1941...  Training loss: 1.3645...  5.4634 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1942...  Training loss: 1.4180...  5.6639 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1943...  Training loss: 1.4291...  4.9333 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1944...  Training loss: 1.3911...  4.7389 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1945...  Training loss: 1.3938...  4.6728 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1946...  Training loss: 1.3994...  4.7570 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1947...  Training loss: 1.4038...  4.9404 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1948...  Training loss: 1.3924...  5.0225 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1949...  Training loss: 1.4232...  5.2981 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1950...  Training loss: 1.4639...  5.1909 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1951...  Training loss: 1.4017...  5.2831 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1952...  Training loss: 1.3963...  4.9073 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1953...  Training loss: 1.3855...  4.9614 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1954...  Training loss: 1.3763...  4.7840 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1955...  Training loss: 1.4196...  4.7710 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1956...  Training loss: 1.3913...  4.6878 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1957...  Training loss: 1.4015...  4.6758 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1958...  Training loss: 1.3694...  4.6177 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1959...  Training loss: 1.3839...  4.6417 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1960...  Training loss: 1.4116...  4.6517 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1961...  Training loss: 1.3724...  4.8993 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1962...  Training loss: 1.3711...  4.9714 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1963...  Training loss: 1.3703...  4.8411 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1964...  Training loss: 1.3816...  5.3823 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1965...  Training loss: 1.3934...  5.5657 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1966...  Training loss: 1.3919...  5.8954 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1967...  Training loss: 1.3865...  5.1829 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1968...  Training loss: 1.3824...  5.4394 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1969...  Training loss: 1.4171...  4.9404 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1970...  Training loss: 1.3793...  4.9023 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1971...  Training loss: 1.3862...  5.4334 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1972...  Training loss: 1.3788...  5.4795 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1973...  Training loss: 1.3798...  5.8382 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1974...  Training loss: 1.3681...  5.5033 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1975...  Training loss: 1.3864...  5.1557 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1976...  Training loss: 1.3723...  5.1437 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1977...  Training loss: 1.3655...  5.0846 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1978...  Training loss: 1.3983...  5.0075 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1979...  Training loss: 1.3882...  5.2018 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1980...  Training loss: 1.3754...  5.1327 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1981...  Training loss: 1.5719...  5.3982 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1982...  Training loss: 1.4308...  5.2429 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1983...  Training loss: 1.4115...  4.9744 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1984...  Training loss: 1.4146...  4.9965 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1985...  Training loss: 1.3856...  5.0766 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1986...  Training loss: 1.3696...  5.0656 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1987...  Training loss: 1.4068...  4.9424 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1988...  Training loss: 1.3879...  4.9704 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1989...  Training loss: 1.4103...  5.0355 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1990...  Training loss: 1.3933...  5.1958 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 1991...  Training loss: 1.3816...  4.9965 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1992...  Training loss: 1.3965...  4.9845 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1993...  Training loss: 1.3856...  4.9324 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1994...  Training loss: 1.4227...  5.3861 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1995...  Training loss: 1.3908...  5.8369 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1996...  Training loss: 1.3725...  6.3008 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1997...  Training loss: 1.4149...  6.4779 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1998...  Training loss: 1.4211...  5.5519 sec/batch\n",
      "Epoch: 11/20...  Training Step: 1999...  Training loss: 1.3902...  5.5329 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2000...  Training loss: 1.4110...  5.7876 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2001...  Training loss: 1.3876...  5.8436 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2002...  Training loss: 1.4039...  5.4180 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2003...  Training loss: 1.3882...  5.2622 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2004...  Training loss: 1.4077...  5.6198 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2005...  Training loss: 1.3874...  5.6308 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2006...  Training loss: 1.3490...  5.9544 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2007...  Training loss: 1.3557...  6.5008 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2008...  Training loss: 1.3997...  7.1282 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2009...  Training loss: 1.4067...  6.8145 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2010...  Training loss: 1.4070...  6.1682 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2011...  Training loss: 1.3743...  6.3480 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2012...  Training loss: 1.3514...  5.8785 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2013...  Training loss: 1.3929...  5.6008 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2014...  Training loss: 1.3915...  6.0024 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2015...  Training loss: 1.3714...  5.8595 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2016...  Training loss: 1.3877...  6.0324 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2017...  Training loss: 1.3563...  6.3740 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2018...  Training loss: 1.3509...  5.8186 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2019...  Training loss: 1.3321...  6.5488 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2020...  Training loss: 1.3681...  5.5329 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2021...  Training loss: 1.3593...  5.2003 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2022...  Training loss: 1.4226...  5.9315 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2023...  Training loss: 1.3671...  6.4089 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2024...  Training loss: 1.3653...  8.5246 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2025...  Training loss: 1.3927...  9.0690 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2026...  Training loss: 1.3634...  7.1981 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2027...  Training loss: 1.3794...  6.7795 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2028...  Training loss: 1.3744...  6.7556 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2029...  Training loss: 1.3692...  6.4719 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2030...  Training loss: 1.3964...  5.9834 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2031...  Training loss: 1.3615...  5.3721 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2032...  Training loss: 1.4275...  5.7666 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2033...  Training loss: 1.3852...  5.3691 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2034...  Training loss: 1.4010...  5.5359 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2035...  Training loss: 1.3801...  5.3771 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2036...  Training loss: 1.3832...  5.7896 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2037...  Training loss: 1.4039...  5.5369 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2038...  Training loss: 1.3632...  4.8786 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2039...  Training loss: 1.3616...  4.8636 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2040...  Training loss: 1.4215...  4.8766 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2041...  Training loss: 1.3885...  4.8886 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2042...  Training loss: 1.4338...  5.2742 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2043...  Training loss: 1.4034...  5.7517 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2044...  Training loss: 1.3851...  5.6418 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2045...  Training loss: 1.3785...  4.9615 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2046...  Training loss: 1.3969...  5.5372 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2047...  Training loss: 1.3935...  7.1809 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2048...  Training loss: 1.3624...  5.7563 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2049...  Training loss: 1.3787...  5.8703 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2050...  Training loss: 1.3759...  5.3431 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2051...  Training loss: 1.4308...  5.0420 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2052...  Training loss: 1.4083...  5.0470 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2053...  Training loss: 1.4072...  4.7639 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2054...  Training loss: 1.3645...  5.0370 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2055...  Training loss: 1.3872...  5.1250 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2056...  Training loss: 1.3971...  4.9740 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2057...  Training loss: 1.3740...  5.1270 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2058...  Training loss: 1.3715...  6.4026 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2059...  Training loss: 1.3367...  8.0732 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2060...  Training loss: 1.3929...  7.3880 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2061...  Training loss: 1.3458...  6.6467 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2062...  Training loss: 1.3750...  6.1715 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2063...  Training loss: 1.3391...  5.3741 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2064...  Training loss: 1.3629...  5.5932 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2065...  Training loss: 1.3678...  6.1855 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2066...  Training loss: 1.3758...  4.9750 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2067...  Training loss: 1.3579...  4.8489 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2068...  Training loss: 1.3639...  4.8019 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2069...  Training loss: 1.3371...  5.1010 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2070...  Training loss: 1.3799...  5.7003 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2071...  Training loss: 1.3581...  5.7843 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2072...  Training loss: 1.3612...  5.1491 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2073...  Training loss: 1.3490...  5.0400 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2074...  Training loss: 1.3596...  4.7769 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2075...  Training loss: 1.3660...  4.6699 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2076...  Training loss: 1.3753...  4.9060 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2077...  Training loss: 1.3813...  4.9100 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2078...  Training loss: 1.3373...  5.0870 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2079...  Training loss: 1.3495...  5.1551 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2080...  Training loss: 1.3482...  4.9810 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2081...  Training loss: 1.3793...  4.9880 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2082...  Training loss: 1.3658...  4.7609 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2083...  Training loss: 1.3696...  4.8289 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2084...  Training loss: 1.3685...  4.7229 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2085...  Training loss: 1.3610...  4.9990 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2086...  Training loss: 1.3637...  5.1751 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2087...  Training loss: 1.3724...  4.9930 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2088...  Training loss: 1.3706...  5.0680 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11/20...  Training Step: 2089...  Training loss: 1.3658...  5.6823 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2090...  Training loss: 1.3934...  6.3725 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2091...  Training loss: 1.3561...  8.0722 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2092...  Training loss: 1.3712...  6.9358 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2093...  Training loss: 1.3753...  7.0638 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2094...  Training loss: 1.3660...  5.9624 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2095...  Training loss: 1.3479...  5.6363 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2096...  Training loss: 1.3221...  5.5182 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2097...  Training loss: 1.3702...  5.3571 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2098...  Training loss: 1.3807...  5.2851 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2099...  Training loss: 1.3740...  5.2271 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2100...  Training loss: 1.3660...  5.7453 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2101...  Training loss: 1.3731...  5.7289 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2102...  Training loss: 1.3245...  5.9574 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2103...  Training loss: 1.3202...  6.0514 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2104...  Training loss: 1.3822...  5.7322 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2105...  Training loss: 1.3512...  5.8172 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2106...  Training loss: 1.3266...  6.0144 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2107...  Training loss: 1.3751...  7.0213 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2108...  Training loss: 1.3791...  6.6220 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2109...  Training loss: 1.3561...  5.5540 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2110...  Training loss: 1.3282...  5.6541 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2111...  Training loss: 1.3268...  5.4689 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2112...  Training loss: 1.3405...  5.1516 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2113...  Training loss: 1.3933...  5.5640 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2114...  Training loss: 1.3897...  5.8272 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2115...  Training loss: 1.3774...  5.2757 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2116...  Training loss: 1.3696...  5.1256 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2117...  Training loss: 1.4007...  5.3048 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2118...  Training loss: 1.3815...  5.4179 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2119...  Training loss: 1.3705...  6.1045 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2120...  Training loss: 1.3684...  6.9923 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2121...  Training loss: 1.4268...  6.2476 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2122...  Training loss: 1.3740...  5.9584 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2123...  Training loss: 1.3604...  5.5970 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2124...  Training loss: 1.4002...  5.3218 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2125...  Training loss: 1.3519...  5.2928 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2126...  Training loss: 1.3930...  5.3048 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2127...  Training loss: 1.3727...  5.2317 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2128...  Training loss: 1.4037...  5.1106 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2129...  Training loss: 1.3975...  5.2307 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2130...  Training loss: 1.3558...  5.2727 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2131...  Training loss: 1.3388...  5.8853 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2132...  Training loss: 1.3500...  6.0965 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2133...  Training loss: 1.3758...  6.2586 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2134...  Training loss: 1.3713...  6.3907 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2135...  Training loss: 1.3732...  6.2186 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2136...  Training loss: 1.3596...  5.5080 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2137...  Training loss: 1.3761...  5.7782 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2138...  Training loss: 1.3634...  5.1737 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2139...  Training loss: 1.3301...  5.2487 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2140...  Training loss: 1.3924...  5.4599 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2141...  Training loss: 1.4014...  4.9394 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2142...  Training loss: 1.3701...  4.6902 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2143...  Training loss: 1.3659...  4.6472 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2144...  Training loss: 1.3825...  4.8313 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2145...  Training loss: 1.3676...  4.7653 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2146...  Training loss: 1.3633...  4.8193 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2147...  Training loss: 1.3935...  4.9044 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2148...  Training loss: 1.4374...  4.7022 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2149...  Training loss: 1.3737...  4.7222 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2150...  Training loss: 1.3738...  4.8424 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2151...  Training loss: 1.3549...  4.7963 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2152...  Training loss: 1.3478...  5.2657 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2153...  Training loss: 1.3952...  4.9114 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2154...  Training loss: 1.3783...  4.9104 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2155...  Training loss: 1.3844...  4.9186 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2156...  Training loss: 1.3481...  4.8263 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2157...  Training loss: 1.3586...  5.0085 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2158...  Training loss: 1.3900...  5.1607 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2159...  Training loss: 1.3520...  5.3579 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2160...  Training loss: 1.3409...  5.0225 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2161...  Training loss: 1.3369...  4.9755 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2162...  Training loss: 1.3640...  4.8483 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2163...  Training loss: 1.3661...  4.7793 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2164...  Training loss: 1.3479...  4.7582 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2165...  Training loss: 1.3623...  4.9424 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2166...  Training loss: 1.3593...  4.9124 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2167...  Training loss: 1.3820...  5.0616 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2168...  Training loss: 1.3590...  5.2307 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2169...  Training loss: 1.3650...  5.1366 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2170...  Training loss: 1.3601...  5.0285 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2171...  Training loss: 1.3443...  5.0075 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2172...  Training loss: 1.3425...  4.9374 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2173...  Training loss: 1.3714...  4.8994 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2174...  Training loss: 1.3576...  5.0145 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2175...  Training loss: 1.3334...  4.8183 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2176...  Training loss: 1.3687...  4.8483 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2177...  Training loss: 1.3608...  4.7973 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2178...  Training loss: 1.3454...  4.7542 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2179...  Training loss: 1.5444...  4.7742 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2180...  Training loss: 1.3634...  4.8153 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2181...  Training loss: 1.3663...  4.9685 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2182...  Training loss: 1.3813...  5.3882 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2183...  Training loss: 1.3493...  5.7448 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2184...  Training loss: 1.3273...  5.2915 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2185...  Training loss: 1.3800...  5.0220 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2186...  Training loss: 1.3610...  4.9041 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 2187...  Training loss: 1.3804...  4.8882 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2188...  Training loss: 1.3546...  4.9740 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2189...  Training loss: 1.3470...  5.5102 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2190...  Training loss: 1.3720...  5.8736 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2191...  Training loss: 1.3649...  5.2047 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2192...  Training loss: 1.3910...  5.5821 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2193...  Training loss: 1.3541...  5.9135 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2194...  Training loss: 1.3311...  5.2356 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2195...  Training loss: 1.3808...  5.2156 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2196...  Training loss: 1.3921...  5.0669 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2197...  Training loss: 1.3776...  4.9980 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2198...  Training loss: 1.3889...  5.1368 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2199...  Training loss: 1.3584...  5.0629 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2200...  Training loss: 1.3786...  4.8932 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2201...  Training loss: 1.3562...  5.6200 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2202...  Training loss: 1.3681...  5.1318 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2203...  Training loss: 1.3645...  4.9371 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2204...  Training loss: 1.3187...  4.9591 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2205...  Training loss: 1.3351...  5.1028 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2206...  Training loss: 1.3763...  5.0529 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2207...  Training loss: 1.3670...  5.0150 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2208...  Training loss: 1.3821...  4.8353 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2209...  Training loss: 1.3587...  4.7933 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2210...  Training loss: 1.3305...  4.7354 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2211...  Training loss: 1.3685...  4.9770 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2212...  Training loss: 1.3598...  4.9930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2213...  Training loss: 1.3504...  5.3165 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2214...  Training loss: 1.3556...  5.1827 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2215...  Training loss: 1.3319...  4.9930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2216...  Training loss: 1.3228...  4.8382 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2217...  Training loss: 1.3175...  4.8852 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2218...  Training loss: 1.3359...  4.9910 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2219...  Training loss: 1.3312...  4.8852 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2220...  Training loss: 1.3950...  4.9331 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2221...  Training loss: 1.3486...  4.8442 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2222...  Training loss: 1.3356...  4.7534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2223...  Training loss: 1.3688...  5.0010 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2224...  Training loss: 1.3306...  4.7923 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2225...  Training loss: 1.3454...  4.7534 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2226...  Training loss: 1.3483...  4.9221 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2227...  Training loss: 1.3502...  4.9011 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2228...  Training loss: 1.3639...  5.0539 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2229...  Training loss: 1.3379...  5.1158 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2230...  Training loss: 1.3961...  5.3035 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2231...  Training loss: 1.3568...  5.3085 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2232...  Training loss: 1.3770...  5.0409 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2233...  Training loss: 1.3521...  5.1747 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2234...  Training loss: 1.3628...  4.8412 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2235...  Training loss: 1.3773...  4.8273 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2236...  Training loss: 1.3388...  4.7194 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2237...  Training loss: 1.3383...  4.7783 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2238...  Training loss: 1.4065...  4.8323 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2239...  Training loss: 1.3651...  5.0309 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2240...  Training loss: 1.4126...  4.8353 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2241...  Training loss: 1.3868...  4.8640 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2242...  Training loss: 1.3603...  4.9880 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2243...  Training loss: 1.3517...  5.8040 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2244...  Training loss: 1.3721...  5.6000 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2245...  Training loss: 1.3693...  5.9070 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2246...  Training loss: 1.3357...  4.9750 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2247...  Training loss: 1.3590...  4.8310 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2248...  Training loss: 1.3429...  4.6840 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2249...  Training loss: 1.4011...  4.7920 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2250...  Training loss: 1.3683...  4.9100 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2251...  Training loss: 1.3772...  5.5130 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2252...  Training loss: 1.3341...  5.3040 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2253...  Training loss: 1.3534...  5.1630 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2254...  Training loss: 1.3715...  5.6160 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2255...  Training loss: 1.3508...  5.5390 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2256...  Training loss: 1.3436...  5.9620 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2257...  Training loss: 1.3177...  6.0110 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2258...  Training loss: 1.3574...  5.8480 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2259...  Training loss: 1.3198...  6.3640 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2260...  Training loss: 1.3509...  5.6320 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2261...  Training loss: 1.3224...  5.3780 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2262...  Training loss: 1.3446...  5.2530 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2263...  Training loss: 1.3322...  5.5080 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2264...  Training loss: 1.3459...  5.5440 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2265...  Training loss: 1.3183...  5.3570 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2266...  Training loss: 1.3244...  5.0040 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2267...  Training loss: 1.3066...  4.8770 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2268...  Training loss: 1.3630...  5.0060 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2269...  Training loss: 1.3263...  4.9510 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2270...  Training loss: 1.3382...  4.8060 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2271...  Training loss: 1.3262...  4.9770 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2272...  Training loss: 1.3278...  4.9580 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2273...  Training loss: 1.3288...  5.0400 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2274...  Training loss: 1.3542...  5.3400 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2275...  Training loss: 1.3599...  5.0790 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2276...  Training loss: 1.3189...  5.6490 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2277...  Training loss: 1.3311...  5.5320 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2278...  Training loss: 1.3151...  5.0450 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2279...  Training loss: 1.3591...  4.9630 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2280...  Training loss: 1.3388...  4.8070 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2281...  Training loss: 1.3504...  4.8170 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2282...  Training loss: 1.3525...  4.7420 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2283...  Training loss: 1.3366...  4.7990 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2284...  Training loss: 1.3453...  4.7100 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12/20...  Training Step: 2285...  Training loss: 1.3445...  5.1170 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2286...  Training loss: 1.3487...  5.1930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2287...  Training loss: 1.3348...  5.5170 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2288...  Training loss: 1.3733...  5.2830 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2289...  Training loss: 1.3266...  5.8980 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2290...  Training loss: 1.3545...  5.0940 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2291...  Training loss: 1.3502...  5.0750 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2292...  Training loss: 1.3409...  5.1990 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2293...  Training loss: 1.3203...  5.3930 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2294...  Training loss: 1.3086...  4.8940 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2295...  Training loss: 1.3574...  4.8270 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2296...  Training loss: 1.3565...  4.7730 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2297...  Training loss: 1.3411...  4.7250 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2298...  Training loss: 1.3456...  4.6410 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2299...  Training loss: 1.3447...  4.7236 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2300...  Training loss: 1.3074...  4.7468 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2301...  Training loss: 1.3044...  4.6358 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2302...  Training loss: 1.3529...  4.9740 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2303...  Training loss: 1.3416...  4.9119 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2304...  Training loss: 1.3086...  4.8369 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2305...  Training loss: 1.3530...  5.1971 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2306...  Training loss: 1.3502...  5.0941 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2307...  Training loss: 1.3241...  5.2411 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2308...  Training loss: 1.3045...  5.3362 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2309...  Training loss: 1.3015...  5.0230 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2310...  Training loss: 1.3189...  4.8549 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2311...  Training loss: 1.3687...  4.6508 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2312...  Training loss: 1.3522...  4.8709 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2313...  Training loss: 1.3517...  4.7258 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2314...  Training loss: 1.3457...  4.6798 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2315...  Training loss: 1.3769...  4.8009 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2316...  Training loss: 1.3590...  4.6958 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2317...  Training loss: 1.3426...  4.9370 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2318...  Training loss: 1.3500...  5.4132 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2319...  Training loss: 1.4061...  5.4633 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2320...  Training loss: 1.3537...  5.1251 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2321...  Training loss: 1.3359...  5.1311 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2322...  Training loss: 1.3837...  4.8149 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2323...  Training loss: 1.3310...  4.9880 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2324...  Training loss: 1.3727...  5.1911 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2325...  Training loss: 1.3455...  4.8599 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2326...  Training loss: 1.3886...  4.8859 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2327...  Training loss: 1.3682...  4.7759 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2328...  Training loss: 1.3387...  4.6728 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2329...  Training loss: 1.3181...  4.6418 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2330...  Training loss: 1.3233...  4.6718 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2331...  Training loss: 1.3565...  4.6708 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2332...  Training loss: 1.3551...  4.7058 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2333...  Training loss: 1.3410...  4.7058 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2334...  Training loss: 1.3444...  4.6938 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2335...  Training loss: 1.3512...  4.8399 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2336...  Training loss: 1.3379...  5.0040 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2337...  Training loss: 1.3093...  5.3502 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2338...  Training loss: 1.3727...  5.1851 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2339...  Training loss: 1.3748...  4.9340 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2340...  Training loss: 1.3472...  4.9750 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2341...  Training loss: 1.3506...  4.7208 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2342...  Training loss: 1.3388...  4.7088 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2343...  Training loss: 1.3426...  4.7819 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2344...  Training loss: 1.3426...  4.6738 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2345...  Training loss: 1.3777...  4.7508 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2346...  Training loss: 1.4035...  4.6698 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2347...  Training loss: 1.3520...  4.6338 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2348...  Training loss: 1.3516...  4.7769 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2349...  Training loss: 1.3278...  5.4763 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2350...  Training loss: 1.3254...  5.5133 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2351...  Training loss: 1.3729...  5.0270 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2352...  Training loss: 1.3530...  5.4423 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2353...  Training loss: 1.3655...  5.0290 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2354...  Training loss: 1.3212...  4.9099 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2355...  Training loss: 1.3399...  4.7228 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2356...  Training loss: 1.3740...  4.7208 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2357...  Training loss: 1.3182...  4.7218 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2358...  Training loss: 1.3212...  4.6678 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2359...  Training loss: 1.3235...  4.6458 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2360...  Training loss: 1.3387...  4.6404 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2361...  Training loss: 1.3336...  4.7738 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2362...  Training loss: 1.3352...  4.6697 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2363...  Training loss: 1.3356...  4.5907 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2364...  Training loss: 1.3306...  4.7238 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2365...  Training loss: 1.3728...  4.7438 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2366...  Training loss: 1.3297...  5.0110 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2367...  Training loss: 1.3304...  5.1791 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2368...  Training loss: 1.3364...  5.2292 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2369...  Training loss: 1.3285...  5.1351 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2370...  Training loss: 1.3212...  5.2452 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2371...  Training loss: 1.3370...  5.5124 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2372...  Training loss: 1.3227...  5.0871 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2373...  Training loss: 1.3084...  5.3653 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2374...  Training loss: 1.3568...  4.8659 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2375...  Training loss: 1.3359...  4.9369 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2376...  Training loss: 1.3291...  4.8799 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2377...  Training loss: 1.4940...  4.7968 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2378...  Training loss: 1.3633...  4.9189 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2379...  Training loss: 1.3553...  4.6557 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2380...  Training loss: 1.3680...  4.8509 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2381...  Training loss: 1.3324...  4.7628 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2382...  Training loss: 1.3273...  4.8879 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 2383...  Training loss: 1.3644...  5.3853 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2384...  Training loss: 1.3377...  4.7998 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2385...  Training loss: 1.3491...  4.7818 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2386...  Training loss: 1.3360...  4.8017 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2387...  Training loss: 1.3298...  4.7298 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2388...  Training loss: 1.3403...  4.8437 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2389...  Training loss: 1.3505...  4.6789 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2390...  Training loss: 1.3602...  4.7498 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2391...  Training loss: 1.3340...  4.8347 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2392...  Training loss: 1.3270...  4.7069 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2393...  Training loss: 1.3570...  4.7868 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2394...  Training loss: 1.3703...  4.7418 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2395...  Training loss: 1.3519...  4.7298 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2396...  Training loss: 1.3772...  4.8737 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2397...  Training loss: 1.3465...  5.0374 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2398...  Training loss: 1.3571...  5.3171 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2399...  Training loss: 1.3351...  4.9506 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2400...  Training loss: 1.3513...  4.7888 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2401...  Training loss: 1.3490...  4.9905 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2402...  Training loss: 1.3042...  4.8677 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2403...  Training loss: 1.3129...  4.9675 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2404...  Training loss: 1.3687...  4.7558 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2405...  Training loss: 1.3560...  4.7698 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2406...  Training loss: 1.3469...  4.5870 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2407...  Training loss: 1.3260...  4.6410 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2408...  Training loss: 1.3022...  4.6210 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2409...  Training loss: 1.3447...  4.6969 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2410...  Training loss: 1.3506...  4.9286 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2411...  Training loss: 1.3361...  4.9316 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2412...  Training loss: 1.3489...  4.9935 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2413...  Training loss: 1.3090...  5.1113 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2414...  Training loss: 1.2959...  5.0344 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2415...  Training loss: 1.2966...  4.8637 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2416...  Training loss: 1.3222...  4.9346 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2417...  Training loss: 1.3059...  4.6639 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2418...  Training loss: 1.3605...  4.6539 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2419...  Training loss: 1.3178...  4.6629 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2420...  Training loss: 1.3181...  4.7089 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2421...  Training loss: 1.3411...  4.7608 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2422...  Training loss: 1.3136...  5.2661 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2423...  Training loss: 1.3160...  5.3650 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2424...  Training loss: 1.3230...  5.0035 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2425...  Training loss: 1.3316...  4.9905 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2426...  Training loss: 1.3526...  4.7888 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2427...  Training loss: 1.3146...  4.9486 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2428...  Training loss: 1.3735...  5.0734 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2429...  Training loss: 1.3390...  5.3311 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2430...  Training loss: 1.3480...  5.0424 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2431...  Training loss: 1.3347...  4.9286 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2432...  Training loss: 1.3346...  4.8866 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2433...  Training loss: 1.3508...  4.7688 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2434...  Training loss: 1.3206...  4.7358 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2435...  Training loss: 1.3124...  4.7918 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2436...  Training loss: 1.3728...  4.6709 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2437...  Training loss: 1.3424...  4.7768 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2438...  Training loss: 1.3810...  4.6470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2439...  Training loss: 1.3649...  4.6140 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2440...  Training loss: 1.3459...  4.6110 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2441...  Training loss: 1.3328...  5.6247 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2442...  Training loss: 1.3534...  5.7915 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2443...  Training loss: 1.3525...  5.7815 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2444...  Training loss: 1.3128...  5.9799 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2445...  Training loss: 1.3388...  5.9520 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2446...  Training loss: 1.3283...  5.7680 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2447...  Training loss: 1.3748...  5.2450 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2448...  Training loss: 1.3604...  5.1080 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2449...  Training loss: 1.3606...  4.8870 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2450...  Training loss: 1.3215...  4.7930 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2451...  Training loss: 1.3289...  4.6610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2452...  Training loss: 1.3581...  4.5940 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2453...  Training loss: 1.3238...  4.9020 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2454...  Training loss: 1.3207...  4.7270 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2455...  Training loss: 1.3005...  4.6320 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2456...  Training loss: 1.3445...  4.6290 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2457...  Training loss: 1.2895...  4.6580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2458...  Training loss: 1.3255...  4.7090 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2459...  Training loss: 1.2992...  4.9910 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2460...  Training loss: 1.3241...  4.7170 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2461...  Training loss: 1.3135...  5.0800 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2462...  Training loss: 1.3286...  4.7800 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2463...  Training loss: 1.3068...  4.8020 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2464...  Training loss: 1.3087...  4.8060 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2465...  Training loss: 1.3019...  4.7500 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2466...  Training loss: 1.3322...  4.8570 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2467...  Training loss: 1.3058...  4.6330 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2468...  Training loss: 1.3202...  4.6880 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2469...  Training loss: 1.3069...  4.6550 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2470...  Training loss: 1.3071...  4.9000 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2471...  Training loss: 1.3065...  5.2290 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2472...  Training loss: 1.3421...  5.0230 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2473...  Training loss: 1.3380...  4.8580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2474...  Training loss: 1.3008...  4.9590 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2475...  Training loss: 1.3102...  5.1280 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2476...  Training loss: 1.3053...  4.8540 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2477...  Training loss: 1.3353...  5.2880 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2478...  Training loss: 1.3127...  5.0840 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2479...  Training loss: 1.3280...  4.9460 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2480...  Training loss: 1.3165...  4.7020 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13/20...  Training Step: 2481...  Training loss: 1.3157...  4.7920 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2482...  Training loss: 1.3180...  4.6650 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2483...  Training loss: 1.3309...  4.8150 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2484...  Training loss: 1.3319...  4.7260 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2485...  Training loss: 1.3155...  4.6820 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2486...  Training loss: 1.3435...  4.7780 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2487...  Training loss: 1.3086...  4.7330 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2488...  Training loss: 1.3282...  4.7170 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2489...  Training loss: 1.3264...  4.7580 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2490...  Training loss: 1.3209...  5.1810 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2491...  Training loss: 1.3024...  5.8460 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2492...  Training loss: 1.2839...  5.2110 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2493...  Training loss: 1.3255...  5.1650 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2494...  Training loss: 1.3385...  4.9860 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2495...  Training loss: 1.3140...  5.4940 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2496...  Training loss: 1.3194...  5.0460 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2497...  Training loss: 1.3175...  5.0250 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2498...  Training loss: 1.2946...  4.8620 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2499...  Training loss: 1.2919...  4.9020 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2500...  Training loss: 1.3300...  4.6660 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2501...  Training loss: 1.3227...  5.2940 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2502...  Training loss: 1.2754...  5.1800 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2503...  Training loss: 1.3410...  5.1610 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2504...  Training loss: 1.3355...  5.3380 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2505...  Training loss: 1.2996...  5.5556 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2506...  Training loss: 1.2877...  5.3061 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2507...  Training loss: 1.2851...  5.2391 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2508...  Training loss: 1.3003...  5.3181 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2509...  Training loss: 1.3399...  4.9950 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2510...  Training loss: 1.3344...  4.8890 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2511...  Training loss: 1.3362...  4.7939 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2512...  Training loss: 1.3250...  4.6929 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2513...  Training loss: 1.3615...  4.6709 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2514...  Training loss: 1.3380...  4.6959 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2515...  Training loss: 1.3228...  4.7159 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2516...  Training loss: 1.3290...  4.8299 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2517...  Training loss: 1.3826...  4.7449 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2518...  Training loss: 1.3331...  4.7559 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2519...  Training loss: 1.3140...  5.2781 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2520...  Training loss: 1.3567...  5.3631 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2521...  Training loss: 1.3098...  5.1250 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2522...  Training loss: 1.3549...  4.9270 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2523...  Training loss: 1.3345...  5.1701 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2524...  Training loss: 1.3620...  4.9840 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2525...  Training loss: 1.3462...  4.7399 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2526...  Training loss: 1.3159...  4.6679 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2527...  Training loss: 1.2988...  4.9190 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2528...  Training loss: 1.3009...  4.9770 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2529...  Training loss: 1.3264...  4.7869 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2530...  Training loss: 1.3191...  4.7139 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2531...  Training loss: 1.3132...  4.8199 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2532...  Training loss: 1.3184...  4.6939 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2533...  Training loss: 1.3225...  4.7369 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2534...  Training loss: 1.3148...  4.7019 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2535...  Training loss: 1.2846...  4.6589 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2536...  Training loss: 1.3492...  4.8850 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2537...  Training loss: 1.3428...  4.7169 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2538...  Training loss: 1.3289...  4.6619 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2539...  Training loss: 1.3104...  4.6218 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2540...  Training loss: 1.3287...  4.6389 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2541...  Training loss: 1.3247...  4.7859 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2542...  Training loss: 1.3259...  4.6579 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2543...  Training loss: 1.3459...  4.8369 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2544...  Training loss: 1.3906...  5.0850 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2545...  Training loss: 1.3296...  5.2761 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2546...  Training loss: 1.3248...  4.9670 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2547...  Training loss: 1.3159...  4.8119 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2548...  Training loss: 1.3019...  4.6779 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2549...  Training loss: 1.3582...  4.7279 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2550...  Training loss: 1.3248...  5.6463 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2551...  Training loss: 1.3401...  5.2141 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2552...  Training loss: 1.3104...  5.5302 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2553...  Training loss: 1.3131...  5.7443 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2554...  Training loss: 1.3540...  5.4412 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2555...  Training loss: 1.3033...  5.1020 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2556...  Training loss: 1.2856...  4.9470 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2557...  Training loss: 1.3012...  4.7489 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2558...  Training loss: 1.3168...  4.6789 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2559...  Training loss: 1.3159...  4.7849 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2560...  Training loss: 1.3140...  4.6899 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2561...  Training loss: 1.3112...  4.6569 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2562...  Training loss: 1.3003...  4.7639 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2563...  Training loss: 1.3546...  4.7699 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2564...  Training loss: 1.3104...  4.6509 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2565...  Training loss: 1.3201...  4.7119 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2566...  Training loss: 1.3170...  4.7455 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2567...  Training loss: 1.2956...  4.8569 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2568...  Training loss: 1.3053...  5.1751 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2569...  Training loss: 1.3186...  4.9420 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2570...  Training loss: 1.3124...  5.0700 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2571...  Training loss: 1.2913...  4.9430 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2572...  Training loss: 1.3243...  5.6314 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2573...  Training loss: 1.3238...  5.0130 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2574...  Training loss: 1.3016...  4.7939 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2575...  Training loss: 1.4625...  4.6398 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2576...  Training loss: 1.3273...  4.9280 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2577...  Training loss: 1.3253...  5.1231 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2578...  Training loss: 1.3483...  5.1221 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 2579...  Training loss: 1.3032...  5.9135 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2580...  Training loss: 1.2992...  5.0270 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2581...  Training loss: 1.3454...  4.6508 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2582...  Training loss: 1.3160...  4.7819 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2583...  Training loss: 1.3357...  4.6988 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2584...  Training loss: 1.3114...  4.7929 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2585...  Training loss: 1.3059...  4.6608 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2586...  Training loss: 1.3053...  4.7178 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2587...  Training loss: 1.3192...  4.7579 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2588...  Training loss: 1.3380...  4.6918 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2589...  Training loss: 1.3156...  4.6638 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2590...  Training loss: 1.2963...  4.6438 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2591...  Training loss: 1.3368...  4.5878 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2592...  Training loss: 1.3343...  5.2652 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2593...  Training loss: 1.3277...  5.4516 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2594...  Training loss: 1.3497...  5.2063 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2595...  Training loss: 1.3213...  4.7008 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2596...  Training loss: 1.3411...  4.6719 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2597...  Training loss: 1.3147...  4.8207 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2598...  Training loss: 1.3329...  4.6599 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2599...  Training loss: 1.3181...  4.7148 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2600...  Training loss: 1.2818...  4.6878 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2601...  Training loss: 1.2868...  5.2552 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2602...  Training loss: 1.3383...  4.9555 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2603...  Training loss: 1.3388...  4.8656 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2604...  Training loss: 1.3296...  4.8497 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2605...  Training loss: 1.3123...  4.8177 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2606...  Training loss: 1.2872...  4.6878 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2607...  Training loss: 1.3192...  4.7558 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2608...  Training loss: 1.3251...  4.6798 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2609...  Training loss: 1.2988...  4.7078 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2610...  Training loss: 1.3239...  4.8736 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2611...  Training loss: 1.3033...  5.3971 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2612...  Training loss: 1.2708...  5.5769 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2613...  Training loss: 1.2681...  5.3861 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2614...  Training loss: 1.3037...  5.2572 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2615...  Training loss: 1.2813...  5.1503 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2616...  Training loss: 1.3537...  5.6408 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2617...  Training loss: 1.3006...  5.0365 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2618...  Training loss: 1.2932...  4.9186 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2619...  Training loss: 1.3242...  4.7618 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2620...  Training loss: 1.2961...  4.7787 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2621...  Training loss: 1.3081...  4.7618 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2622...  Training loss: 1.3126...  4.7508 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2623...  Training loss: 1.3034...  4.6659 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2624...  Training loss: 1.3348...  4.6669 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2625...  Training loss: 1.2834...  4.7058 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2626...  Training loss: 1.3490...  4.6649 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2627...  Training loss: 1.3217...  4.7028 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2628...  Training loss: 1.3302...  5.1234 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2629...  Training loss: 1.3101...  5.3301 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2630...  Training loss: 1.3199...  4.7278 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2631...  Training loss: 1.3277...  4.8057 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2632...  Training loss: 1.2997...  4.7807 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2633...  Training loss: 1.2944...  4.8946 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2634...  Training loss: 1.3531...  4.6709 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2635...  Training loss: 1.3125...  4.6828 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2636...  Training loss: 1.3626...  4.6169 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2637...  Training loss: 1.3377...  4.6009 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2638...  Training loss: 1.3234...  4.8886 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2639...  Training loss: 1.3176...  4.7048 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2640...  Training loss: 1.3162...  4.6599 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2641...  Training loss: 1.3378...  5.1284 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2642...  Training loss: 1.3030...  5.4320 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2643...  Training loss: 1.3210...  5.2023 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2644...  Training loss: 1.2984...  4.8656 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2645...  Training loss: 1.3599...  4.9076 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2646...  Training loss: 1.3314...  4.7008 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2647...  Training loss: 1.3473...  5.1024 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2648...  Training loss: 1.2984...  5.0754 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2649...  Training loss: 1.3260...  4.7697 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2650...  Training loss: 1.3272...  4.6339 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2651...  Training loss: 1.3036...  4.6139 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2652...  Training loss: 1.3032...  4.7538 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2653...  Training loss: 1.2732...  4.5730 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2654...  Training loss: 1.3222...  4.7346 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2655...  Training loss: 1.2736...  4.7395 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2656...  Training loss: 1.3156...  4.6945 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2657...  Training loss: 1.2837...  4.6315 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2658...  Training loss: 1.2982...  4.6085 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2659...  Training loss: 1.2896...  4.7075 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2660...  Training loss: 1.3055...  4.6515 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2661...  Training loss: 1.2855...  4.8295 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2662...  Training loss: 1.2908...  4.9795 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2663...  Training loss: 1.2740...  4.6095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2664...  Training loss: 1.3215...  4.6795 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2665...  Training loss: 1.2827...  4.6345 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2666...  Training loss: 1.3044...  5.3135 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2667...  Training loss: 1.2756...  4.9495 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2668...  Training loss: 1.2890...  5.2095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2669...  Training loss: 1.2908...  4.8645 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2670...  Training loss: 1.3228...  4.7055 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2671...  Training loss: 1.3133...  4.6615 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2672...  Training loss: 1.2749...  4.6675 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2673...  Training loss: 1.2905...  4.6015 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2674...  Training loss: 1.2905...  4.6445 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2675...  Training loss: 1.3142...  4.7345 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2676...  Training loss: 1.3003...  4.6635 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/20...  Training Step: 2677...  Training loss: 1.3194...  4.7825 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2678...  Training loss: 1.3063...  4.6545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2679...  Training loss: 1.3005...  4.8155 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2680...  Training loss: 1.2994...  4.6905 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2681...  Training loss: 1.3109...  4.7535 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2682...  Training loss: 1.3092...  4.7485 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2683...  Training loss: 1.2926...  4.6285 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2684...  Training loss: 1.3264...  4.6615 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2685...  Training loss: 1.2931...  5.2655 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2686...  Training loss: 1.3171...  5.3195 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2687...  Training loss: 1.3129...  5.5104 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2688...  Training loss: 1.3057...  5.3575 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2689...  Training loss: 1.2833...  5.3055 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2690...  Training loss: 1.2694...  5.8164 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2691...  Training loss: 1.3178...  5.6084 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2692...  Training loss: 1.3125...  5.0565 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2693...  Training loss: 1.2997...  5.1685 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2694...  Training loss: 1.3115...  4.8755 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2695...  Training loss: 1.2992...  4.6935 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2696...  Training loss: 1.2729...  4.7465 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2697...  Training loss: 1.2668...  4.6665 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2698...  Training loss: 1.3098...  4.6185 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2699...  Training loss: 1.2928...  4.6255 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2700...  Training loss: 1.2597...  4.6025 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2701...  Training loss: 1.3130...  4.5945 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2702...  Training loss: 1.3111...  4.6655 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2703...  Training loss: 1.2869...  4.7535 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2704...  Training loss: 1.2681...  4.7075 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2705...  Training loss: 1.2610...  4.7735 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2706...  Training loss: 1.2790...  4.7185 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2707...  Training loss: 1.3372...  4.7455 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2708...  Training loss: 1.3213...  4.6855 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2709...  Training loss: 1.3137...  4.7125 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2710...  Training loss: 1.3063...  4.9665 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2711...  Training loss: 1.3414...  4.9095 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2712...  Training loss: 1.3351...  4.6825 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2713...  Training loss: 1.3162...  4.6965 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2714...  Training loss: 1.3176...  4.6545 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2715...  Training loss: 1.3610...  4.7715 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2716...  Training loss: 1.3236...  5.2786 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2717...  Training loss: 1.2890...  5.3686 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2718...  Training loss: 1.3447...  5.0785 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2719...  Training loss: 1.2958...  4.9385 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2720...  Training loss: 1.3349...  4.6544 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2721...  Training loss: 1.3034...  4.7034 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2722...  Training loss: 1.3438...  4.6674 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2723...  Training loss: 1.3379...  4.6234 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2724...  Training loss: 1.2913...  4.5944 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2725...  Training loss: 1.2776...  4.6274 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2726...  Training loss: 1.2881...  4.5804 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2727...  Training loss: 1.3113...  4.6294 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2728...  Training loss: 1.3017...  4.7524 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2729...  Training loss: 1.3054...  4.6844 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2730...  Training loss: 1.3082...  4.6214 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2731...  Training loss: 1.3057...  4.6404 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2732...  Training loss: 1.2984...  4.6084 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2733...  Training loss: 1.2661...  4.7254 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2734...  Training loss: 1.3194...  4.8715 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2735...  Training loss: 1.3330...  5.2016 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2736...  Training loss: 1.3032...  5.5347 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2737...  Training loss: 1.2993...  5.3756 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2738...  Training loss: 1.2983...  5.4236 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2739...  Training loss: 1.2990...  6.2719 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2740...  Training loss: 1.3089...  5.8017 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2741...  Training loss: 1.3358...  5.1375 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2742...  Training loss: 1.3777...  5.0795 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2743...  Training loss: 1.3123...  4.7784 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2744...  Training loss: 1.3102...  5.0025 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2745...  Training loss: 1.3031...  4.7264 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2746...  Training loss: 1.2885...  4.6834 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2747...  Training loss: 1.3415...  4.7284 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2748...  Training loss: 1.3046...  4.6604 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2749...  Training loss: 1.3192...  4.6254 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2750...  Training loss: 1.2826...  4.6424 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2751...  Training loss: 1.2964...  5.1716 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2752...  Training loss: 1.3326...  5.1505 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2753...  Training loss: 1.2873...  4.8665 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2754...  Training loss: 1.2850...  4.8885 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2755...  Training loss: 1.2844...  4.7104 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2756...  Training loss: 1.2975...  4.6644 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2757...  Training loss: 1.3001...  4.7634 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2758...  Training loss: 1.2946...  4.6014 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2759...  Training loss: 1.3008...  4.6004 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2760...  Training loss: 1.2872...  4.6674 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2761...  Training loss: 1.3263...  4.6274 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2762...  Training loss: 1.2976...  4.6274 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2763...  Training loss: 1.2893...  4.6194 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2764...  Training loss: 1.2969...  4.6334 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2765...  Training loss: 1.2797...  5.5697 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2766...  Training loss: 1.2829...  5.1195 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2767...  Training loss: 1.2957...  5.2726 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2768...  Training loss: 1.2914...  4.7704 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2769...  Training loss: 1.2671...  4.6294 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2770...  Training loss: 1.3109...  4.8615 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2771...  Training loss: 1.2917...  4.6794 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2772...  Training loss: 1.2904...  4.6554 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2773...  Training loss: 1.4307...  4.6044 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2774...  Training loss: 1.3128...  4.6064 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 2775...  Training loss: 1.3083...  4.5844 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2776...  Training loss: 1.3242...  4.6114 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2777...  Training loss: 1.2702...  4.6634 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2778...  Training loss: 1.2728...  4.6762 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2779...  Training loss: 1.3172...  4.5883 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2780...  Training loss: 1.2922...  4.6993 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2781...  Training loss: 1.3199...  4.6373 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2782...  Training loss: 1.2906...  4.6353 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2783...  Training loss: 1.2773...  4.8174 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2784...  Training loss: 1.2948...  4.6523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2785...  Training loss: 1.2978...  4.6183 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2786...  Training loss: 1.3154...  4.6623 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2787...  Training loss: 1.2880...  4.6143 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2788...  Training loss: 1.2729...  4.5693 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2789...  Training loss: 1.3191...  4.6513 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2790...  Training loss: 1.3210...  5.0475 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2791...  Training loss: 1.3062...  5.3287 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2792...  Training loss: 1.3250...  4.9425 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2793...  Training loss: 1.3029...  4.8144 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2794...  Training loss: 1.3194...  4.7754 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2795...  Training loss: 1.2912...  5.0035 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2796...  Training loss: 1.3279...  5.2876 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2797...  Training loss: 1.3013...  5.3907 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2798...  Training loss: 1.2571...  5.3026 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2799...  Training loss: 1.2759...  5.4707 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2800...  Training loss: 1.3217...  5.3697 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2801...  Training loss: 1.3181...  5.3558 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2802...  Training loss: 1.3207...  5.0989 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2803...  Training loss: 1.2890...  4.6263 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2804...  Training loss: 1.2807...  4.6393 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2805...  Training loss: 1.3040...  4.6862 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2806...  Training loss: 1.3012...  4.6013 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2807...  Training loss: 1.2810...  4.6523 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2808...  Training loss: 1.3008...  4.6133 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2809...  Training loss: 1.2799...  4.7602 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2810...  Training loss: 1.2592...  4.5953 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2811...  Training loss: 1.2595...  4.6423 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2812...  Training loss: 1.2942...  4.5544 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2813...  Training loss: 1.2716...  4.9730 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2814...  Training loss: 1.3357...  5.5695 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2815...  Training loss: 1.2840...  4.9750 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2816...  Training loss: 1.2587...  4.8431 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2817...  Training loss: 1.3161...  4.6583 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2818...  Training loss: 1.2820...  4.6573 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2819...  Training loss: 1.2760...  4.6073 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2820...  Training loss: 1.2923...  4.6243 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2821...  Training loss: 1.2934...  4.7012 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2822...  Training loss: 1.3116...  4.7482 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2823...  Training loss: 1.2733...  4.8331 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2824...  Training loss: 1.3452...  5.0330 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2825...  Training loss: 1.3055...  5.0420 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2826...  Training loss: 1.3134...  5.2348 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2827...  Training loss: 1.2866...  5.4806 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2828...  Training loss: 1.3001...  4.9910 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2829...  Training loss: 1.3135...  4.8891 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2830...  Training loss: 1.2800...  4.7392 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2831...  Training loss: 1.2737...  4.6013 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2832...  Training loss: 1.3396...  4.5963 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2833...  Training loss: 1.3029...  4.6313 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2834...  Training loss: 1.3397...  4.7412 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2835...  Training loss: 1.3248...  4.7542 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2836...  Training loss: 1.3046...  4.8681 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2837...  Training loss: 1.3009...  4.9031 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2838...  Training loss: 1.3074...  5.5406 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2839...  Training loss: 1.3151...  5.7744 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2840...  Training loss: 1.2820...  5.2638 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2841...  Training loss: 1.2986...  5.3008 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2842...  Training loss: 1.2896...  5.1838 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2843...  Training loss: 1.3409...  4.8371 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2844...  Training loss: 1.3138...  4.8801 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2845...  Training loss: 1.3181...  4.8831 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2846...  Training loss: 1.2839...  4.9241 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2847...  Training loss: 1.3044...  4.6852 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2848...  Training loss: 1.3234...  4.6922 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2849...  Training loss: 1.2969...  4.6203 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2850...  Training loss: 1.2889...  4.6133 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2851...  Training loss: 1.2505...  4.7342 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2852...  Training loss: 1.3034...  4.6423 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2853...  Training loss: 1.2560...  4.5943 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2854...  Training loss: 1.3010...  4.6553 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2855...  Training loss: 1.2659...  4.6093 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2856...  Training loss: 1.2875...  4.6633 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2857...  Training loss: 1.2708...  4.7022 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2858...  Training loss: 1.2900...  4.7572 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2859...  Training loss: 1.2689...  4.6922 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2860...  Training loss: 1.2733...  4.7892 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2861...  Training loss: 1.2570...  4.7902 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2862...  Training loss: 1.3024...  4.6353 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2863...  Training loss: 1.2761...  4.7085 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2864...  Training loss: 1.2865...  5.2610 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2865...  Training loss: 1.2654...  5.3780 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2866...  Training loss: 1.2634...  5.1770 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2867...  Training loss: 1.2791...  4.8440 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2868...  Training loss: 1.2897...  4.7500 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2869...  Training loss: 1.2906...  4.8580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2870...  Training loss: 1.2695...  5.1520 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2871...  Training loss: 1.2728...  5.2180 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2872...  Training loss: 1.2635...  5.5220 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15/20...  Training Step: 2873...  Training loss: 1.2865...  5.1050 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2874...  Training loss: 1.2978...  5.3310 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2875...  Training loss: 1.2977...  5.1160 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2876...  Training loss: 1.2807...  4.9830 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2877...  Training loss: 1.2840...  4.6420 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2878...  Training loss: 1.2947...  4.6390 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2879...  Training loss: 1.2946...  4.6350 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2880...  Training loss: 1.3013...  4.6380 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2881...  Training loss: 1.2783...  4.6800 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2882...  Training loss: 1.3038...  4.6170 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2883...  Training loss: 1.2783...  4.6650 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2884...  Training loss: 1.3003...  4.7000 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2885...  Training loss: 1.2975...  4.9120 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2886...  Training loss: 1.2886...  5.2240 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2887...  Training loss: 1.2688...  4.8570 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2888...  Training loss: 1.2615...  5.2230 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2889...  Training loss: 1.2976...  5.2600 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2890...  Training loss: 1.3039...  5.1240 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2891...  Training loss: 1.2823...  4.9380 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2892...  Training loss: 1.2981...  4.7760 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2893...  Training loss: 1.2924...  4.8330 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2894...  Training loss: 1.2569...  4.6720 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2895...  Training loss: 1.2551...  4.6690 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2896...  Training loss: 1.2925...  4.6370 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2897...  Training loss: 1.2845...  4.7870 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2898...  Training loss: 1.2495...  4.6340 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2899...  Training loss: 1.2975...  4.6220 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2900...  Training loss: 1.2877...  4.6080 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2901...  Training loss: 1.2685...  4.7210 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2902...  Training loss: 1.2567...  4.6140 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2903...  Training loss: 1.2404...  4.6630 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2904...  Training loss: 1.2779...  4.6440 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2905...  Training loss: 1.3180...  4.6250 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2906...  Training loss: 1.3036...  4.6170 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2907...  Training loss: 1.3000...  4.8230 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2908...  Training loss: 1.2914...  4.6630 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2909...  Training loss: 1.3203...  4.6370 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2910...  Training loss: 1.3085...  4.6930 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2911...  Training loss: 1.3001...  4.6980 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2912...  Training loss: 1.3062...  4.6260 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2913...  Training loss: 1.3423...  4.7650 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2914...  Training loss: 1.3120...  5.2430 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2915...  Training loss: 1.2843...  5.3220 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2916...  Training loss: 1.3234...  4.9180 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2917...  Training loss: 1.2644...  4.7510 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2918...  Training loss: 1.3132...  4.8160 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2919...  Training loss: 1.2984...  5.2490 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2920...  Training loss: 1.3224...  5.5100 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2921...  Training loss: 1.3088...  5.3980 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2922...  Training loss: 1.2777...  5.5640 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2923...  Training loss: 1.2553...  5.5870 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2924...  Training loss: 1.2652...  5.4731 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2925...  Training loss: 1.3014...  5.1030 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2926...  Training loss: 1.2932...  4.6059 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2927...  Training loss: 1.2817...  4.6429 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2928...  Training loss: 1.2866...  4.7990 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2929...  Training loss: 1.2912...  4.5999 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2930...  Training loss: 1.2785...  4.6419 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2931...  Training loss: 1.2650...  4.6109 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2932...  Training loss: 1.3142...  4.7620 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2933...  Training loss: 1.3150...  4.6489 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2934...  Training loss: 1.2903...  4.6399 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2935...  Training loss: 1.2918...  4.7059 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2936...  Training loss: 1.2955...  4.6789 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2937...  Training loss: 1.2967...  4.6519 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2938...  Training loss: 1.2886...  5.2871 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2939...  Training loss: 1.3173...  4.9460 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2940...  Training loss: 1.3574...  5.0190 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2941...  Training loss: 1.2982...  4.8000 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2942...  Training loss: 1.2947...  4.7389 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2943...  Training loss: 1.2817...  4.7209 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2944...  Training loss: 1.2758...  4.6509 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2945...  Training loss: 1.3152...  4.7079 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2946...  Training loss: 1.2959...  4.6159 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2947...  Training loss: 1.3003...  4.6299 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2948...  Training loss: 1.2699...  4.7409 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2949...  Training loss: 1.2868...  4.6369 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2950...  Training loss: 1.3144...  4.7079 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2951...  Training loss: 1.2746...  4.6349 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2952...  Training loss: 1.2681...  4.6089 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2953...  Training loss: 1.2780...  4.6349 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2954...  Training loss: 1.2865...  4.6249 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2955...  Training loss: 1.2941...  4.5959 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2956...  Training loss: 1.2725...  4.6299 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2957...  Training loss: 1.2796...  4.5889 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2958...  Training loss: 1.2768...  4.6209 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2959...  Training loss: 1.3201...  4.7209 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2960...  Training loss: 1.2778...  4.6239 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2961...  Training loss: 1.2780...  4.7359 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2962...  Training loss: 1.2807...  4.7479 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2963...  Training loss: 1.2655...  5.0260 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2964...  Training loss: 1.2657...  5.0580 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2965...  Training loss: 1.2949...  5.2240 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2966...  Training loss: 1.2839...  4.9230 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2967...  Training loss: 1.2510...  4.7770 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2968...  Training loss: 1.2928...  4.7139 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2969...  Training loss: 1.2766...  4.6609 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2970...  Training loss: 1.2786...  4.7009 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 2971...  Training loss: 1.4150...  4.6479 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2972...  Training loss: 1.3124...  4.6749 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2973...  Training loss: 1.2876...  4.5769 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2974...  Training loss: 1.3088...  4.6679 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2975...  Training loss: 1.2687...  4.7269 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2976...  Training loss: 1.2502...  4.6629 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2977...  Training loss: 1.2952...  4.6109 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2978...  Training loss: 1.2858...  4.6159 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2979...  Training loss: 1.2925...  4.6159 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2980...  Training loss: 1.2764...  4.6089 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2981...  Training loss: 1.2709...  4.6579 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2982...  Training loss: 1.2772...  5.0950 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2983...  Training loss: 1.2950...  5.2050 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2984...  Training loss: 1.2975...  5.5291 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2985...  Training loss: 1.2694...  5.5951 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2986...  Training loss: 1.2642...  5.4851 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2987...  Training loss: 1.2992...  5.1262 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2988...  Training loss: 1.3074...  5.4126 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2989...  Training loss: 1.2921...  4.9415 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2990...  Training loss: 1.3119...  5.1275 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2991...  Training loss: 1.2961...  4.9385 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2992...  Training loss: 1.3042...  4.6724 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2993...  Training loss: 1.2882...  4.8004 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2994...  Training loss: 1.2979...  4.6534 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2995...  Training loss: 1.2928...  4.5844 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2996...  Training loss: 1.2448...  4.6224 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2997...  Training loss: 1.2586...  4.5894 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2998...  Training loss: 1.3125...  4.6494 sec/batch\n",
      "Epoch: 16/20...  Training Step: 2999...  Training loss: 1.3058...  5.4286 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3000...  Training loss: 1.3033...  5.2676 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3001...  Training loss: 1.2753...  4.9975 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3002...  Training loss: 1.2674...  4.7804 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3003...  Training loss: 1.2957...  4.8084 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3004...  Training loss: 1.3062...  4.8455 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3005...  Training loss: 1.2707...  4.7334 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3006...  Training loss: 1.2883...  4.6404 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3007...  Training loss: 1.2701...  4.8354 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3008...  Training loss: 1.2478...  4.6924 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3009...  Training loss: 1.2418...  4.6594 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3010...  Training loss: 1.2716...  4.6184 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3011...  Training loss: 1.2538...  4.6914 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3012...  Training loss: 1.3257...  4.8124 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3013...  Training loss: 1.2771...  4.8274 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3014...  Training loss: 1.2590...  4.6369 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3015...  Training loss: 1.2890...  4.5962 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3016...  Training loss: 1.2652...  4.6182 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3017...  Training loss: 1.2759...  4.6402 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3018...  Training loss: 1.2785...  4.7102 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3019...  Training loss: 1.2721...  4.6352 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3020...  Training loss: 1.3003...  4.6062 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3021...  Training loss: 1.2649...  4.6162 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3022...  Training loss: 1.3173...  4.6542 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3023...  Training loss: 1.2828...  4.6962 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3024...  Training loss: 1.2895...  4.6792 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3025...  Training loss: 1.2789...  4.7212 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3026...  Training loss: 1.2784...  4.7741 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3027...  Training loss: 1.2917...  4.5892 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3028...  Training loss: 1.2701...  4.6542 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3029...  Training loss: 1.2574...  4.6092 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3030...  Training loss: 1.3145...  4.6652 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3031...  Training loss: 1.3018...  4.6332 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3032...  Training loss: 1.3331...  4.6982 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3033...  Training loss: 1.3004...  4.7072 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3034...  Training loss: 1.2889...  4.6462 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3035...  Training loss: 1.2842...  4.6232 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3036...  Training loss: 1.2964...  4.6012 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3037...  Training loss: 1.3027...  4.6262 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3038...  Training loss: 1.2696...  4.7661 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3039...  Training loss: 1.2893...  5.1739 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3040...  Training loss: 1.2690...  4.9071 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3041...  Training loss: 1.3279...  4.7581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3042...  Training loss: 1.3005...  4.7761 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3043...  Training loss: 1.3049...  4.7471 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3044...  Training loss: 1.2708...  4.7581 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3045...  Training loss: 1.2901...  4.7182 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3046...  Training loss: 1.3051...  4.6662 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3047...  Training loss: 1.2825...  4.7332 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3048...  Training loss: 1.2624...  4.7651 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3049...  Training loss: 1.2408...  4.6372 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3050...  Training loss: 1.2808...  4.7312 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3051...  Training loss: 1.2584...  4.9210 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3052...  Training loss: 1.2786...  4.8661 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3053...  Training loss: 1.2468...  4.6782 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3054...  Training loss: 1.2744...  4.6332 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3055...  Training loss: 1.2587...  4.6182 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3056...  Training loss: 1.2691...  4.6022 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3057...  Training loss: 1.2540...  5.0560 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3058...  Training loss: 1.2567...  5.1949 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3059...  Training loss: 1.2392...  5.3438 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3060...  Training loss: 1.2783...  5.5377 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3061...  Training loss: 1.2586...  5.2009 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3062...  Training loss: 1.2662...  5.3178 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3063...  Training loss: 1.2523...  5.1769 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3064...  Training loss: 1.2560...  5.5087 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3065...  Training loss: 1.2639...  4.9280 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3066...  Training loss: 1.2884...  4.9300 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3067...  Training loss: 1.2811...  4.7901 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3068...  Training loss: 1.2438...  4.6802 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 3069...  Training loss: 1.2584...  4.6722 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3070...  Training loss: 1.2458...  4.6132 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3071...  Training loss: 1.2750...  4.6072 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3072...  Training loss: 1.2670...  4.5972 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3073...  Training loss: 1.2763...  4.5952 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3074...  Training loss: 1.2712...  4.6262 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3075...  Training loss: 1.2725...  4.7012 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3076...  Training loss: 1.2699...  4.6432 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3077...  Training loss: 1.2833...  4.8152 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3078...  Training loss: 1.2874...  4.8520 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3079...  Training loss: 1.2684...  5.0730 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3080...  Training loss: 1.2895...  4.9740 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3081...  Training loss: 1.2618...  4.9150 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3082...  Training loss: 1.2815...  4.6410 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3083...  Training loss: 1.2860...  4.6590 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3084...  Training loss: 1.2584...  4.7180 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3085...  Training loss: 1.2587...  5.0170 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3086...  Training loss: 1.2328...  4.8330 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3087...  Training loss: 1.2831...  4.7310 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3088...  Training loss: 1.2817...  4.9140 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3089...  Training loss: 1.2685...  5.3970 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3090...  Training loss: 1.2610...  5.4330 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3091...  Training loss: 1.2760...  4.8350 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3092...  Training loss: 1.2449...  4.7820 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3093...  Training loss: 1.2307...  4.8220 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3094...  Training loss: 1.2741...  4.6380 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3095...  Training loss: 1.2544...  4.6490 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3096...  Training loss: 1.2415...  4.6440 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3097...  Training loss: 1.2846...  4.6070 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3098...  Training loss: 1.2756...  4.5890 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3099...  Training loss: 1.2553...  4.6130 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3100...  Training loss: 1.2326...  4.7200 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3101...  Training loss: 1.2341...  4.6060 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3102...  Training loss: 1.2526...  4.6700 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3103...  Training loss: 1.2899...  4.7930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3104...  Training loss: 1.2944...  4.5900 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3105...  Training loss: 1.2783...  4.6370 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3106...  Training loss: 1.2730...  4.6930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3107...  Training loss: 1.3059...  4.6260 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3108...  Training loss: 1.2922...  4.7910 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3109...  Training loss: 1.2797...  4.6450 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3110...  Training loss: 1.2860...  4.7720 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3111...  Training loss: 1.3236...  4.6380 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3112...  Training loss: 1.2909...  4.6620 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3113...  Training loss: 1.2653...  4.7140 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3114...  Training loss: 1.3091...  5.1040 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3115...  Training loss: 1.2599...  4.9580 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3116...  Training loss: 1.3004...  5.0900 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3117...  Training loss: 1.2918...  4.8470 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3118...  Training loss: 1.3173...  4.7930 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3119...  Training loss: 1.2969...  5.2710 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3120...  Training loss: 1.2661...  5.4400 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3121...  Training loss: 1.2528...  5.4840 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3122...  Training loss: 1.2556...  5.0660 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3123...  Training loss: 1.2895...  5.0550 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3124...  Training loss: 1.2688...  4.9390 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3125...  Training loss: 1.2642...  4.8340 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3126...  Training loss: 1.2632...  4.7710 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3127...  Training loss: 1.2755...  4.7060 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3128...  Training loss: 1.2618...  4.7980 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3129...  Training loss: 1.2376...  4.8220 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3130...  Training loss: 1.2848...  4.6670 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3131...  Training loss: 1.2925...  4.6390 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3132...  Training loss: 1.2769...  4.6420 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3133...  Training loss: 1.2631...  4.6030 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3134...  Training loss: 1.2769...  4.6330 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3135...  Training loss: 1.2728...  4.6150 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3136...  Training loss: 1.2765...  4.6280 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3137...  Training loss: 1.3025...  4.6510 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3138...  Training loss: 1.3471...  5.6870 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3139...  Training loss: 1.2834...  5.6924 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3140...  Training loss: 1.2817...  5.2711 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3141...  Training loss: 1.2691...  4.9310 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3142...  Training loss: 1.2612...  4.8750 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3143...  Training loss: 1.3068...  4.7720 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3144...  Training loss: 1.2760...  4.7239 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3145...  Training loss: 1.2897...  4.7419 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3146...  Training loss: 1.2582...  4.6299 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3147...  Training loss: 1.2648...  4.6669 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3148...  Training loss: 1.2963...  4.5889 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3149...  Training loss: 1.2544...  4.6249 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3150...  Training loss: 1.2506...  4.6809 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3151...  Training loss: 1.2558...  4.6499 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3152...  Training loss: 1.2767...  4.6579 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3153...  Training loss: 1.2712...  4.5629 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3154...  Training loss: 1.2620...  4.6519 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3155...  Training loss: 1.2643...  4.8720 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3156...  Training loss: 1.2499...  4.5849 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3157...  Training loss: 1.2967...  4.7179 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3158...  Training loss: 1.2687...  4.6399 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3159...  Training loss: 1.2672...  4.6259 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3160...  Training loss: 1.2672...  4.6159 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3161...  Training loss: 1.2535...  4.6039 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3162...  Training loss: 1.2626...  4.6409 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3163...  Training loss: 1.2861...  4.7449 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3164...  Training loss: 1.2567...  5.0610 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3165...  Training loss: 1.2468...  5.1900 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3166...  Training loss: 1.2729...  4.7960 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16/20...  Training Step: 3167...  Training loss: 1.2619...  4.8100 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3168...  Training loss: 1.2613...  5.1530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3169...  Training loss: 1.3901...  5.0650 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3170...  Training loss: 1.2861...  5.0930 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3171...  Training loss: 1.2799...  5.2170 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3172...  Training loss: 1.3011...  5.2621 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3173...  Training loss: 1.2466...  5.1060 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3174...  Training loss: 1.2414...  5.2170 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3175...  Training loss: 1.2851...  4.8380 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3176...  Training loss: 1.2675...  4.6329 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3177...  Training loss: 1.2799...  4.6469 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3178...  Training loss: 1.2630...  4.6769 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3179...  Training loss: 1.2600...  4.5909 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3180...  Training loss: 1.2667...  4.7890 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3181...  Training loss: 1.2733...  4.6849 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3182...  Training loss: 1.2847...  4.6849 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3183...  Training loss: 1.2531...  4.7179 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3184...  Training loss: 1.2570...  4.5729 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3185...  Training loss: 1.2956...  4.6739 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3186...  Training loss: 1.2903...  5.2080 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3187...  Training loss: 1.2693...  5.4751 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3188...  Training loss: 1.2950...  5.2851 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3189...  Training loss: 1.2658...  4.9590 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3190...  Training loss: 1.2850...  4.9690 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3191...  Training loss: 1.2702...  4.9680 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3192...  Training loss: 1.2874...  4.9750 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3193...  Training loss: 1.2697...  4.7880 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3194...  Training loss: 1.2361...  4.6919 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3195...  Training loss: 1.2361...  4.6639 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3196...  Training loss: 1.2903...  4.6479 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3197...  Training loss: 1.2909...  4.6079 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3198...  Training loss: 1.2838...  4.6389 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3199...  Training loss: 1.2546...  4.7289 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3200...  Training loss: 1.2493...  4.6439 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3201...  Training loss: 1.2737...  6.5836 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3202...  Training loss: 1.2855...  4.5624 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3203...  Training loss: 1.2565...  4.7374 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3204...  Training loss: 1.2763...  4.7844 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3205...  Training loss: 1.2470...  4.7764 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3206...  Training loss: 1.2379...  4.6724 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3207...  Training loss: 1.2245...  4.6754 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3208...  Training loss: 1.2550...  4.6754 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3209...  Training loss: 1.2334...  4.6064 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3210...  Training loss: 1.3071...  4.6404 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3211...  Training loss: 1.2562...  4.7204 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3212...  Training loss: 1.2412...  4.6684 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3213...  Training loss: 1.2757...  4.8595 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3214...  Training loss: 1.2488...  4.5884 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3215...  Training loss: 1.2565...  4.6744 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3216...  Training loss: 1.2679...  4.6174 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3217...  Training loss: 1.2669...  4.6474 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3218...  Training loss: 1.2890...  4.6934 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3219...  Training loss: 1.2435...  4.7424 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3220...  Training loss: 1.3065...  4.6964 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3221...  Training loss: 1.2771...  4.6304 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3222...  Training loss: 1.2840...  4.6224 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3223...  Training loss: 1.2636...  4.5954 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3224...  Training loss: 1.2698...  4.6484 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3225...  Training loss: 1.2851...  4.6824 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3226...  Training loss: 1.2523...  4.6467 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3227...  Training loss: 1.2432...  4.6017 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3228...  Training loss: 1.3092...  4.5747 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3229...  Training loss: 1.2730...  4.5797 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3230...  Training loss: 1.3117...  4.6367 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3231...  Training loss: 1.2884...  5.3073 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3232...  Training loss: 1.2844...  6.4078 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3233...  Training loss: 1.2730...  5.8231 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3234...  Training loss: 1.2789...  5.2764 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3235...  Training loss: 1.2892...  5.3233 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3236...  Training loss: 1.2594...  5.1094 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3237...  Training loss: 1.2661...  4.9395 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3238...  Training loss: 1.2551...  5.1794 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3239...  Training loss: 1.3131...  5.1954 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3240...  Training loss: 1.2933...  4.9405 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3241...  Training loss: 1.2966...  4.8386 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3242...  Training loss: 1.2504...  4.8306 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3243...  Training loss: 1.2728...  4.7866 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3244...  Training loss: 1.3035...  4.7916 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3245...  Training loss: 1.2691...  4.6837 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3246...  Training loss: 1.2619...  4.6327 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3247...  Training loss: 1.2298...  4.5967 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3248...  Training loss: 1.2743...  4.6117 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3249...  Training loss: 1.2277...  4.7226 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3250...  Training loss: 1.2645...  4.6167 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3251...  Training loss: 1.2501...  4.6607 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3252...  Training loss: 1.2672...  4.6197 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3253...  Training loss: 1.2387...  4.5907 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3254...  Training loss: 1.2658...  4.5807 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3255...  Training loss: 1.2429...  4.6447 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3256...  Training loss: 1.2475...  4.6127 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3257...  Training loss: 1.2310...  4.7776 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3258...  Training loss: 1.2734...  4.6417 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3259...  Training loss: 1.2480...  4.6887 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3260...  Training loss: 1.2559...  4.6157 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3261...  Training loss: 1.2451...  4.6027 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3262...  Training loss: 1.2484...  4.7476 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3263...  Training loss: 1.2482...  5.2604 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3264...  Training loss: 1.2717...  4.8376 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 3265...  Training loss: 1.2631...  4.8336 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3266...  Training loss: 1.2437...  4.8686 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3267...  Training loss: 1.2635...  4.6887 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3268...  Training loss: 1.2448...  4.7996 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3269...  Training loss: 1.2655...  4.8086 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3270...  Training loss: 1.2500...  5.0175 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3271...  Training loss: 1.2795...  5.2914 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3272...  Training loss: 1.2581...  5.0305 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3273...  Training loss: 1.2560...  4.7796 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3274...  Training loss: 1.2625...  4.8436 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3275...  Training loss: 1.2509...  4.7646 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3276...  Training loss: 1.2697...  4.6177 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3277...  Training loss: 1.2449...  4.5957 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3278...  Training loss: 1.2827...  4.6577 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3279...  Training loss: 1.2567...  4.5947 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3280...  Training loss: 1.2722...  4.5877 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3281...  Training loss: 1.2656...  4.6507 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3282...  Training loss: 1.2491...  4.7956 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3283...  Training loss: 1.2399...  4.6827 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3284...  Training loss: 1.2290...  4.7486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3285...  Training loss: 1.2709...  4.8866 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3286...  Training loss: 1.2708...  4.8546 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3287...  Training loss: 1.2528...  5.1634 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3288...  Training loss: 1.2594...  5.1935 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3289...  Training loss: 1.2635...  4.9200 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3290...  Training loss: 1.2322...  4.7780 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3291...  Training loss: 1.2195...  4.7180 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3292...  Training loss: 1.2554...  4.7410 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3293...  Training loss: 1.2536...  4.7770 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3294...  Training loss: 1.2218...  4.7610 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3295...  Training loss: 1.2740...  4.8690 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3296...  Training loss: 1.2724...  4.6940 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3297...  Training loss: 1.2382...  4.5700 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3298...  Training loss: 1.2221...  4.6890 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3299...  Training loss: 1.2241...  4.7120 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3300...  Training loss: 1.2392...  4.6480 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3301...  Training loss: 1.2824...  4.7990 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3302...  Training loss: 1.2663...  4.7500 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3303...  Training loss: 1.2692...  5.1100 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3304...  Training loss: 1.2600...  4.8090 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3305...  Training loss: 1.2982...  5.4700 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3306...  Training loss: 1.2750...  5.4460 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3307...  Training loss: 1.2649...  5.6220 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3308...  Training loss: 1.2652...  5.7570 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3309...  Training loss: 1.3116...  5.5690 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3310...  Training loss: 1.2783...  5.2610 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3311...  Training loss: 1.2557...  5.0650 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3312...  Training loss: 1.2933...  5.4950 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3313...  Training loss: 1.2457...  5.2180 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3314...  Training loss: 1.2913...  5.1150 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3315...  Training loss: 1.2870...  4.8550 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3316...  Training loss: 1.3074...  4.7670 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3317...  Training loss: 1.2932...  4.9510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3318...  Training loss: 1.2556...  4.9750 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3319...  Training loss: 1.2281...  4.8720 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3320...  Training loss: 1.2438...  4.9040 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3321...  Training loss: 1.2704...  4.5850 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3322...  Training loss: 1.2559...  4.6220 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3323...  Training loss: 1.2637...  4.8000 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3324...  Training loss: 1.2560...  4.6160 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3325...  Training loss: 1.2668...  4.6220 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3326...  Training loss: 1.2606...  4.6400 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3327...  Training loss: 1.2301...  4.6340 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3328...  Training loss: 1.2887...  4.6890 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3329...  Training loss: 1.2743...  4.8510 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3330...  Training loss: 1.2642...  5.0750 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3331...  Training loss: 1.2538...  5.1660 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3332...  Training loss: 1.2737...  5.0780 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3333...  Training loss: 1.2587...  5.2000 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3334...  Training loss: 1.2614...  4.9530 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3335...  Training loss: 1.2919...  4.9130 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3336...  Training loss: 1.3281...  5.1520 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3337...  Training loss: 1.2683...  5.1020 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3338...  Training loss: 1.2664...  5.1920 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3339...  Training loss: 1.2578...  4.8870 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3340...  Training loss: 1.2492...  4.6690 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3341...  Training loss: 1.2915...  4.7030 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3342...  Training loss: 1.2686...  4.6850 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3343...  Training loss: 1.2756...  4.6880 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3344...  Training loss: 1.2445...  4.6320 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3345...  Training loss: 1.2598...  4.6460 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3346...  Training loss: 1.2946...  4.7070 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3347...  Training loss: 1.2417...  4.7990 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3348...  Training loss: 1.2413...  4.8230 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3349...  Training loss: 1.2396...  4.9174 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3350...  Training loss: 1.2539...  4.9645 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3351...  Training loss: 1.2573...  4.9605 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3352...  Training loss: 1.2459...  4.7885 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3353...  Training loss: 1.2463...  4.8595 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3354...  Training loss: 1.2354...  5.4595 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3355...  Training loss: 1.2890...  5.1905 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3356...  Training loss: 1.2621...  5.5766 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3357...  Training loss: 1.2538...  5.5486 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3358...  Training loss: 1.2547...  5.4455 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3359...  Training loss: 1.2420...  4.9125 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3360...  Training loss: 1.2425...  5.1155 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3361...  Training loss: 1.2631...  5.0625 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3362...  Training loss: 1.2437...  5.3115 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17/20...  Training Step: 3363...  Training loss: 1.2211...  5.0315 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3364...  Training loss: 1.2609...  4.8455 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3365...  Training loss: 1.2496...  5.1665 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3366...  Training loss: 1.2467...  5.0485 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3367...  Training loss: 1.3799...  4.7575 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3368...  Training loss: 1.2756...  4.6905 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3369...  Training loss: 1.2614...  4.6445 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3370...  Training loss: 1.2850...  5.4215 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3371...  Training loss: 1.2423...  5.3205 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3372...  Training loss: 1.2187...  4.9895 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3373...  Training loss: 1.2617...  4.7445 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3374...  Training loss: 1.2516...  4.6615 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3375...  Training loss: 1.2625...  4.6865 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3376...  Training loss: 1.2482...  4.6225 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3377...  Training loss: 1.2426...  4.6435 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3378...  Training loss: 1.2618...  4.7465 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3379...  Training loss: 1.2662...  4.8405 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3380...  Training loss: 1.2664...  5.2805 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3381...  Training loss: 1.2474...  5.0965 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3382...  Training loss: 1.2342...  4.6885 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3383...  Training loss: 1.2716...  4.8925 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3384...  Training loss: 1.2768...  4.7255 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3385...  Training loss: 1.2637...  5.3275 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3386...  Training loss: 1.2828...  4.9725 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3387...  Training loss: 1.2634...  4.9905 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3388...  Training loss: 1.2683...  4.8965 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3389...  Training loss: 1.2560...  4.6955 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3390...  Training loss: 1.2827...  4.6725 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3391...  Training loss: 1.2547...  4.5935 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3392...  Training loss: 1.2175...  4.6165 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3393...  Training loss: 1.2300...  4.7535 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3394...  Training loss: 1.2812...  4.8275 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3395...  Training loss: 1.2776...  5.0895 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3396...  Training loss: 1.2722...  5.1755 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3397...  Training loss: 1.2530...  4.8145 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3398...  Training loss: 1.2356...  4.6255 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3399...  Training loss: 1.2566...  4.6215 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3400...  Training loss: 1.2744...  4.6485 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3401...  Training loss: 1.2414...  5.6976 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3402...  Training loss: 1.2616...  5.2685 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3403...  Training loss: 1.2369...  5.0305 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3404...  Training loss: 1.2098...  4.8105 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3405...  Training loss: 1.2176...  4.7095 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3406...  Training loss: 1.2467...  4.5905 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3407...  Training loss: 1.2352...  4.6755 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3408...  Training loss: 1.3044...  4.9765 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3409...  Training loss: 1.2485...  4.9026 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3410...  Training loss: 1.2381...  4.8860 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3411...  Training loss: 1.2641...  4.9300 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3412...  Training loss: 1.2373...  4.7930 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3413...  Training loss: 1.2450...  4.8100 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3414...  Training loss: 1.2556...  5.1780 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3415...  Training loss: 1.2525...  5.5471 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3416...  Training loss: 1.2696...  5.8122 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3417...  Training loss: 1.2291...  5.4861 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3418...  Training loss: 1.2928...  5.0810 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3419...  Training loss: 1.2665...  5.0570 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3420...  Training loss: 1.2652...  5.1700 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3421...  Training loss: 1.2497...  4.7590 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3422...  Training loss: 1.2571...  4.7049 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3423...  Training loss: 1.2724...  4.8480 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3424...  Training loss: 1.2461...  4.8160 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3425...  Training loss: 1.2387...  4.8880 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3426...  Training loss: 1.2918...  4.9230 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3427...  Training loss: 1.2582...  4.8110 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3428...  Training loss: 1.3048...  4.8820 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3429...  Training loss: 1.2734...  4.6819 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3430...  Training loss: 1.2589...  4.6709 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3431...  Training loss: 1.2642...  4.6019 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3432...  Training loss: 1.2703...  4.6809 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3433...  Training loss: 1.2910...  4.8840 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3434...  Training loss: 1.2452...  5.6201 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3435...  Training loss: 1.2645...  5.2160 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3436...  Training loss: 1.2410...  5.0490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3437...  Training loss: 1.3025...  4.8230 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3438...  Training loss: 1.2718...  4.6029 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3439...  Training loss: 1.2823...  4.8280 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3440...  Training loss: 1.2346...  4.9190 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3441...  Training loss: 1.2661...  5.2460 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3442...  Training loss: 1.2774...  5.3501 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3443...  Training loss: 1.2575...  5.1130 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3444...  Training loss: 1.2486...  5.0740 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3445...  Training loss: 1.2239...  4.8180 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3446...  Training loss: 1.2601...  5.0430 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3447...  Training loss: 1.2233...  4.7880 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3448...  Training loss: 1.2539...  4.7379 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3449...  Training loss: 1.2266...  4.7780 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3450...  Training loss: 1.2526...  4.7449 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3451...  Training loss: 1.2328...  4.7129 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3452...  Training loss: 1.2507...  4.8530 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3453...  Training loss: 1.2321...  4.7419 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3454...  Training loss: 1.2450...  5.2420 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3455...  Training loss: 1.2142...  5.0250 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3456...  Training loss: 1.2575...  5.0410 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3457...  Training loss: 1.2344...  5.2020 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3458...  Training loss: 1.2511...  5.4341 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3459...  Training loss: 1.2334...  5.2250 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3460...  Training loss: 1.2222...  5.2180 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 3461...  Training loss: 1.2394...  5.0660 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3462...  Training loss: 1.2619...  5.3751 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3463...  Training loss: 1.2625...  5.2901 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3464...  Training loss: 1.2240...  4.8060 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3465...  Training loss: 1.2405...  5.2450 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3466...  Training loss: 1.2364...  4.9050 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3467...  Training loss: 1.2577...  4.8350 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3468...  Training loss: 1.2345...  4.9490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3469...  Training loss: 1.2547...  5.0520 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3470...  Training loss: 1.2451...  5.1140 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3471...  Training loss: 1.2359...  5.2380 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3472...  Training loss: 1.2490...  5.0890 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3473...  Training loss: 1.2530...  5.4431 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3474...  Training loss: 1.2581...  6.0332 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3475...  Training loss: 1.2280...  6.0122 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3476...  Training loss: 1.2634...  6.0172 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3477...  Training loss: 1.2387...  6.3663 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3478...  Training loss: 1.2523...  6.7353 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3479...  Training loss: 1.2433...  5.5871 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3480...  Training loss: 1.2441...  5.6871 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3481...  Training loss: 1.2272...  5.5781 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3482...  Training loss: 1.2135...  5.4871 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3483...  Training loss: 1.2569...  5.3601 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3484...  Training loss: 1.2568...  5.2280 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3485...  Training loss: 1.2415...  5.2651 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3486...  Training loss: 1.2543...  5.5861 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3487...  Training loss: 1.2536...  5.5921 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3488...  Training loss: 1.2185...  5.8102 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3489...  Training loss: 1.2137...  5.9332 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3490...  Training loss: 1.2549...  5.0660 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3491...  Training loss: 1.2370...  5.0100 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3492...  Training loss: 1.2092...  4.7479 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3493...  Training loss: 1.2590...  4.7580 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3494...  Training loss: 1.2589...  4.6769 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3495...  Training loss: 1.2367...  4.6569 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3496...  Training loss: 1.2120...  4.7169 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3497...  Training loss: 1.2112...  4.6619 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3498...  Training loss: 1.2323...  4.7169 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3499...  Training loss: 1.2717...  4.8400 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3500...  Training loss: 1.2560...  4.8120 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3501...  Training loss: 1.2553...  5.0070 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3502...  Training loss: 1.2437...  4.9650 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3503...  Training loss: 1.2892...  5.1440 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3504...  Training loss: 1.2769...  5.1900 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3505...  Training loss: 1.2520...  5.2260 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3506...  Training loss: 1.2542...  4.9840 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3507...  Training loss: 1.2988...  4.9340 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3508...  Training loss: 1.2670...  4.6819 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3509...  Training loss: 1.2409...  4.8040 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3510...  Training loss: 1.2841...  4.7720 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3511...  Training loss: 1.2305...  4.6139 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3512...  Training loss: 1.2795...  4.6839 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3513...  Training loss: 1.2648...  4.6289 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3514...  Training loss: 1.2879...  4.7009 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3515...  Training loss: 1.2812...  5.0110 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3516...  Training loss: 1.2400...  4.8350 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3517...  Training loss: 1.2212...  4.8790 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3518...  Training loss: 1.2334...  4.8740 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3519...  Training loss: 1.2573...  5.1440 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3520...  Training loss: 1.2460...  4.7610 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3521...  Training loss: 1.2389...  4.6529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3522...  Training loss: 1.2467...  4.6599 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3523...  Training loss: 1.2558...  4.6579 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3524...  Training loss: 1.2395...  4.7850 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3525...  Training loss: 1.2097...  4.7069 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3526...  Training loss: 1.2684...  4.6529 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3527...  Training loss: 1.2720...  4.7750 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3528...  Training loss: 1.2525...  5.0820 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3529...  Training loss: 1.2405...  4.9680 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3530...  Training loss: 1.2489...  4.9810 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3531...  Training loss: 1.2468...  5.1750 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3532...  Training loss: 1.2457...  5.0970 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3533...  Training loss: 1.2798...  5.6371 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3534...  Training loss: 1.3092...  5.3721 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3535...  Training loss: 1.2648...  5.6401 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3536...  Training loss: 1.2508...  5.5461 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3537...  Training loss: 1.2431...  5.6021 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3538...  Training loss: 1.2351...  5.1960 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3539...  Training loss: 1.2790...  4.6759 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3540...  Training loss: 1.2560...  4.7099 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3541...  Training loss: 1.2627...  4.6789 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3542...  Training loss: 1.2270...  4.6009 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3543...  Training loss: 1.2388...  4.6599 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3544...  Training loss: 1.2760...  4.7510 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3545...  Training loss: 1.2289...  4.6669 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3546...  Training loss: 1.2225...  4.8610 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3547...  Training loss: 1.2282...  4.8810 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3548...  Training loss: 1.2463...  4.8780 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3549...  Training loss: 1.2461...  4.8190 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3550...  Training loss: 1.2396...  5.1460 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3551...  Training loss: 1.2479...  5.2060 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3552...  Training loss: 1.2280...  4.9840 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3553...  Training loss: 1.2774...  5.1780 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3554...  Training loss: 1.2531...  5.1490 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3555...  Training loss: 1.2431...  4.9060 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3556...  Training loss: 1.2318...  4.9440 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3557...  Training loss: 1.2239...  4.7079 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3558...  Training loss: 1.2338...  4.7620 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18/20...  Training Step: 3559...  Training loss: 1.2577...  4.8450 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3560...  Training loss: 1.2453...  5.1200 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3561...  Training loss: 1.2114...  5.4201 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3562...  Training loss: 1.2581...  5.3501 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3563...  Training loss: 1.2354...  5.2671 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3564...  Training loss: 1.2386...  4.7930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3565...  Training loss: 1.3552...  4.7079 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3566...  Training loss: 1.2601...  4.6709 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3567...  Training loss: 1.2511...  4.6359 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3568...  Training loss: 1.2658...  4.7139 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3569...  Training loss: 1.2305...  4.7019 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3570...  Training loss: 1.2068...  4.6339 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3571...  Training loss: 1.2468...  4.6209 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3572...  Training loss: 1.2434...  4.6289 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3573...  Training loss: 1.2580...  5.2200 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3574...  Training loss: 1.2350...  4.9740 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3575...  Training loss: 1.2334...  5.0690 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3576...  Training loss: 1.2499...  5.1620 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3577...  Training loss: 1.2506...  5.2500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3578...  Training loss: 1.2606...  5.4951 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3579...  Training loss: 1.2298...  4.8370 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3580...  Training loss: 1.2241...  4.8360 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3581...  Training loss: 1.2660...  4.9790 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3582...  Training loss: 1.2744...  4.6589 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3583...  Training loss: 1.2524...  4.6769 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3584...  Training loss: 1.2727...  4.6559 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3585...  Training loss: 1.2560...  4.6989 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3586...  Training loss: 1.2675...  4.6479 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3587...  Training loss: 1.2426...  4.6369 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3588...  Training loss: 1.2759...  4.6559 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3589...  Training loss: 1.2478...  4.6849 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3590...  Training loss: 1.2049...  4.6959 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3591...  Training loss: 1.2250...  4.6639 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3592...  Training loss: 1.2729...  4.7479 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3593...  Training loss: 1.2585...  4.9300 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3594...  Training loss: 1.2620...  5.9192 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3595...  Training loss: 1.2357...  5.4641 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3596...  Training loss: 1.2227...  5.8062 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3597...  Training loss: 1.2585...  5.5141 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3598...  Training loss: 1.2545...  5.0610 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3599...  Training loss: 1.2403...  5.1220 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3600...  Training loss: 1.2520...  4.7890 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3601...  Training loss: 1.2221...  5.7592 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3602...  Training loss: 1.2135...  5.0080 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3603...  Training loss: 1.2138...  5.1540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3604...  Training loss: 1.2441...  4.8900 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3605...  Training loss: 1.2192...  4.6139 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3606...  Training loss: 1.2813...  4.8060 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3607...  Training loss: 1.2301...  4.7780 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3608...  Training loss: 1.2143...  4.8800 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3609...  Training loss: 1.2540...  4.9870 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3610...  Training loss: 1.2227...  4.9430 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3611...  Training loss: 1.2278...  4.9000 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3612...  Training loss: 1.2404...  4.7710 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3613...  Training loss: 1.2317...  4.9150 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3614...  Training loss: 1.2583...  4.7229 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3615...  Training loss: 1.2134...  4.6279 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3616...  Training loss: 1.2759...  4.6669 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3617...  Training loss: 1.2398...  4.7049 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3618...  Training loss: 1.2629...  4.6759 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3619...  Training loss: 1.2392...  4.7870 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3620...  Training loss: 1.2467...  4.6229 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3621...  Training loss: 1.2490...  4.5929 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3622...  Training loss: 1.2284...  4.6359 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3623...  Training loss: 1.2228...  4.6449 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3624...  Training loss: 1.2738...  4.8720 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3625...  Training loss: 1.2494...  5.0990 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3626...  Training loss: 1.2951...  5.5601 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3627...  Training loss: 1.2595...  4.9770 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3628...  Training loss: 1.2569...  4.7750 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3629...  Training loss: 1.2490...  4.7319 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3630...  Training loss: 1.2589...  4.8440 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3631...  Training loss: 1.2718...  4.6439 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3632...  Training loss: 1.2309...  4.9060 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3633...  Training loss: 1.2507...  4.6889 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3634...  Training loss: 1.2396...  4.6819 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3635...  Training loss: 1.2905...  4.6529 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3636...  Training loss: 1.2623...  4.6799 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3637...  Training loss: 1.2734...  4.6709 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3638...  Training loss: 1.2365...  4.6339 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3639...  Training loss: 1.2537...  4.6838 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3640...  Training loss: 1.2639...  4.7967 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3641...  Training loss: 1.2515...  4.7897 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3642...  Training loss: 1.2420...  5.0844 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3643...  Training loss: 1.2011...  4.7737 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3644...  Training loss: 1.2572...  4.7297 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3645...  Training loss: 1.2140...  4.9076 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3646...  Training loss: 1.2481...  4.7128 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3647...  Training loss: 1.2244...  4.7517 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3648...  Training loss: 1.2325...  4.6428 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3649...  Training loss: 1.2290...  4.7407 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3650...  Training loss: 1.2402...  4.8247 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3651...  Training loss: 1.2249...  5.1783 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3652...  Training loss: 1.2276...  4.9645 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3653...  Training loss: 1.2109...  4.9905 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3654...  Training loss: 1.2437...  4.7377 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3655...  Training loss: 1.2098...  4.7657 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3656...  Training loss: 1.2345...  4.7487 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 3657...  Training loss: 1.2202...  4.8596 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3658...  Training loss: 1.2244...  5.0884 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3659...  Training loss: 1.2263...  4.7677 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3660...  Training loss: 1.2466...  4.8217 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3661...  Training loss: 1.2463...  4.8646 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3662...  Training loss: 1.2137...  4.6878 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3663...  Training loss: 1.2326...  4.6158 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3664...  Training loss: 1.2240...  4.6298 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3665...  Training loss: 1.2466...  4.5939 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3666...  Training loss: 1.2314...  4.5759 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3667...  Training loss: 1.2418...  4.8966 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3668...  Training loss: 1.2258...  5.1953 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3669...  Training loss: 1.2309...  5.2543 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3670...  Training loss: 1.2322...  5.3032 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3671...  Training loss: 1.2440...  5.3991 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3672...  Training loss: 1.2481...  5.1903 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3673...  Training loss: 1.2287...  5.0964 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3674...  Training loss: 1.2615...  4.9445 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3675...  Training loss: 1.2303...  5.0205 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3676...  Training loss: 1.2448...  4.8866 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3677...  Training loss: 1.2450...  4.8336 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3678...  Training loss: 1.2260...  4.9525 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3679...  Training loss: 1.2132...  4.7407 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3680...  Training loss: 1.2108...  4.6908 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3681...  Training loss: 1.2401...  4.7307 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3682...  Training loss: 1.2480...  4.7827 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3683...  Training loss: 1.2328...  4.9226 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3684...  Training loss: 1.2360...  4.5729 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3685...  Training loss: 1.2434...  4.6298 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3686...  Training loss: 1.1987...  4.7347 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3687...  Training loss: 1.1936...  4.8786 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3688...  Training loss: 1.2439...  4.8057 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3689...  Training loss: 1.2278...  5.1574 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3690...  Training loss: 1.1980...  5.4351 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3691...  Training loss: 1.2409...  5.3322 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3692...  Training loss: 1.2311...  5.1094 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3693...  Training loss: 1.2243...  5.3652 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3694...  Training loss: 1.1928...  5.4851 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3695...  Training loss: 1.1906...  5.3112 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3696...  Training loss: 1.2231...  5.0844 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3697...  Training loss: 1.2611...  5.2862 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3698...  Training loss: 1.2422...  5.1893 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3699...  Training loss: 1.2405...  5.7948 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3700...  Training loss: 1.2318...  5.4425 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3701...  Training loss: 1.2654...  5.2350 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3702...  Training loss: 1.2640...  5.1260 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3703...  Training loss: 1.2406...  5.4619 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3704...  Training loss: 1.2428...  5.3669 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3705...  Training loss: 1.2889...  5.1040 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3706...  Training loss: 1.2572...  5.0800 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3707...  Training loss: 1.2271...  5.4289 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3708...  Training loss: 1.2750...  5.5819 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3709...  Training loss: 1.2263...  5.1720 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3710...  Training loss: 1.2707...  5.2370 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3711...  Training loss: 1.2471...  5.5169 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3712...  Training loss: 1.2740...  5.2170 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3713...  Training loss: 1.2704...  5.4099 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3714...  Training loss: 1.2373...  5.0580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3715...  Training loss: 1.2047...  5.5079 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3716...  Training loss: 1.2210...  5.3449 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3717...  Training loss: 1.2556...  5.5489 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3718...  Training loss: 1.2457...  5.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3719...  Training loss: 1.2232...  5.2579 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3720...  Training loss: 1.2373...  5.2589 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3721...  Training loss: 1.2354...  5.8958 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3722...  Training loss: 1.2209...  6.5947 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3723...  Training loss: 1.2035...  6.4727 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3724...  Training loss: 1.2607...  5.3429 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3725...  Training loss: 1.2613...  5.2699 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3726...  Training loss: 1.2384...  5.2849 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3727...  Training loss: 1.2219...  5.1890 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3728...  Training loss: 1.2396...  5.1490 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3729...  Training loss: 1.2337...  5.3669 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3730...  Training loss: 1.2354...  5.8598 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3731...  Training loss: 1.2700...  5.0600 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3732...  Training loss: 1.3031...  4.9780 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3733...  Training loss: 1.2465...  4.8700 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3734...  Training loss: 1.2512...  4.7910 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3735...  Training loss: 1.2315...  4.7411 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3736...  Training loss: 1.2327...  4.8550 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3737...  Training loss: 1.2743...  4.7670 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3738...  Training loss: 1.2435...  5.1140 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3739...  Training loss: 1.2517...  4.8870 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3740...  Training loss: 1.2150...  4.7321 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3741...  Training loss: 1.2316...  4.7441 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3742...  Training loss: 1.2652...  4.8960 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3743...  Training loss: 1.2200...  4.8040 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3744...  Training loss: 1.2137...  4.7740 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3745...  Training loss: 1.2171...  5.2470 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3746...  Training loss: 1.2389...  5.1540 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3747...  Training loss: 1.2321...  4.7710 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3748...  Training loss: 1.2357...  4.7830 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3749...  Training loss: 1.2378...  4.6351 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3750...  Training loss: 1.2215...  4.5761 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3751...  Training loss: 1.2578...  4.6231 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3752...  Training loss: 1.2357...  4.7610 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3753...  Training loss: 1.2362...  4.7930 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3754...  Training loss: 1.2378...  4.9480 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20...  Training Step: 3755...  Training loss: 1.2266...  4.9750 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3756...  Training loss: 1.2265...  4.9580 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3757...  Training loss: 1.2442...  4.7500 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3758...  Training loss: 1.2257...  4.5714 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3759...  Training loss: 1.1992...  4.6420 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3760...  Training loss: 1.2480...  4.5790 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3761...  Training loss: 1.2240...  4.6560 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3762...  Training loss: 1.2261...  4.5950 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3763...  Training loss: 1.3487...  4.6540 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3764...  Training loss: 1.2495...  4.5860 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3765...  Training loss: 1.2389...  4.6480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3766...  Training loss: 1.2671...  4.7820 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3767...  Training loss: 1.2163...  4.6030 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3768...  Training loss: 1.2050...  4.8630 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3769...  Training loss: 1.2458...  4.9660 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3770...  Training loss: 1.2342...  5.5400 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3771...  Training loss: 1.2383...  5.4050 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3772...  Training loss: 1.2255...  4.9900 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3773...  Training loss: 1.2260...  4.7420 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3774...  Training loss: 1.2286...  4.6830 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3775...  Training loss: 1.2379...  4.6620 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3776...  Training loss: 1.2408...  4.6530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3777...  Training loss: 1.2286...  4.6790 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3778...  Training loss: 1.2135...  4.6970 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3779...  Training loss: 1.2482...  4.6420 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3780...  Training loss: 1.2643...  4.7480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3781...  Training loss: 1.2346...  4.8770 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3782...  Training loss: 1.2648...  4.6380 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3783...  Training loss: 1.2411...  4.6480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3784...  Training loss: 1.2458...  4.8310 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3785...  Training loss: 1.2365...  4.7650 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3786...  Training loss: 1.2597...  5.3780 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3787...  Training loss: 1.2329...  5.3090 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3788...  Training loss: 1.1927...  5.6330 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3789...  Training loss: 1.2110...  5.1420 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3790...  Training loss: 1.2557...  5.2850 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3791...  Training loss: 1.2521...  5.5160 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3792...  Training loss: 1.2464...  5.0870 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3793...  Training loss: 1.2212...  4.8480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3794...  Training loss: 1.2111...  5.5090 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3795...  Training loss: 1.2413...  4.9060 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3796...  Training loss: 1.2429...  4.7860 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3797...  Training loss: 1.2268...  4.7590 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3798...  Training loss: 1.2442...  4.8030 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3799...  Training loss: 1.2103...  4.9020 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3800...  Training loss: 1.2011...  5.0710 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3801...  Training loss: 1.1949...  5.2320 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3802...  Training loss: 1.2204...  4.8140 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3803...  Training loss: 1.2113...  4.7680 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3804...  Training loss: 1.2783...  4.8530 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3805...  Training loss: 1.2340...  4.7620 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3806...  Training loss: 1.2095...  4.8430 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3807...  Training loss: 1.2455...  4.6570 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3808...  Training loss: 1.2195...  4.6560 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3809...  Training loss: 1.2170...  4.5550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3810...  Training loss: 1.2222...  4.6070 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3811...  Training loss: 1.2251...  4.5810 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3812...  Training loss: 1.2491...  4.6370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3813...  Training loss: 1.2153...  4.6800 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3814...  Training loss: 1.2705...  4.8980 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3815...  Training loss: 1.2396...  4.8950 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3816...  Training loss: 1.2503...  5.0480 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3817...  Training loss: 1.2343...  4.9620 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3818...  Training loss: 1.2364...  5.5920 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3819...  Training loss: 1.2542...  5.8891 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3820...  Training loss: 1.2253...  6.3816 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3821...  Training loss: 1.2188...  5.5836 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3822...  Training loss: 1.2759...  5.2445 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3823...  Training loss: 1.2408...  5.2025 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3824...  Training loss: 1.2749...  5.1445 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3825...  Training loss: 1.2600...  5.1445 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3826...  Training loss: 1.2438...  5.1145 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3827...  Training loss: 1.2297...  5.1335 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3828...  Training loss: 1.2479...  5.3545 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3829...  Training loss: 1.2533...  5.2265 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3830...  Training loss: 1.2250...  5.8056 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3831...  Training loss: 1.2416...  5.5036 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3832...  Training loss: 1.2287...  5.5556 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3833...  Training loss: 1.2797...  5.6316 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3834...  Training loss: 1.2558...  5.9446 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3835...  Training loss: 1.2613...  5.4615 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3836...  Training loss: 1.2159...  5.8106 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3837...  Training loss: 1.2506...  5.7736 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3838...  Training loss: 1.2600...  5.2815 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3839...  Training loss: 1.2353...  4.8005 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3840...  Training loss: 1.2269...  5.3355 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3841...  Training loss: 1.2010...  5.0735 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3842...  Training loss: 1.2426...  5.2595 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3843...  Training loss: 1.2039...  4.9665 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3844...  Training loss: 1.2331...  5.3175 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3845...  Training loss: 1.2145...  4.8475 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3846...  Training loss: 1.2247...  4.9215 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3847...  Training loss: 1.2078...  4.7115 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3848...  Training loss: 1.2340...  4.6435 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3849...  Training loss: 1.2124...  4.6275 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3850...  Training loss: 1.2132...  4.6045 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3851...  Training loss: 1.2019...  4.7775 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3852...  Training loss: 1.2297...  4.6715 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3853...  Training loss: 1.2080...  4.6825 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3854...  Training loss: 1.2216...  4.6615 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3855...  Training loss: 1.2054...  4.7745 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3856...  Training loss: 1.2000...  4.6065 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3857...  Training loss: 1.2237...  4.6285 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3858...  Training loss: 1.2449...  4.8455 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3859...  Training loss: 1.2393...  4.7835 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3860...  Training loss: 1.2062...  4.9005 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3861...  Training loss: 1.2138...  4.8155 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3862...  Training loss: 1.2126...  4.8325 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3863...  Training loss: 1.2317...  4.7705 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3864...  Training loss: 1.2247...  4.8075 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3865...  Training loss: 1.2363...  4.9355 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3866...  Training loss: 1.2169...  4.9815 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3867...  Training loss: 1.2200...  5.0355 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3868...  Training loss: 1.2277...  4.8315 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3869...  Training loss: 1.2324...  4.7685 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3870...  Training loss: 1.2329...  4.8035 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3871...  Training loss: 1.2155...  4.7455 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3872...  Training loss: 1.2506...  4.6155 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3873...  Training loss: 1.2197...  4.6795 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3874...  Training loss: 1.2363...  4.8415 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3875...  Training loss: 1.2358...  4.8235 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3876...  Training loss: 1.2208...  5.3975 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3877...  Training loss: 1.2130...  4.9555 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3878...  Training loss: 1.1981...  4.7405 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3879...  Training loss: 1.2464...  4.7788 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3880...  Training loss: 1.2366...  4.7640 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3881...  Training loss: 1.2214...  4.7089 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3882...  Training loss: 1.2235...  4.6729 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3883...  Training loss: 1.2394...  4.6329 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3884...  Training loss: 1.2018...  4.6209 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3885...  Training loss: 1.1877...  4.6289 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3886...  Training loss: 1.2241...  4.6289 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3887...  Training loss: 1.2127...  4.6579 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3888...  Training loss: 1.1833...  4.6649 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3889...  Training loss: 1.2430...  4.7910 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3890...  Training loss: 1.2314...  5.1200 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3891...  Training loss: 1.2073...  5.4031 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3892...  Training loss: 1.1836...  5.5361 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3893...  Training loss: 1.1813...  5.1750 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3894...  Training loss: 1.2150...  4.9380 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3895...  Training loss: 1.2509...  5.5791 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3896...  Training loss: 1.2368...  5.3401 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3897...  Training loss: 1.2445...  5.2771 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3898...  Training loss: 1.2318...  5.3161 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3899...  Training loss: 1.2643...  5.1160 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3900...  Training loss: 1.2566...  4.9380 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3901...  Training loss: 1.2427...  4.9340 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3902...  Training loss: 1.2410...  4.6299 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3903...  Training loss: 1.2816...  4.6569 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3904...  Training loss: 1.2531...  4.6549 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3905...  Training loss: 1.2207...  4.7479 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3906...  Training loss: 1.2576...  4.9300 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3907...  Training loss: 1.2274...  4.8710 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3908...  Training loss: 1.2571...  5.0080 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3909...  Training loss: 1.2451...  4.7620 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3910...  Training loss: 1.2642...  4.7339 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3911...  Training loss: 1.2516...  4.8550 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3912...  Training loss: 1.2288...  5.0600 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3913...  Training loss: 1.2014...  5.1240 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3914...  Training loss: 1.2114...  5.2761 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3915...  Training loss: 1.2340...  5.0110 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3916...  Training loss: 1.2183...  4.8510 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3917...  Training loss: 1.2198...  4.8350 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3918...  Training loss: 1.2266...  5.1450 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3919...  Training loss: 1.2307...  4.8360 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3920...  Training loss: 1.2203...  5.4421 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3921...  Training loss: 1.1994...  4.9080 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3922...  Training loss: 1.2512...  4.8780 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3923...  Training loss: 1.2565...  5.1060 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3924...  Training loss: 1.2353...  5.8432 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3925...  Training loss: 1.2211...  5.2891 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3926...  Training loss: 1.2318...  4.9910 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3927...  Training loss: 1.2224...  4.8350 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3928...  Training loss: 1.2269...  4.7469 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3929...  Training loss: 1.2623...  4.6949 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3930...  Training loss: 1.2917...  4.9240 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3931...  Training loss: 1.2415...  4.7399 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3932...  Training loss: 1.2337...  4.6939 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3933...  Training loss: 1.2302...  4.7880 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3934...  Training loss: 1.2162...  4.6979 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3935...  Training loss: 1.2593...  4.7019 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3936...  Training loss: 1.2377...  4.8300 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3937...  Training loss: 1.2396...  5.0140 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3938...  Training loss: 1.2118...  5.3081 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3939...  Training loss: 1.2217...  5.3441 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3940...  Training loss: 1.2648...  5.2941 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3941...  Training loss: 1.2131...  5.0980 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3942...  Training loss: 1.2014...  5.3621 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3943...  Training loss: 1.2154...  5.0100 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3944...  Training loss: 1.2170...  4.9700 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3945...  Training loss: 1.2252...  4.9370 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3946...  Training loss: 1.2206...  5.0760 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3947...  Training loss: 1.2196...  4.8300 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3948...  Training loss: 1.2131...  4.8400 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3949...  Training loss: 1.2467...  4.9580 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3950...  Training loss: 1.2281...  4.7169 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20...  Training Step: 3951...  Training loss: 1.2261...  4.8760 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3952...  Training loss: 1.2221...  4.8270 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3953...  Training loss: 1.2149...  4.9710 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3954...  Training loss: 1.2076...  5.0870 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3955...  Training loss: 1.2255...  5.6401 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3956...  Training loss: 1.2173...  6.2933 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3957...  Training loss: 1.2002...  5.5591 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3958...  Training loss: 1.2278...  5.6381 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3959...  Training loss: 1.2275...  5.2531 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3960...  Training loss: 1.2200...  4.9730 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "save_every_n = 200\n",
    "\n",
    "model = charRNN(len(vocab),batch_size=batch_size,num_steps=num_steps,\n",
    "                lstm_size=lstm_size,num_layers=num_layers,learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for e in range (epoch):\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x,y in create_batches(encoded, batch_size, num_steps):\n",
    "        #while True:\n",
    "            #x,y=next(create_batches(encoded, batch_size, num_steps))\n",
    "            counter +=1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_proba: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            \n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            end = time.time()\n",
    "            print('Epoch: {}/{}... '.format(e+1, epoch),\n",
    "                  'Training Step: {}... '.format(counter),\n",
    "                  'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                  #'Training state: {:.4f}... '.format(new_state),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints\\\\i3960_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints\\\\i3960_l512.ckpt\""
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "Now that the network is trained, we'll can use it to generate new text. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text. I also included some functionality to prime the network with some text by passing in a string and building up a state from that.\n",
    "\n",
    "The network gives us predictions for each character. To reduce noise and make things a little less random, I'm going to only choose a new character from the top N most likely characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_N(preds,vocab_size,top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]]=0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = charRNN(len(vocab),lstm_size=lstm_size,sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess,checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1,1))\n",
    "            x[0,0]=chat_to_int[c]\n",
    "            feed_dict = { model.inputs:x,\n",
    "                        model.keep_proba:1,\n",
    "                        model.initial_state:new_state}\n",
    "            preds,new_state = sess.run([model.prediction,model.final_state],feed_dict=feed_dict)\n",
    "        \n",
    "        c = pick_top_N(preds,len(vocab))\n",
    "        samples.append(int_to_car[c])\n",
    "        for i in range(n_samples):\n",
    "            x[0,0]=c\n",
    "            feed_dict = { model.inputs:x,\n",
    "                        model.keep_proba:1,\n",
    "                        model.initial_state:new_state}\n",
    "            preds,new_state = sess.run([model.prediction,model.final_state],feed_dict=feed_dict)\n",
    "            \n",
    "            c = pick_top_N(preds,len(vocab))\n",
    "            samples.append(int_to_car[c])\n",
    "            \n",
    "    return ''.join(samples)\n",
    "                       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, pass in the path to a checkpoint and sample from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints\\\\i3960_l512.ckpt'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i3960_l512.ckpt\n",
      "Farriath was\n",
      "all, to her the sound of the people. The point of the country and\n",
      "had bought in the past, and he went to spake towers the steams, and\n",
      "to her heart. He saw that they and he saw she was that he was in tell the\n",
      "province, would have said something. Turns of her cases and at the table\n",
      "with which she shameded all the same way as he had at the same\n",
      "crack of any sterning to the country in a force that they were both\n",
      "as seemed to to hear and when he saw her hellow only of the same she\n",
      "had said:\n",
      "\n",
      "\"That's all about you,\" said the baby shooting, but she felt so\n",
      "show herself and decided in anyone when he was seeing, and was sorry\n",
      "of the more and marshing which in haspen back he would have than\n",
      "ever see how in his brather and thinking, he had been such a man to\n",
      "believe in the proposed, though to dress in any words would have\n",
      "been some straight of interest in, and the day about him. And all\n",
      "too had no death seemed it he came for him, but he felt suddenly\n",
      "felt a shoulders, with his breaked hands and at home.\n",
      "\n",
      "Tanya would have been that so the day but his brother was as she was\n",
      "so as it was all he was not seeing their humor.\n",
      "\n",
      "His higher hige starile wife and the praces of the clevar, and was a\n",
      "mother, and with a crief of coarse stream, the most prayer of\n",
      "clutching of this attitude. He had been to charge three things with his\n",
      "short, his lugs and sheeps, and stopped in the pleasure, and the chair with\n",
      "her husband, and his head only to say a feeling. But which had not\n",
      "tellen to seat her husband, to trying; the conversation was in the\n",
      "marrhal to her, all to steps on a men on the sound of the chalmeron's\n",
      "hights and hearted thousands of her hands to take such a subject and the\n",
      "man, he had not said he did not, any she saw that in them, so had been\n",
      "already to be and settled to the still often.\n",
      "\n",
      "\"Why, if you don't know what you see anything but see it. The supper to\n",
      "stop a complacences of any sort of man was a match as an implace on a\n",
      "silence. But that is they all say one, and talki\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint,2000,lstm_size,len(vocab), prime=\"Far\")\n",
    "print (samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i200_l512.ckpt\n",
      "Farng had atinnd. \" on te ortis the san hos he shes set to atinn ant an at theed an he wis on here he th an tant an hang ose shos he an tore an ot and wer tit ale san oserad as ooth tha tis thar anth the tha ing wh ise tot the as ite hit ared. he athes hhite he ho tes ha aneg on heese hee and asestharesat ifo te son his tot ise tar han sithire so te tho he ware te hons hos, son thar soton sot orer ald ont ise he her althes theresend,, the the wire ans her ons ans tou sart int oond ate the arinnte al ond th as the thers tortitit he tho tire heres totin ton onte se an alt an otithe sot hind torin the wim tansid there sothe\n",
      "heessand\n",
      "on that hes anthan an the se athin on who that so he sasitos war tou the wans int onthat on aress ho the hang ho ta than thit ang whe thiser that the wos to has so on an tis ant at as is her sot the sirind tho he sor tho wh ter thon he antile wos ithere wind whe the hand tou serat ho times an hised on te arederesind wor ar in hiss he ward an tan hhe ar hhe te tor \n"
     ]
    }
   ],
   "source": [
    "checkpoint=\"checkpoints\\\\i200_l512.ckpt\"\n",
    "samp = sample(checkpoint,1000,lstm_size,len(vocab), prime=\"Far\")\n",
    "print (samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
